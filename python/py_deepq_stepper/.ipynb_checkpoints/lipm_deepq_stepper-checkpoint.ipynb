{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file contains a prototype idea of trying to learn a value function that represents the viable region\n",
    "## for a LIP model. \n",
    "## Author : Avadesh Meduri\n",
    "## Date : 20/02/2020\n",
    "\n",
    "import numpy as np\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import pickle as p\n",
    "import os\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIPM Environment\n",
    "\n",
    "class LipmEnv:\n",
    "    def __init__(self, h):\n",
    "        self.omega = np.sqrt(9.81/h)\n",
    "        self.max_leg_length = 0.6\n",
    "        self.dt = 0.001\n",
    "        self.h = h\n",
    "        self.A = np.matrix([[1, self.dt], [(self.omega**2)*self.dt, 1]])\n",
    "        self.B = np.matrix([0, -(self.omega**2)*self.dt])\n",
    "        self.t = 0\n",
    "                                 \n",
    "    def integrate_lip_dynamics(self, x_t, u_t):\n",
    "        ## integrates dynamics for one step\n",
    "        assert np.shape(x_t) == (2,)\n",
    "        x_t_1 = np.matmul(self.A, np.transpose(x_t)) + np.matmul(self.B.transpose(), [u_t])\n",
    "        return x_t_1\n",
    "\n",
    "    def reset_env(self, x0, u0, epi_time):\n",
    "        ## initialises environment\n",
    "        self.t = 0\n",
    "        self.sim_data = np.zeros((4, int(epi_time/self.dt)+1))\n",
    "        self.sim_data[:,0][0:2] = x0\n",
    "        self.sim_data[:,0][2] = u0\n",
    "        self.sim_data[:,0][3] = self.h\n",
    "    def step_env(self):\n",
    "        ## integrates the simulation one step\n",
    "        self.sim_data[:,self.t + 1][0:2] = self.integrate_lip_dynamics(self.sim_data[:,self.t][0:2],\\\n",
    "                                                   self.sim_data[:,self.t][2])\n",
    "        self.sim_data[:,self.t + 1][2] = self.sim_data[:,self.t][2]\n",
    "        self.sim_data[:,self.t + 1][3] = self.sim_data[:,self.t][3] \n",
    "        self.t += 1\n",
    "    \n",
    "    def set_action(self, u):\n",
    "        self.sim_data[:,self.t][2] = u\n",
    "        \n",
    "    def return_sample_data(self):\n",
    "        return self.sim_data[:,0:self.t]\n",
    "           \n",
    "    def show_episode(self, freq, i_no):\n",
    "        ## Input:\n",
    "            ## Freq : frame rate (if freq = 5 one in 5 is shown)\n",
    "            ## i_no : iteration number \n",
    "        sim_data = self.sim_data[:,::freq]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes(xlim=(-5, 5), ylim=(0, sim_data[:,0][3] + 0.2))\n",
    "        text_str = \"iter - \" + str(i_no)\n",
    "        line, = ax.plot([], [], lw=3)\n",
    "        def init():\n",
    "            line.set_data([], [])\n",
    "            return line,\n",
    "        def animate(i):\n",
    "            x = sim_data[:,i][0]\n",
    "            y = sim_data[:,i][3]\n",
    "            u = sim_data[:,i][2]\n",
    "            line.set_data([u,x], [0,y])\n",
    "            return line,\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(0.05, 0.95, text_str, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "        \n",
    "        anim = FuncAnimation(fig, animate, init_func=init,\n",
    "                                       frames=np.shape(sim_data)[1], interval=25, blit=True)\n",
    "\n",
    "        plt.close(fig)\n",
    "        plt.close(anim._fig)\n",
    "        IPython.display.display_html(IPython.core.display.HTML(anim.to_html5_video()))\n",
    "\n",
    "    def compute_reward(self, step_time):\n",
    "        ## Computes the reward after step\n",
    "        r = 0\n",
    "        step_data = self.sim_data[:,int(self.t - step_time*1000):int(self.t)].copy()\n",
    "        step_data[0] = np.subtract(step_data[0], step_data[2])\n",
    "        min_dist = step_data[0].argmin()\n",
    "        if abs(step_data[0][min_dist]) < 0.002: ## min distance between COM and COP\n",
    "            r += 10\n",
    "        if abs(step_data[1][min_dist]) < 0.002 ## Min velocity when min dist is achieved\n",
    "            r += 1-\n",
    "        \n",
    "#         if self.sim_data[:,int(self.t - step_time*1000)-1][2] != self.sim_data[:,int(self.t - step_time*1000)][2]:\n",
    "#             r -= 1 ## penalises if step is taken\n",
    "#         else:\n",
    "#             r += 10 ## rewards if no step is taken\n",
    "            \n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This block samples and store data using epsillon greedy algorithm\n",
    "\n",
    "def sample_data(no_episodes, epi_t, h, action_set, value_function, show_episode = False):\n",
    "    # this function samples data\n",
    "    env = LipmEnv(h)\n",
    "    sample_data = []\n",
    "    for e in range(no_episodes):\n",
    "        print(\"running iter number - \" + str(e))\n",
    "        x = [0.0, 4*np.random.random() - 2]\n",
    "        u0 = action_set[np.random.randint(9)]\n",
    "        #espillon greedy\n",
    "        if np.random.random() > 0.2:\n",
    "            x_in = np.tile([u0 - x[0], x[1], 0],(len(action_set),1)) \n",
    "            x_in[:,2] = action_set\n",
    "            a = np.argmax(value_function(torch.tensor((x_in), dtype=torch.float)).cpu().detach().numpy())\n",
    "        else:\n",
    "            a = np.random.randint(9)\n",
    "        step_time = 0.15\n",
    "        env.reset_env(x, u0, epi_t)\n",
    "\n",
    "        ## sars_t = s_t, a_t, r_t, s_t+1\n",
    "        sars_t = np.zeros(6)\n",
    "        sars_t[0] = u0 - x[0]\n",
    "        sars_t[1] = x[1]\n",
    "        sars_t[2] = action_set[a]\n",
    "        for t in range(0, int(epi_t*1000) - 1):\n",
    "            if t % int(step_time * 1000) == 0 and t > 0:\n",
    "                sars_t[3] = env.compute_reward(step_time)\n",
    "                env.set_action(env.sim_data[:,env.t][0] + action_set[a]) ## setting action\n",
    "                sars_t[4] = env.sim_data[:,env.t][2] - env.sim_data[:,env.t][0]\n",
    "                sars_t[5] = env.sim_data[:,env.t][1]\n",
    "                sample_data.append(sars_t)\n",
    "                \n",
    "                sars_t = np.zeros(6)\n",
    "                sars_t[0] = env.sim_data[:,env.t][2] - env.sim_data[:,env.t][0]\n",
    "                sars_t[1] = env.sim_data[:,env.t][1]\n",
    "                # epsillon greedy\n",
    "                if np.random.random() > 0.2:\n",
    "                    x_in = [sars_t[0], sars_t[1], 0].copy()\n",
    "                    x_in = np.tile(x_in,((len(action_set),1)))\n",
    "                    x_in[:,2] = action_set\n",
    "                    a = np.argmax(value_function(torch.tensor((x_in), dtype=torch.float)).cpu().detach().numpy())\n",
    "                else:\n",
    "                    a = np.random.randint(9)\n",
    "    \n",
    "                sars_t[2] = action_set[a]\n",
    "        \n",
    "            env.step_env()\n",
    "    \n",
    "        if show_episode: \n",
    "            env.show_episode(5, e)\n",
    "            \n",
    "    ## shifting reward up since the reward\n",
    "    sample_data = np.asarray(sample_data)\n",
    "    sample_data[:,3] = np.roll(sample_data[:,3],-1)\n",
    "    sample_data = sample_data[0:-2]\n",
    "    return np.asarray(sample_data)\n",
    "\n",
    "def store_data(data_array, file_name, dir):\n",
    "    batch_no = str(len(os.listdir(dir)))\n",
    "    f = open(dir + file_name + \"_\" + batch_no + \".pkl\", 'wb')\n",
    "    print(\"dumping data ...\")\n",
    "    p.dump(data_array, f, -1)  \n",
    "    f.close()    \n",
    "    print(\"finished dumping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this block is for the Q function\n",
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, outputs):\n",
    "        super(ANN, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, 128)\n",
    "        self.l2 = nn.Linear(128, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.action_value = nn.Linear(128, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l1 = F.relu(self.l1(x))\n",
    "        l2 = F.relu(self.l2(l1))\n",
    "        l3 = F.relu(self.l3(l2))\n",
    "        return self.action_value(l3)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iter number - 0\n"
     ]
    }
   ],
   "source": [
    "## This block shows how data sampling is done\n",
    "device = torch.device(\"cpu\")\n",
    "dq_sampler = ANN(3, 1).to(device) \n",
    "## input to the ANN is u - x (u is cop, x is com location), xd, a_set(possible set of actions)\n",
    "action_set = np.linspace(-0.2, 0.2, 9)\n",
    "sample = sample_data(1, 1.5, 0.2, action_set, dq_sampler, False)\n",
    "# sample[0:20]\n",
    "## The simulation below shows stepping sequences using an epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iter number - 0\n",
      "running iter number - 1\n",
      "running iter number - 2\n",
      "running iter number - 3\n",
      "running iter number - 4\n",
      "running iter number - 5\n",
      "running iter number - 6\n",
      "running iter number - 7\n",
      "running iter number - 8\n",
      "running iter number - 9\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 440us/step - loss: 10.7768\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 130us/step - loss: 7.4323\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 119us/step - loss: 0.7739\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 124us/step - loss: 3.5958\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 121us/step - loss: 4.2210\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 136us/step - loss: 1.2287\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 147us/step - loss: 0.1813\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 122us/step - loss: 0.1106\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 125us/step - loss: 0.1104\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 134us/step - loss: 0.1088\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 130us/step - loss: 0.1174\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 126us/step - loss: 0.1113\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 120us/step - loss: 0.1020\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 124us/step - loss: 0.1061\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 106us/step - loss: 0.1040\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 82us/step - loss: 0.1018\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 109us/step - loss: 0.0965\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 116us/step - loss: 0.0959\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 130us/step - loss: 0.0953\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 133us/step - loss: 0.0961\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 128us/step - loss: 0.1037\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 142us/step - loss: 0.1039\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 137us/step - loss: 0.1038\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 117us/step - loss: 0.0902\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 117us/step - loss: 0.1007\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 135us/step - loss: 0.1066\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 119us/step - loss: 0.1300\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 118us/step - loss: 0.1266\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 117us/step - loss: 0.1376\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 130us/step - loss: 0.1136\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 122us/step - loss: 0.0932\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 119us/step - loss: 0.0931\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 115us/step - loss: 0.0893\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 110us/step - loss: 0.0894\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 131us/step - loss: 0.0902\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 133us/step - loss: 0.0886\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 122us/step - loss: 0.0876\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 143us/step - loss: 0.0956\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 118us/step - loss: 0.0932\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 104us/step - loss: 0.0901\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 113us/step - loss: 0.0911\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 124us/step - loss: 0.0889\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 127us/step - loss: 0.0905\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 80us/step - loss: 0.0964\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 106us/step - loss: 0.1069\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 118us/step - loss: 0.0932\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 95us/step - loss: 0.0934\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 109us/step - loss: 0.1000\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 136us/step - loss: 0.0875\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.059 - 0s 116us/step - loss: 0.0887\n"
     ]
    }
   ],
   "source": [
    "########### To be removed ###############################################################\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "sample = sample_data(10, 1.5, 0.2, action_set, dq_sampler, False)\n",
    "x_train = sample[:,0:3]\n",
    "y_train = sample[:,3]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=3, activation = 'relu'))\n",
    "model.add(Dense(128, activation= 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.8)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "metrics=['loss']\n",
    "history = model.fit(x_train, y_train, epochs = 50, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ameduri/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([88])) that is different to the input size (torch.Size([88, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9637, grad_fn=<MseLossBackward>)\n",
      "tensor(16.5409, grad_fn=<MseLossBackward>)\n",
      "tensor(9.8843, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1184, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1169, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1226, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1039, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0951, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0989, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1054, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1000, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0940, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0976, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0978, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0942, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0943, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0930, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0923, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0929, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0923, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0916, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0918, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0916, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0912, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0911, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0911, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0909, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0908, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0907, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0907, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0905, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0905, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0904, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0904, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0902, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0902, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0902, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0902, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "######################### To be removed #################################################\n",
    "\n",
    "ann = ANN(3, 1).to(device)\n",
    "optimizer = optim.SGD(ann.parameters(), lr= 0.005, momentum=0.8)\n",
    "x_train_tc = torch.from_numpy(x_train).float()\n",
    "y_train_tc = torch.from_numpy(y_train).float()\n",
    "\n",
    "for i in range(200):\n",
    "    prediction = ann(x_train_tc)\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.mse_loss(prediction, y_train_tc)\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block contains the deep Q stepper\n",
    "class DeepQStepper:\n",
    "    \n",
    "    def __init__(self, no_inputs, no_outputs, action_set, h, step_time):\n",
    "        '''\n",
    "        Input:\n",
    "            no_inputs : size of the feature vector into the ANN\n",
    "            no_outputs: Size of the output array from the ANN\n",
    "            action_set: The list of all possible actions\n",
    "            h : height of lipm above ground\n",
    "            step_time: the step time\n",
    "        '''\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        ## input to the ANN is u - x (u is cop, x is com location), xd, a_set(possible set of actions)\n",
    "        self.dq_stepper = ANN(no_inputs, no_outputs).to(self.device) \n",
    "        ## creating a target network to stabilize training\n",
    "        self.dq_stepper_tar = copy.deepcopy(self.dq_stepper)\n",
    "        self.action_set = action_set\n",
    "        self.h = h\n",
    "        self.step_time = step_time\n",
    "        self.dq_stepper_optimizer = optim.SGD(self.dq_stepper.parameters(), lr = 1e-3, momentum=0.8)\n",
    "        \n",
    "    def sample_data(self, no_episodes, epi_t, h, action_set, q_function, show_episode = False):\n",
    "        '''\n",
    "        This method samples data and returns the data in the SARS form [state, action, reward, state_t+1].\n",
    "        Input:\n",
    "            no_episodes : number of episodes of sample data\n",
    "            epi_t : duration of each episode\n",
    "            h : height of LIPM from the ground\n",
    "            action_set : the array of possible actions\n",
    "            q_function : the ANN that predicts value function given current state, action. Q(s,a)\n",
    "            show_episode : shows a simulation of the episode.\n",
    "        '''\n",
    "        # this function samples data\n",
    "        env = LipmEnv(h)\n",
    "        sample_data = []\n",
    "        for e in range(no_episodes):\n",
    "#             print(\"running iter number - \" + str(e))\n",
    "            x = [0.0, 4*np.random.random() - 2]\n",
    "            u0 = action_set[np.random.randint(9)]\n",
    "            env.reset_env(x, u0, epi_t)\n",
    "            #espillon greedy\n",
    "            x[0] = u0 - x[0]\n",
    "            if np.random.random() > 0.2:\n",
    "                a = self.compute_max_Q(x, action_set, q_function)[1]\n",
    "            else:\n",
    "                a = np.random.randint(9)\n",
    "\n",
    "            ## sars_t = s_t, a_t, r_t, s_t+1\n",
    "            sars_t = np.zeros(6)\n",
    "            sars_t[0] = u0 - x[0]\n",
    "            sars_t[1] = x[1]\n",
    "            sars_t[2] = action_set[a]\n",
    "            for t in range(0, int(epi_t*1000) - 1):\n",
    "                if t % int(self.step_time * 1000) == 0 and t > 0:\n",
    "                    sars_t[3] = env.compute_reward(self.step_time)\n",
    "                    env.set_action(env.sim_data[:,env.t][0] + action_set[a]) ## setting action\n",
    "                    sars_t[4] = env.sim_data[:,env.t][2] - env.sim_data[:,env.t][0]\n",
    "                    sars_t[5] = env.sim_data[:,env.t][1]\n",
    "                    sample_data.append(sars_t)\n",
    "\n",
    "                    sars_t = np.zeros(6)\n",
    "                    sars_t[0] = env.sim_data[:,env.t][2] - env.sim_data[:,env.t][0]\n",
    "                    sars_t[1] = env.sim_data[:,env.t][1]\n",
    "                    # epsillon greedy\n",
    "                    if np.random.random() > 0.2:\n",
    "                        a = self.compute_max_Q(sars_t[0:2], action_set, q_function)[1]\n",
    "                    else:\n",
    "                        a = np.random.randint(9)\n",
    "\n",
    "                    sars_t[2] = action_set[a]\n",
    "\n",
    "                env.step_env()\n",
    "\n",
    "            if show_episode: \n",
    "                env.show_episode(5, e)\n",
    "        \n",
    "        ## shifting reward up since the reward is obtained in the next step\n",
    "        ## TODO; Check is this matters\n",
    "        sample_data = np.asarray(sample_data)\n",
    "        sample_data[:,3] = np.roll(sample_data[:,3],-1)\n",
    "        sample_data = sample_data[0:-2]\n",
    "                        \n",
    "        return np.asarray(sample_data)\n",
    "        \n",
    "    def compute_max_Q(self, x, action_set, q_function):\n",
    "        '''\n",
    "        This function returns the max Q value for the given state for the set of possible actions.\n",
    "        It also returns the action that has maximum value\n",
    "        Input:\n",
    "            x : state to be evaluated\n",
    "            action_set : the array of possible actions\n",
    "            q_function: model to compute Q value\n",
    "        '''\n",
    "        x_in = [x[0], x[1], 0]\n",
    "        x_in = np.tile(x_in, ((len(action_set),1)))\n",
    "        x_in[:,2] = action_set\n",
    "        state_values = q_function(torch.FloatTensor(x_in)).cpu().detach().numpy()\n",
    "        a_opt = np.argmax(state_values)\n",
    "        q_opt = np.max(state_values)\n",
    "\n",
    "        return q_opt, a_opt\n",
    "    \n",
    "    def sample_mini_batch(self, buffer, batch_size):\n",
    "        '''\n",
    "        This function generates a mini batch by randomly selecting elements from the buffer. \n",
    "        It tries to keep atleast 10 elements that are from the latest sample to ensure fast convergence\n",
    "        Input:\n",
    "            buffer: buffer array\n",
    "            batch_size : size of the batch\n",
    "        '''\n",
    "        mini_batch = np.zeros((batch_size, np.shape(buffer)[1]))\n",
    "        for i in range(batch_size):\n",
    "            if i < batch_size - 10:\n",
    "                mini_batch[i] = buffer[np.random.randint(0, int(0.7*len(buffer)))]\n",
    "            else:\n",
    "                mini_batch[i] = buffer[np.random.randint(int(0.7*len(buffer)), int(len(buffer))-1)]\n",
    "                \n",
    "        return mini_batch\n",
    "        \n",
    "    def optimize(self, no_iter, no_episodes, epi_t, batch_size, gamma, tau):\n",
    "        '''\n",
    "        This method trains the deepQstepper model.\n",
    "        Input:\n",
    "            no_episodes: number of episodes worth of data to be stored in buffer\n",
    "            no_iter: number of iteration of training\n",
    "            epi_t: duration of one episode in seconds\n",
    "            batch_size: size of the batch\n",
    "            gamma : discount factor\n",
    "            tau: the rate at which the target q function is updated\n",
    "        '''\n",
    "        for i in range(no_iter):\n",
    "            if i == 0:\n",
    "                buffer = self.sample_data(no_episodes, epi_t, self.h, self.action_set, self.dq_stepper)\n",
    "            else:\n",
    "                new_sample = self.sample_data(1 , epi_t, self.h, self.action_set, self.dq_stepper)\n",
    "                buffer = np.concatenate((buffer, new_sample), axis = 0)\n",
    "                buffer = buffer[len(new_sample):]\n",
    "\n",
    "            mini_batch = self.sample_mini_batch(buffer, batch_size)\n",
    "            X_train = torch.from_numpy(mini_batch[:,0:3]).float()\n",
    "            Y_train = mini_batch[:,3]\n",
    "\n",
    "            for j in range(len(mini_batch)):\n",
    "                Y_train[j] += gamma*self.compute_max_Q(mini_batch[j][4:6], self.action_set, self.dq_stepper_tar)[0]\n",
    "            \n",
    "            Y_train = torch.from_numpy(np.reshape(Y_train,(len(Y_train), 1))).float()\n",
    "            \n",
    "            prediction = self.dq_stepper(X_train)\n",
    "            self.dq_stepper_optimizer.zero_grad()\n",
    "            loss = F.mse_loss(prediction, Y_train)\n",
    "            loss.backward()\n",
    "            self.dq_stepper_optimizer.step()\n",
    "            \n",
    "            for param, target_param in zip(self.dq_stepper.parameters(), self.dq_stepper_tar.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \n",
    "            \n",
    "            ## evaluating the performance of the deepq_stepper at the current iteration\n",
    "            reward = self.show_performance(epi_t)\n",
    "            \n",
    "#             print(\"running iteration number: \" + str(i), \"loss:\" + str(loss))\n",
    "            print(\"running iteration number: \" + str(i), \"reward:\" + str(reward))\n",
    "\n",
    "        return self.dq_stepper\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        '''\n",
    "        This saves the dq_stepper\n",
    "        '''\n",
    "        torch.save(self.dq_stepper.state_dict(), \"../../models/\" + file_name)\n",
    "    \n",
    "    def show_performance(self, epi_t, x0 = None, show_episode = False):\n",
    "        '''\n",
    "        This function returs the reward obtained from the episode using the stepper policy\n",
    "        It also returns a simulation of stepping using the policy\n",
    "        epi_t : episode time\n",
    "        x0 : initial state (position, velocity)\n",
    "        '''\n",
    "        env = LipmEnv(self.h)\n",
    "        if x0:\n",
    "            x = x0\n",
    "        else:\n",
    "            x = [0.0, 4*np.random.random() - 2]\n",
    "        u0 = action_set[np.random.randint(9)]\n",
    "        env.reset_env(x, u0, epi_t)\n",
    "        x[0] = u0 - x[0]\n",
    "        a = self.compute_max_Q(x, self.action_set, self.dq_stepper)[1]\n",
    "        episode_reward = 0\n",
    "        for t in range(0, int(epi_t*1000) - 1):\n",
    "            if t % int(self.step_time * 1000) == 0 and t > 0:\n",
    "                episode_reward += env.compute_reward(self.step_time) # compute reward\n",
    "                env.set_action(env.sim_data[:,env.t][0] + action_set[a]) ## setting action\n",
    "                x = env.sim_data[:,env.t][0:3].copy()\n",
    "                x[0] = x[2] - x[0]\n",
    "                x = x[0:2]\n",
    "                a = self.compute_max_Q(x, self.action_set, self.dq_stepper)[1]\n",
    "            env.step_env()\n",
    "        if show_episode:\n",
    "            env.show_episode(5, 0)        \n",
    "        return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration number: 0 reward:4.517857666911811\n",
      "running iteration number: 1 reward:2.6581387419993976\n",
      "running iteration number: 2 reward:1.4316709654567683\n",
      "running iteration number: 3 reward:3.5844069621155596\n",
      "running iteration number: 4 reward:4.7895519684714385\n",
      "running iteration number: 5 reward:3.155977457463562\n",
      "running iteration number: 6 reward:0.4283892220036243\n",
      "running iteration number: 7 reward:1.6565318582292734\n",
      "running iteration number: 8 reward:0.9439355036000096\n",
      "running iteration number: 9 reward:4.0007578809303705\n",
      "running iteration number: 10 reward:1.0787276476940255\n",
      "running iteration number: 11 reward:0.23896968581175485\n",
      "running iteration number: 12 reward:1.1946058431383904\n",
      "running iteration number: 13 reward:2.2364321118572246\n",
      "running iteration number: 14 reward:1.7737386909346073\n",
      "running iteration number: 15 reward:1.1990334183826854\n",
      "running iteration number: 16 reward:1.0733939703897537\n",
      "running iteration number: 17 reward:1.7255872962756005\n",
      "running iteration number: 18 reward:0.030706815776141833\n",
      "running iteration number: 19 reward:0.45535089277577645\n",
      "running iteration number: 20 reward:1.2925981005177403\n",
      "running iteration number: 21 reward:3.4981108142269126\n",
      "running iteration number: 22 reward:0.4774053234857719\n",
      "running iteration number: 23 reward:3.8699664656806427\n",
      "running iteration number: 24 reward:3.5881540396953553\n",
      "running iteration number: 25 reward:3.1086984819156234\n",
      "running iteration number: 26 reward:0.10875690485076661\n",
      "running iteration number: 27 reward:0.16896584579325563\n",
      "running iteration number: 28 reward:0.20455733018999833\n",
      "running iteration number: 29 reward:0.979832724565505\n",
      "running iteration number: 30 reward:3.9900376830203217\n",
      "running iteration number: 31 reward:0.6915796564226339\n",
      "running iteration number: 32 reward:1.0905052541482927\n",
      "running iteration number: 33 reward:2.0250439725192666\n",
      "running iteration number: 34 reward:3.811818652716244\n",
      "running iteration number: 35 reward:3.013017556107107\n",
      "running iteration number: 36 reward:0.7563936457504723\n",
      "running iteration number: 37 reward:0.23859413853824174\n",
      "running iteration number: 38 reward:1.5523670742855813\n",
      "running iteration number: 39 reward:0.19736492124179958\n",
      "running iteration number: 40 reward:3.5654524034108865\n",
      "running iteration number: 41 reward:1.2624873060493313\n",
      "running iteration number: 42 reward:3.4449321573624774\n",
      "running iteration number: 43 reward:3.2172859333940806\n",
      "running iteration number: 44 reward:1.000096525550968\n",
      "running iteration number: 45 reward:1.1109072540862461\n",
      "running iteration number: 46 reward:4.649337354276158\n",
      "running iteration number: 47 reward:0.47679695364930014\n",
      "running iteration number: 48 reward:0.5503005654703941\n",
      "running iteration number: 49 reward:1.2372897523879025\n",
      "running iteration number: 50 reward:1.7822103022655347\n",
      "running iteration number: 51 reward:0.8620419160887625\n",
      "running iteration number: 52 reward:1.9853573824842432\n",
      "running iteration number: 53 reward:3.0432529180872776\n",
      "running iteration number: 54 reward:3.558359981364689\n",
      "running iteration number: 55 reward:2.1905220854978045\n",
      "running iteration number: 56 reward:2.3600468410692312\n",
      "running iteration number: 57 reward:1.594286905590759\n",
      "running iteration number: 58 reward:0.8623343803087967\n",
      "running iteration number: 59 reward:3.9369915348806352\n",
      "running iteration number: 60 reward:3.264988338418596\n",
      "running iteration number: 61 reward:1.6371573264247612\n",
      "running iteration number: 62 reward:2.1597236299137803\n",
      "running iteration number: 63 reward:1.2911173441524397\n",
      "running iteration number: 64 reward:1.9310807816502802\n",
      "running iteration number: 65 reward:2.145401868034859\n",
      "running iteration number: 66 reward:4.904248476831688\n",
      "running iteration number: 67 reward:2.2697959768202223\n",
      "running iteration number: 68 reward:0.14203085040429017\n",
      "running iteration number: 69 reward:1.309527321172639\n",
      "running iteration number: 70 reward:2.766331309314963\n",
      "running iteration number: 71 reward:1.6801299457551586\n",
      "running iteration number: 72 reward:4.042341248144109\n",
      "running iteration number: 73 reward:0.5834493511457167\n",
      "running iteration number: 74 reward:0.6662977292152633\n",
      "running iteration number: 75 reward:0.863043722301118\n",
      "running iteration number: 76 reward:3.105387968858249\n",
      "running iteration number: 77 reward:3.1711567626279793\n",
      "running iteration number: 78 reward:4.6498150617627445\n",
      "running iteration number: 79 reward:1.4964258956380394\n",
      "running iteration number: 80 reward:0.056004113892235884\n",
      "running iteration number: 81 reward:0.4597080730755897\n",
      "running iteration number: 82 reward:1.5533352735419315\n",
      "running iteration number: 83 reward:3.5864492275043114\n",
      "running iteration number: 84 reward:0.7218275130424886\n",
      "running iteration number: 85 reward:2.7011669511046468\n",
      "running iteration number: 86 reward:3.094774452891734\n",
      "running iteration number: 87 reward:0.44386148926004576\n",
      "running iteration number: 88 reward:3.6176625269019937\n",
      "running iteration number: 89 reward:0.7205742735278109\n",
      "running iteration number: 90 reward:0.4167051827000595\n",
      "running iteration number: 91 reward:4.161222493730345\n",
      "running iteration number: 92 reward:1.226580639274828\n",
      "running iteration number: 93 reward:1.919449637276618\n",
      "running iteration number: 94 reward:2.3342261410412664\n",
      "running iteration number: 95 reward:3.957362171284763\n",
      "running iteration number: 96 reward:2.3063378587331447\n",
      "running iteration number: 97 reward:2.6985467893568202\n",
      "running iteration number: 98 reward:3.548427954751234\n",
      "running iteration number: 99 reward:0.19184840738058737\n",
      "running iteration number: 100 reward:1.2538121457313236\n",
      "running iteration number: 101 reward:2.2916841874762226\n",
      "running iteration number: 102 reward:3.842455867346667\n",
      "running iteration number: 103 reward:1.485796965077944\n",
      "running iteration number: 104 reward:3.859692442354571\n",
      "running iteration number: 105 reward:1.361176028489204\n",
      "running iteration number: 106 reward:0.5079330861123502\n",
      "running iteration number: 107 reward:0.12228766329910634\n",
      "running iteration number: 108 reward:3.5314282190433057\n",
      "running iteration number: 109 reward:3.21161517653167\n",
      "running iteration number: 110 reward:3.4669718665430356\n",
      "running iteration number: 111 reward:0.24658006107950617\n",
      "running iteration number: 112 reward:1.8966129321175633\n",
      "running iteration number: 113 reward:0.4222807893178045\n",
      "running iteration number: 114 reward:0.6506606392774864\n",
      "running iteration number: 115 reward:0.07545517506043666\n",
      "running iteration number: 116 reward:2.839189404335402\n",
      "running iteration number: 117 reward:1.5733040645188476\n",
      "running iteration number: 118 reward:0.25023639836122\n",
      "running iteration number: 119 reward:2.5129480392824086\n",
      "running iteration number: 120 reward:0.7030502143102895\n",
      "running iteration number: 121 reward:3.751242170209645\n",
      "running iteration number: 122 reward:0.5259250966138422\n",
      "running iteration number: 123 reward:1.1933365971958594\n",
      "running iteration number: 124 reward:2.561624778041741\n",
      "running iteration number: 125 reward:2.450174358985233\n",
      "running iteration number: 126 reward:2.1338733147966313\n",
      "running iteration number: 127 reward:0.4325206834939413\n",
      "running iteration number: 128 reward:5.134281374246528\n",
      "running iteration number: 129 reward:0.47793033391458245\n",
      "running iteration number: 130 reward:3.4953330343518934\n",
      "running iteration number: 131 reward:3.065138938001265\n",
      "running iteration number: 132 reward:1.23240911923066\n",
      "running iteration number: 133 reward:4.6764152453662735\n",
      "running iteration number: 134 reward:3.1278743416007178\n",
      "running iteration number: 135 reward:3.2287701306097305\n",
      "running iteration number: 136 reward:3.99183478060527\n",
      "running iteration number: 137 reward:2.3894726885527864\n",
      "running iteration number: 138 reward:0.5520388408496583\n",
      "running iteration number: 139 reward:0.3043518104196544\n",
      "running iteration number: 140 reward:0.5391770533525508\n",
      "running iteration number: 141 reward:0.7168583955542002\n",
      "running iteration number: 142 reward:0.973728782104556\n",
      "running iteration number: 143 reward:0.011237999621143257\n",
      "running iteration number: 144 reward:2.5791162332577335\n",
      "running iteration number: 145 reward:0.16286130132235693\n",
      "running iteration number: 146 reward:1.2300661670157587\n",
      "running iteration number: 147 reward:0.6860398178310034\n",
      "running iteration number: 148 reward:0.17303271288421757\n",
      "running iteration number: 149 reward:1.5468647453814917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration number: 150 reward:2.962869897364556\n",
      "running iteration number: 151 reward:4.659165059682833\n",
      "running iteration number: 152 reward:0.05446605131082827\n",
      "running iteration number: 153 reward:3.9286962829432963\n",
      "running iteration number: 154 reward:0.4981255949616406\n",
      "running iteration number: 155 reward:5.002613052464532\n",
      "running iteration number: 156 reward:0.04913118861680444\n",
      "running iteration number: 157 reward:0.43720390537850523\n",
      "running iteration number: 158 reward:0.4239252736860575\n",
      "running iteration number: 159 reward:3.430803785857892\n",
      "running iteration number: 160 reward:0.7621691076203375\n",
      "running iteration number: 161 reward:4.915602406634688\n",
      "running iteration number: 162 reward:2.680757811123787\n",
      "running iteration number: 163 reward:2.468399741522611\n",
      "running iteration number: 164 reward:4.080193310871913\n",
      "running iteration number: 165 reward:0.7156761458587519\n",
      "running iteration number: 166 reward:0.8735113186645433\n",
      "running iteration number: 167 reward:3.6485878689926556\n",
      "running iteration number: 168 reward:3.839236431210899\n",
      "running iteration number: 169 reward:0.03750225117269045\n",
      "running iteration number: 170 reward:1.2920204397412747\n",
      "running iteration number: 171 reward:0.901591748268368\n",
      "running iteration number: 172 reward:0.9372779336300192\n",
      "running iteration number: 173 reward:0.8651288189306081\n",
      "running iteration number: 174 reward:3.1777359881213325\n",
      "running iteration number: 175 reward:3.2154099921826425\n",
      "running iteration number: 176 reward:4.555511466977994\n",
      "running iteration number: 177 reward:0.9020819027883871\n",
      "running iteration number: 178 reward:0.23951588459298812\n",
      "running iteration number: 179 reward:0.6774420896474882\n",
      "running iteration number: 180 reward:3.881838456583029\n",
      "running iteration number: 181 reward:0.039677535790884115\n",
      "running iteration number: 182 reward:2.3017865039846326\n",
      "running iteration number: 183 reward:2.53428621798845\n",
      "running iteration number: 184 reward:3.370675944740498\n",
      "running iteration number: 185 reward:1.9516218059487012\n",
      "running iteration number: 186 reward:3.0447280300052593\n",
      "running iteration number: 187 reward:4.679910111786532\n",
      "running iteration number: 188 reward:0.4755850093507298\n",
      "running iteration number: 189 reward:4.080671171819442\n",
      "running iteration number: 190 reward:3.905665496869028\n",
      "running iteration number: 191 reward:0.07396846105911131\n",
      "running iteration number: 192 reward:0.9563072028488012\n",
      "running iteration number: 193 reward:3.966339106634727\n",
      "running iteration number: 194 reward:1.9352957557150874\n",
      "running iteration number: 195 reward:0.9719454737676567\n",
      "running iteration number: 196 reward:0.7531259463007738\n",
      "running iteration number: 197 reward:2.8033792583094015\n",
      "running iteration number: 198 reward:1.392683346858168\n",
      "running iteration number: 199 reward:2.7116354286530804\n",
      "running iteration number: 200 reward:0.5227750420782091\n",
      "running iteration number: 201 reward:3.672412080754337\n",
      "running iteration number: 202 reward:2.6285831798407058\n",
      "running iteration number: 203 reward:0.3777470272018257\n",
      "running iteration number: 204 reward:0.8928724963676408\n",
      "running iteration number: 205 reward:0.8431726716464513\n",
      "running iteration number: 206 reward:1.7333088223060606\n",
      "running iteration number: 207 reward:1.7349479564467873\n",
      "running iteration number: 208 reward:1.0716582558950147\n",
      "running iteration number: 209 reward:0.4969257842369667\n",
      "running iteration number: 210 reward:0.0754919253920074\n",
      "running iteration number: 211 reward:2.4979456862641696\n",
      "running iteration number: 212 reward:1.999677562104001\n",
      "running iteration number: 213 reward:0.15312297368359754\n",
      "running iteration number: 214 reward:2.4838726169831222\n",
      "running iteration number: 215 reward:1.9115904763183635\n",
      "running iteration number: 216 reward:1.894638986173448\n",
      "running iteration number: 217 reward:3.966959022874638\n",
      "running iteration number: 218 reward:1.7917070776684194\n",
      "running iteration number: 219 reward:2.660590794162139\n",
      "running iteration number: 220 reward:0.7203735493121577\n",
      "running iteration number: 221 reward:0.7702748339203869\n",
      "running iteration number: 222 reward:2.228774207521253\n",
      "running iteration number: 223 reward:1.5123674939772287\n",
      "running iteration number: 224 reward:0.5375861912053833\n",
      "running iteration number: 225 reward:1.5863430349851864\n",
      "running iteration number: 226 reward:1.8885176429918813\n",
      "running iteration number: 227 reward:0.05801834990210301\n",
      "running iteration number: 228 reward:1.1719661232861553\n",
      "running iteration number: 229 reward:0.08667095509618652\n",
      "running iteration number: 230 reward:3.0219791281852615\n",
      "running iteration number: 231 reward:1.5623072009166887\n",
      "running iteration number: 232 reward:5.135309512597858\n",
      "running iteration number: 233 reward:3.2632148015483953\n",
      "running iteration number: 234 reward:0.25129488660864535\n",
      "running iteration number: 235 reward:1.9645057565014665\n",
      "running iteration number: 236 reward:1.641867575446033\n",
      "running iteration number: 237 reward:3.1246267079694\n",
      "running iteration number: 238 reward:1.0547796182577271\n",
      "running iteration number: 239 reward:0.3528241302425436\n",
      "running iteration number: 240 reward:3.9371602080474823\n",
      "running iteration number: 241 reward:1.4839450253578013\n",
      "running iteration number: 242 reward:0.8828148308560623\n",
      "running iteration number: 243 reward:2.051876469031666\n",
      "running iteration number: 244 reward:2.1958457800329265\n",
      "running iteration number: 245 reward:1.100019563078017\n",
      "running iteration number: 246 reward:0.91054213120299\n",
      "running iteration number: 247 reward:2.3918475176498455\n",
      "running iteration number: 248 reward:4.134012682535072\n",
      "running iteration number: 249 reward:0.5408868562575927\n",
      "running iteration number: 250 reward:0.27795297586622386\n",
      "running iteration number: 251 reward:1.470795336539375\n",
      "running iteration number: 252 reward:0.04097732838996926\n",
      "running iteration number: 253 reward:1.9135322327472553\n",
      "running iteration number: 254 reward:0.6313126684301649\n",
      "running iteration number: 255 reward:1.087775458000837\n",
      "running iteration number: 256 reward:1.5611453559699229\n",
      "running iteration number: 257 reward:1.423917576500962\n",
      "running iteration number: 258 reward:0.3939828180936558\n",
      "running iteration number: 259 reward:0.9372434263079125\n",
      "running iteration number: 260 reward:0.1671798310327804\n",
      "running iteration number: 261 reward:2.8603714987630693\n",
      "running iteration number: 262 reward:0.23041353555908964\n",
      "running iteration number: 263 reward:0.030225417797872422\n",
      "running iteration number: 264 reward:2.260088726376489\n",
      "running iteration number: 265 reward:1.613499176970874\n",
      "running iteration number: 266 reward:0.05515425658269451\n",
      "running iteration number: 267 reward:1.9352636507135976\n",
      "running iteration number: 268 reward:4.5964461867635\n",
      "running iteration number: 269 reward:0.8333262107095264\n",
      "running iteration number: 270 reward:2.0643152204724156\n",
      "running iteration number: 271 reward:0.8109409596756605\n",
      "running iteration number: 272 reward:0.8445078353252957\n",
      "running iteration number: 273 reward:3.0339064500202326\n",
      "running iteration number: 274 reward:3.938315306299894\n",
      "running iteration number: 275 reward:0.19190199656017673\n",
      "running iteration number: 276 reward:1.2089273619929497\n",
      "running iteration number: 277 reward:3.8505370729311506\n",
      "running iteration number: 278 reward:0.1444321020524471\n",
      "running iteration number: 279 reward:1.635514104147113\n",
      "running iteration number: 280 reward:0.6076434295950667\n",
      "running iteration number: 281 reward:4.018696072587205\n",
      "running iteration number: 282 reward:0.9818277461267211\n",
      "running iteration number: 283 reward:1.1733311494882108\n",
      "running iteration number: 284 reward:1.2807887189941654\n",
      "running iteration number: 285 reward:1.0629653608470415\n",
      "running iteration number: 286 reward:0.7814926816132165\n",
      "running iteration number: 287 reward:1.6262346586647443\n",
      "running iteration number: 288 reward:1.8643232510763958\n",
      "running iteration number: 289 reward:4.653946753265432\n",
      "running iteration number: 290 reward:3.6400030288992116\n",
      "running iteration number: 291 reward:0.08738287778548261\n",
      "running iteration number: 292 reward:3.4121266108210735\n",
      "running iteration number: 293 reward:1.3550125685060377\n",
      "running iteration number: 294 reward:1.558255040026158\n",
      "running iteration number: 295 reward:4.794966812802642\n",
      "running iteration number: 296 reward:4.192933630605079\n",
      "running iteration number: 297 reward:2.2034777420483076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration number: 298 reward:2.330364842680023\n",
      "running iteration number: 299 reward:3.1163817358260086\n",
      "running iteration number: 300 reward:0.6020419621525317\n",
      "running iteration number: 301 reward:1.329949456880484\n",
      "running iteration number: 302 reward:2.887499222704308\n",
      "running iteration number: 303 reward:2.88094047068066\n",
      "running iteration number: 304 reward:0.8129983811010496\n",
      "running iteration number: 305 reward:1.5821864328045345\n",
      "running iteration number: 306 reward:1.0636391741649276\n",
      "running iteration number: 307 reward:3.1180763001000744\n",
      "running iteration number: 308 reward:0.8901005413651174\n",
      "running iteration number: 309 reward:1.7305057856576687\n",
      "running iteration number: 310 reward:3.919517190568291\n",
      "running iteration number: 311 reward:2.04298415877181\n",
      "running iteration number: 312 reward:0.5929135842942075\n",
      "running iteration number: 313 reward:0.5458302237392656\n",
      "running iteration number: 314 reward:5.06190473929603\n",
      "running iteration number: 315 reward:2.1818380831493043\n",
      "running iteration number: 316 reward:1.2775033425538997\n",
      "running iteration number: 317 reward:0.5181254961070095\n",
      "running iteration number: 318 reward:0.5932422904044601\n",
      "running iteration number: 319 reward:5.040670853594264\n",
      "running iteration number: 320 reward:2.6627996050759006\n",
      "running iteration number: 321 reward:1.6838483240545907\n",
      "running iteration number: 322 reward:3.126913920177474\n",
      "running iteration number: 323 reward:4.0752696758721605\n",
      "running iteration number: 324 reward:0.12151071803445414\n",
      "running iteration number: 325 reward:2.7123629241049803\n",
      "running iteration number: 326 reward:0.14644145736497155\n",
      "running iteration number: 327 reward:2.455821506471955\n",
      "running iteration number: 328 reward:1.768544712951864\n",
      "running iteration number: 329 reward:2.9183742066877563\n",
      "running iteration number: 330 reward:3.4556606533217176\n",
      "running iteration number: 331 reward:2.119661484106279\n",
      "running iteration number: 332 reward:3.949409049599601\n",
      "running iteration number: 333 reward:4.533731581641466\n",
      "running iteration number: 334 reward:2.7440453659997264\n",
      "running iteration number: 335 reward:4.970302994968857\n",
      "running iteration number: 336 reward:0.11241568043919271\n",
      "running iteration number: 337 reward:1.2022990447253445\n",
      "running iteration number: 338 reward:3.445902912166661\n",
      "running iteration number: 339 reward:3.8360235228462485\n",
      "running iteration number: 340 reward:0.424153291803317\n",
      "running iteration number: 341 reward:1.120014861694975\n",
      "running iteration number: 342 reward:1.3801217871240792\n",
      "running iteration number: 343 reward:4.460154813041845\n",
      "running iteration number: 344 reward:3.7123881016178735\n",
      "running iteration number: 345 reward:3.9821862504890913\n",
      "running iteration number: 346 reward:3.2926617359481827\n",
      "running iteration number: 347 reward:3.0855546811867525\n",
      "running iteration number: 348 reward:1.9566153604328609\n",
      "running iteration number: 349 reward:2.3737681473415657\n",
      "running iteration number: 350 reward:5.124378650275956\n",
      "running iteration number: 351 reward:0.45168220745757437\n",
      "running iteration number: 352 reward:1.1260717861890615\n",
      "running iteration number: 353 reward:0.17294414856845927\n",
      "running iteration number: 354 reward:4.143325743690233\n",
      "running iteration number: 355 reward:3.4968944313501984\n",
      "running iteration number: 356 reward:1.3037599542302474\n",
      "running iteration number: 357 reward:0.38876542607991704\n",
      "running iteration number: 358 reward:0.6180584648603363\n",
      "running iteration number: 359 reward:0.1029598678159834\n",
      "running iteration number: 360 reward:1.9732564168052844\n",
      "running iteration number: 361 reward:1.383893319054317\n",
      "running iteration number: 362 reward:0.8137644041709076\n",
      "running iteration number: 363 reward:0.6075827879178647\n",
      "running iteration number: 364 reward:2.5536722277442263\n",
      "running iteration number: 365 reward:2.0674720311351074\n",
      "running iteration number: 366 reward:4.089400782937382\n",
      "running iteration number: 367 reward:2.3058480255875695\n",
      "running iteration number: 368 reward:1.3106616848991102\n",
      "running iteration number: 369 reward:5.902173867439462\n",
      "running iteration number: 370 reward:3.8390145083678866\n",
      "running iteration number: 371 reward:1.9848609119287632\n",
      "running iteration number: 372 reward:2.250229156791804\n",
      "running iteration number: 373 reward:1.7939019959648117\n",
      "running iteration number: 374 reward:0.5829065440530937\n",
      "running iteration number: 375 reward:2.9049998940415707\n",
      "running iteration number: 376 reward:2.1899613195780554\n",
      "running iteration number: 377 reward:2.525223923302663\n",
      "running iteration number: 378 reward:0.7925976791390762\n",
      "running iteration number: 379 reward:0.1432080108259339\n",
      "running iteration number: 380 reward:1.0820848437276627\n",
      "running iteration number: 381 reward:0.9001652581908524\n",
      "running iteration number: 382 reward:0.2660164628281682\n",
      "running iteration number: 383 reward:4.960097083987878\n",
      "running iteration number: 384 reward:1.0982319393648434\n",
      "running iteration number: 385 reward:2.922506145207313\n",
      "running iteration number: 386 reward:0.14549972730687855\n",
      "running iteration number: 387 reward:3.512007325910694\n",
      "running iteration number: 388 reward:1.0183669435876357\n",
      "running iteration number: 389 reward:0.10702866300075622\n",
      "running iteration number: 390 reward:4.51282581501513\n",
      "running iteration number: 391 reward:1.7667121540560802\n",
      "running iteration number: 392 reward:0.5255334693253234\n",
      "running iteration number: 393 reward:0.7348198590294155\n",
      "running iteration number: 394 reward:3.3909757601547352\n",
      "running iteration number: 395 reward:1.1153474698854267\n",
      "running iteration number: 396 reward:0.716898379676138\n",
      "running iteration number: 397 reward:0.08090473310136222\n",
      "running iteration number: 398 reward:3.6331473246499133\n",
      "running iteration number: 399 reward:0.568410506178805\n",
      "running iteration number: 400 reward:0.22988367833875772\n",
      "running iteration number: 401 reward:2.477980304469887\n",
      "running iteration number: 402 reward:3.726230876039173\n",
      "running iteration number: 403 reward:3.5748030098892984\n",
      "running iteration number: 404 reward:0.24089803961567124\n",
      "running iteration number: 405 reward:3.526652319084302\n",
      "running iteration number: 406 reward:1.7067753525604026\n",
      "running iteration number: 407 reward:0.04623989888262813\n",
      "running iteration number: 408 reward:5.142013831023497\n",
      "running iteration number: 409 reward:0.8550398865876285\n",
      "running iteration number: 410 reward:0.6804258989364189\n",
      "running iteration number: 411 reward:0.858423053421282\n",
      "running iteration number: 412 reward:0.24712921066241822\n",
      "running iteration number: 413 reward:0.14144836900332033\n",
      "running iteration number: 414 reward:2.566597646317103\n",
      "running iteration number: 415 reward:0.1566304715434601\n",
      "running iteration number: 416 reward:0.7677070603669554\n",
      "running iteration number: 417 reward:4.523088914516749\n",
      "running iteration number: 418 reward:1.4877932937921903\n",
      "running iteration number: 419 reward:2.7408268321344265\n",
      "running iteration number: 420 reward:0.6736296748684836\n",
      "running iteration number: 421 reward:0.056943960144582265\n",
      "running iteration number: 422 reward:1.3322478979641779\n",
      "running iteration number: 423 reward:2.2520317129807026\n",
      "running iteration number: 424 reward:0.17238567876590477\n",
      "running iteration number: 425 reward:0.7621132091193057\n",
      "running iteration number: 426 reward:2.0273110046466716\n",
      "running iteration number: 427 reward:2.735425995673768\n",
      "running iteration number: 428 reward:3.389294395180406\n",
      "running iteration number: 429 reward:1.0020281666092876\n",
      "running iteration number: 430 reward:0.6125193899195919\n",
      "running iteration number: 431 reward:0.6561401484495935\n",
      "running iteration number: 432 reward:2.785842969797257\n",
      "running iteration number: 433 reward:5.140206277351176\n",
      "running iteration number: 434 reward:4.001233144506409\n",
      "running iteration number: 435 reward:1.867095206804754\n",
      "running iteration number: 436 reward:0.028910342857949255\n",
      "running iteration number: 437 reward:1.4235541387748754\n",
      "running iteration number: 438 reward:1.7689262372679335\n",
      "running iteration number: 439 reward:4.193067019822234\n",
      "running iteration number: 440 reward:1.8030322583619436\n",
      "running iteration number: 441 reward:2.227369328327008\n",
      "running iteration number: 442 reward:0.7934664194019833\n",
      "running iteration number: 443 reward:1.5070010788035841\n",
      "running iteration number: 444 reward:1.6986755908393074\n",
      "running iteration number: 445 reward:3.7197712882902225\n",
      "running iteration number: 446 reward:0.3858577524135994\n",
      "running iteration number: 447 reward:0.70436644154079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration number: 448 reward:3.848237976559892\n",
      "running iteration number: 449 reward:0.6400756199610619\n",
      "running iteration number: 450 reward:0.14117289045135467\n",
      "running iteration number: 451 reward:1.431190695590875\n",
      "running iteration number: 452 reward:2.910598834210951\n",
      "running iteration number: 453 reward:1.5495797887070133\n",
      "running iteration number: 454 reward:4.072959372525693\n",
      "running iteration number: 455 reward:2.054934684615725\n",
      "running iteration number: 456 reward:1.8883970843390538\n",
      "running iteration number: 457 reward:0.2894897582790384\n",
      "running iteration number: 458 reward:1.5270487796967582\n",
      "running iteration number: 459 reward:5.540410126894028\n",
      "running iteration number: 460 reward:2.657509645049643\n",
      "running iteration number: 461 reward:0.8730073663643574\n",
      "running iteration number: 462 reward:5.067692998655339\n",
      "running iteration number: 463 reward:1.2871639036111897\n",
      "running iteration number: 464 reward:1.6673002070878509\n",
      "running iteration number: 465 reward:1.1644695988302218\n",
      "running iteration number: 466 reward:3.5006910375744438\n",
      "running iteration number: 467 reward:2.677454970473828\n",
      "running iteration number: 468 reward:2.7244796909231748\n",
      "running iteration number: 469 reward:1.5934807685881096\n",
      "running iteration number: 470 reward:2.6464445017530647\n",
      "running iteration number: 471 reward:3.789329836721657\n",
      "running iteration number: 472 reward:3.678833670557877\n",
      "running iteration number: 473 reward:2.7608908229907976\n",
      "running iteration number: 474 reward:0.699349652178443\n",
      "running iteration number: 475 reward:0.9879633050574028\n",
      "running iteration number: 476 reward:3.8462500961458335\n",
      "running iteration number: 477 reward:1.3586265661471595\n",
      "running iteration number: 478 reward:3.4369487861697463\n",
      "running iteration number: 479 reward:0.17083368690885087\n",
      "running iteration number: 480 reward:3.886661199040346\n",
      "running iteration number: 481 reward:3.954798683399344\n",
      "running iteration number: 482 reward:2.0656181181971376\n",
      "running iteration number: 483 reward:3.784235242036647\n",
      "running iteration number: 484 reward:1.7836638691406685\n",
      "running iteration number: 485 reward:2.519773550515988\n",
      "running iteration number: 486 reward:4.565312176723837\n",
      "running iteration number: 487 reward:0.5164336806294083\n",
      "running iteration number: 488 reward:0.024262367889004814\n",
      "running iteration number: 489 reward:3.272778456210513\n",
      "running iteration number: 490 reward:1.9576365721212847\n",
      "running iteration number: 491 reward:0.424229916478422\n",
      "running iteration number: 492 reward:3.2500880971241712\n",
      "running iteration number: 493 reward:0.9638866035254554\n",
      "running iteration number: 494 reward:2.9232294707444675\n",
      "running iteration number: 495 reward:1.2555768310589455\n",
      "running iteration number: 496 reward:1.6698453014991708\n",
      "running iteration number: 497 reward:1.0379286185958507\n",
      "running iteration number: 498 reward:5.924468718231065\n",
      "running iteration number: 499 reward:3.7723538497930957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (l1): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (l2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (action_value): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this block is for training the dq_stepper\n",
    "action_set = np.linspace(-0.15, 0.15, 9)\n",
    "dq_stepper = DeepQStepper(3, 1, action_set, 0.2, 0.15)\n",
    "dq_stepper.optimize(500, 2, 1.5, 32, 0.8, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"432\" height=\"288\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAA7rm1kYXQAAAKuBgX//6rcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTUyIHIyODU0IGU5YTU5MDMgLSBILjI2NC9NUEVHLTQg\n",
       "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE3IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
       "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
       "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
       "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
       "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9OSBsb29r\n",
       "YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n",
       "ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n",
       "bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n",
       "aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n",
       "aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n",
       "cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAtWZYiE\n",
       "ADP//vbsvgU1/Z/QlxEsxdpKcD4qpICAdzTAAAADAAB5FfwTZP1ApEOAKLGJP0SpUh2XxRDdCJTc\n",
       "ZzZ4v4cZYQuR+HDQoeqd8MgYlM90eLyIShBlFOfsfeC3y6ZMB6mIjbVgfgAAAwCHrYt4WrCE2FYv\n",
       "soOKL18FFh6KEO0+xpWmSScXj1JLKqArswGLk2qWuFWZ9VDPZBMhWASqGjdGSGAKi7OT8kWUDOCa\n",
       "JMn6zKbjXSR06XShN15KxK0szcoGeRpx+vJZZfOx/943m+4cMLBNZZyPR1p/0c8DzanAmAh4Ye5L\n",
       "aixpPshaN0s28EtyC2y8zN5ycW6mGS4ZGMZlAkWZ0zhZ4kA/pKqRb8z/I325jP/ZTDplYHAjvM8P\n",
       "JPmYXM0VDSMw8/8wbtsnsKYIOnmzqfnVUcXKZH41BbJZSx1pK2FU5dh+8cI9QmPl40p2a4HKcuXe\n",
       "q+LEF/Ty5Sxt454akmZcTeAQ6RzQ1nyRRFNMAAkbmYnrCqjbGdyd5mbe387JseLElE5do6VwSkR+\n",
       "cm6WTEdTAX9jVfXct1ZWquJ0aN5k0GHL+5rY3MuPx+QYq9GcIyx6ZOGBwOeZ8ioUxOaDEQV7xzTC\n",
       "1emR3f7z25TIIxEc+dkFXHwoNLiKANiEW8FCbV8m7XsGDiY2RT8FswAAc35OodQ9kM7gtf6DKaba\n",
       "x1Tw2gm8J2C9zHFPQHHM793Mal0X0QXCOODYJP67CDfYSoRq21/s8bJB3qHXbCyU7dpw3vBwIV4f\n",
       "Pcu+tGTIU7uS0GMiDSRDmCy9R75hRbMvd0bqjqbt45DHYGTk8gxehJ9I0Nt1IwjCGTNFQUVBJI2v\n",
       "0J2F7WYYIIz3WKKfXbXZr6urM8QAIAF7K5e8thniK1li/H/NrHPPQLvSxDmKLojWHP1AztJIrfQb\n",
       "6wbl6K7Ofthl1hX4BBidAzxcTEPi3D6GXT4EaoUWa/a51dwPH4wUKuvZShw45PHtMeLrpFV4r6EL\n",
       "MJVLkyvpNDaAH+S41Kf0WQ/xQvRTL4Gl/eOXXzUX2n7tZ3NiCOxMAyYyP1XeQn8x0fXzuedOoxgu\n",
       "cj1BEduebkeehg/+Hg2k0CxJgAAAAwDgZXE1liTIBGOwY17OzLDuuULynslVuXm014AysWP1TroU\n",
       "PcX9ZD73ORV8BRlPCQucyjZ//kRXdLFWerbyujah8L4mdV+6l4JJFrkAPXR///+5nJ0PPYI9N7sZ\n",
       "TWA+ZWX99tzuRj16AC+lwoAAdRKJ/mMCLrKsL/iu7MOXNQYeZVC1jNWtryAAziMK9d6IoN4uTQiW\n",
       "7OKQbqXFl/PqM/Yb8vLs1/tVY885BCPzT3xoX0G75Sl2ehiF+zns4bCqRznMJL/u9ocZKDSvnnPy\n",
       "FEO0Fain+1irEwml5fQuiOgm8VjtmXR9/dtq8HdeqLFDtODxtVCHTLFLTHVhzBhGdIwDqhxFHeXO\n",
       "kFfdUp7j4x0RldX3Xg+lSqo6C6qtN7Kye5ns2FbzjcABOEBBL9AE1U2oucDCgFR6J2xyd+DvDlcJ\n",
       "Ms4W+bjjiG2Jl/iwjRegNmQAF60ifBopLg3/m/6UaIYWrmB57//DGdwRGHmof1G0YCV+vcMt5auB\n",
       "GuQqYo0WZ7+VhcJcgyqk3ULM410SvNCUFJHy93Hu7TRjXkjrGAo2m7rEfBYH00Np2vsxJHcX8q47\n",
       "k80KV5sWWLAdpPWgztNUv5EnJvE4oEEiGAdSgcBIOM3hRW3RSW9mcM5IGDHdOJFPAfZI8Gh6gh6H\n",
       "b7ljD/MDRS+oYZSJZCNNlVE5e4Oj5ZSJW7MI0TmDg7915Jm+u6aDPLLQCZSMjXoYB4tWM95Cg2Oj\n",
       "Jr/zP7VtkguK7rCAKwjBut39oBvJPJa2p2OIwbkySi/wmG40SmaTh3gSzdE/KdNgePJzimPFtKB2\n",
       "K/9hgfYZicY7ERbJ7HHCKFhcyVJqtwEBpAmXSwLuEAcwbI7RrXG+hfmnPaw7SXMn9FMFu4Qd0XSS\n",
       "n08DOaTEYwYaGFywm19sLCbRawZPESQVLF9iaPtEf0MapTvvXyi3uEXQuLCxJC7zoFuvaOpnDXAQ\n",
       "KhXPZ+99dhKwEyh4bLruxXLKaYvkuG2NWbHQtgJGM7bR+7NKB0N0aLHV/Io5/8d9sazJ8b+TmcAW\n",
       "WW3qNG7sp67yD08EkpputxR1Q/DXMLtRX7mtc9kNuy3PCQ/5AAJMeJbV3bEE96b//PyM4mpXa6fv\n",
       "bnH5vG7N1OfGnBADvD8pULcId8h1JeWQoQl44IcgwcT+bJL9C/XVsle4468F9pNqx+WihrYgv5Nv\n",
       "6MysqOO5vYT6/BjIwtvnxDteDLlTbXADL7IZq0yAE4fvbI73AE8QocZa3p9gl8IAIZY8ELkOdbG6\n",
       "X/MxQHXBYN/gidReOMnmmJLS2JSlEnq+9SmlYlyEZYzltxhP12rv1iEtXzT2DLHQgmNx4tLsWCJ/\n",
       "Iqd65jC8uZnO0D3g9yU5hssAuZ/nFJGw146x32WAUw6Q1czNZXpR+6DaJocSdy4+dOl0LT0sA83x\n",
       "PtFtCtXXycJD+/9P7xeU2Hh7ddjGjY/EXbUSfyEp/zXgbJHFwbE9dInV/IC6e3qqhhgY6bK4ihS1\n",
       "nJtgTFnf5cJJUtzNUSSdq/RpaJRPZBciBooSSOlu+3BvaDTYlFeQ5aFd5CAS/R9RKNZV+dGnaf+k\n",
       "PQVmN4qoypmqG2iIwUBRMWDiM5rLpQclNj9wKXkOL5oT+ifqlfzCh/TSa9h034Fy1BiHYU9pYRS1\n",
       "E+ffQvUnUdXzwyKrEyxuBt8o7Xozw0rmzY2flYRIZwZxNtCdviatn13EaBkXqOgcMtXGOmyl114T\n",
       "J65cVIJ8JfVd1fxKUSR3QroBUYNwag5DPqrtcoUgB1toCic3Mg3DSE+i6sJZPQmtFIjDKwrTtjPZ\n",
       "LSSY33jMOe6O8exHJORk4TQeUrScb+/TYDddjLpVZYnOR+ipATaAqpMKeTjSkGWwOwNEQ3pa1Ja5\n",
       "3NmhFn6Qk3dlR/2wh7wPhFq4hLuPjYI8hyWD0z07dK9E27EOHTinKJvvG64wHi6eN6grCw+IYMR7\n",
       "e8tDZ95HoxuzyWgp4LvV+UEG9pMKWnpKFbXOUk74E7v5AZOf4j4D741Iph5JZxangot1nw3zNjop\n",
       "Tcu19O9fdGIvscySXllOJROPPQJjrIokbJ1pe2kiw80dpQl2rrxKaWxRYnwvukFMNLW1diufpCxy\n",
       "9JRrcwQGA+HgWuxurHdsdXCPfTyK4nbccH6ToGThsRPB2Mxu3Evt/0pkXYtFYL1zJW6igCn67+wY\n",
       "XOB9vln26K2Jngl9qOtrdQf8AmbDsfzi58VbxJcKN4e4UiCzYTGoE0UHcQAaPYuYU0YasbMYABQF\n",
       "0NL5ukMn9MAp5+XWlKkvzkxD8qBp9jy5L1AXvMIIFP2AyaOJelcgoMU5z844CvTZcPo5opNcYcT9\n",
       "5fmftu9jOLpNCxAn2bbtncWcl7+g6VAyshFmoysXXDDqlR5LNUXsqHy3cHFSeKI93PBZmvjGoRp9\n",
       "Guw8WER9yKm5Tv/++Pfu4qpKgsY4x9TiPacg40prJy8ZqLumqrDUcO+8A9PPL1B3uC/3YxhR4qWm\n",
       "7JJFuAv5xa1wf+VqjWZw+T9tQa5GuRRQcKSd5DF1TxllERsh1FMGDRrB7eyraD2eJMkGubIWbPWM\n",
       "skAdXJ8ZLvoCi7nfq9rrvq16qwe5EaCVggfvH2x63OrCqDXJ/AozSiTT2jtQDKVHaw3LOILie8dY\n",
       "hTb17uByCStaUcM6FG5sQdxvu8wXOULgX73nSFPERe+q25yFVPfyYUjWHsNHjUQGjpeC9B7O08cx\n",
       "I7b5vIblAQ7yz81Bk5/DCjUbbzBQHAl5nOQKo51nd6bbQrRWnH/xirGsABvvYXECWwAAAINBmiRs\n",
       "Qz/+nhAHsrtkAI9+RIuIVNbh2ufpSpHTs9OPZGxHaAz9MRfoflplLY97SzNrWCl0etM74ocOEEF4\n",
       "4ykGusHKE+OHLRiO/EoUBHwmzyFMERFRxhz/oMvtkN7TbLt6UyAO49Y0X++YW1rMe+7D7uZDIJkF\n",
       "ZdDvvQh81WeoW3cOSAAAADBBnkJ4hP8BzglcGTmydT7r9XpVDoCADAmppPWi669BCRa69CLSy6hU\n",
       "r/cEkCG+o9cAAAAZAZ5hdEI/Aiq/iMIH+vrg51Nn+3GSZMS3YAAAABMBnmNqQj8AI7yf8el+6fg6\n",
       "xORNAAAAK0GaaEmoQWiZTAhn//6eEAAKfwgrCWgwGz5B/gVD4a53R+NaH6BKz2nnkIEAAAAhQZ6G\n",
       "RREsJ/8AxF7NWEy5JCKPBIAHN5o0a4AEzffRP9KRAAAAEwGepXRCPwAjq/fx6KGg+P4sEREAAAAN\n",
       "AZ6nakI/ACO8n/ChgAAAADBBmqxJqEFsmUwIZ//+nhAAFPlUIRyieZvAARR9Yy78Adrj+qkbfQ0K\n",
       "NPwh2wgBTdQAAAAcQZ7KRRUsJ/8AxF7NWEy5JCKPBIAHN5o0a4ABlQAAAA0Bnul0Qj8AI6v38KGA\n",
       "AAAADQGe62pCPwAjvJ/woYAAAAAxQZrwSahBbJlMCGf//p4QABT5aKABt8aagTiXBVfSakuHl9XS\n",
       "IfvxWuzXeAX6E3g2aQAAABxBnw5FFSwn/wDEXs1YTLkkIo8EgAc3mjRrgAGVAAAADQGfLXRCPwAj\n",
       "q/fwoYEAAAANAZ8vakI/ACO8n/ChgAAAADZBmzRJqEFsmUwIZ//+nhAAFPlooAGwiGrWwIhtiF0O\n",
       "PbeQnP7cc+jQjbje/vSZ4iZJPUTXBoMAAAAeQZ9SRRUsJ/8AxF7NWEy5JCKPBIAHN5o0a4RA2B3R\n",
       "AAAADQGfcXRCPwAjq/fwoYAAAAANAZ9zakI/ACO8n/ChgAAAAEJBm3hJqEFsmUwIZ//+nhAAFPlo\n",
       "oAGwiGzRkhYvzXBl17Gas2z6P8UL8DHZjMq4JZNIkZICnOsmJMLs9m+cxCYPQYEAAAAeQZ+WRRUs\n",
       "J/8AxF7NWEy5JCKPBIAHN5o0a4RA2B3QAAAADQGftXRCPwAjq/fwoYEAAAAPAZ+3akI/ACO8oAvQ\n",
       "YISBAAAAOEGbvEmoQWyZTAhn//6eEAAU/yHfAFF2uMCnsP/t1LnjLk3eSnzrJH4QUf8MUpvKSDw0\n",
       "GA5j0+HgAAAAH0Gf2kUVLCf/AMRezVhMuSQijwSABzeaNGuEQQegdUEAAAANAZ/5dEI/ACOr9/Ch\n",
       "gAAAAA8Bn/tqQj8AI7ygzsn/0kEAAAETQZvgSahBbJlMCGf//p4QABUAY6zYCEbRrLiHeQa3TiJX\n",
       "VU6SWYflU4xmfVYhOssYwPmNDGiT+1cCll5ISmAaSLOy9j+NUBiopSuHiAKgyW8+gfPdCe+i4jGS\n",
       "bpULgPXderHIWxixGqYtOLT5ACv3mrBtuQTj9gy8593pttIiLR4PX+TFGgUkdM2L65wj7kdcA/sk\n",
       "xd8bBEpfCtC2/2f4t4AAYuVk+d5geCd2c4rpp3AaT4+75Cus0gnYC55rLQacxvv9YZozIByRaMpb\n",
       "iSCtORzx/mTfcMtuMb9NYDpLPGyibAOIl+7bFF4ggKnj5/RVuacDzeJ5erymymjPVDbJeMWdrTAg\n",
       "vPrt4OZbtCiOzb42AYEAAAAuQZ4eRRUsJ/8AxF7NWEy5JCKPBIAHN5o0a4RBGQm6jdVKemFHPZ6Q\n",
       "Z7kwEoLUgAAAABsBnj10Qj8AI6v41HcSlpjslcINTDIex2P1UUQAAAARAZ4/akI/ACO8oM7J82XL\n",
       "XyEAAAA5QZokSahBbJlMCGf//p4QABT5aKACi32hrOZjZgBnjpHtFWwwpm5W7P3hc0w8fbhS8FJC\n",
       "1e1HgLMIAAAAH0GeQkUVLCf/AMRezVhMuSQijwSABzeaNGuEQQIHA48AAAASAZ5hdEI/ACOr+bel\n",
       "rBeKjGWAAAAADQGeY2pCPwAjvJ/woYEAAAAsQZpoSahBbJlMCGf//p4QABT5VBMo+8nSnCuQBl+A\n",
       "/TSqTXNAsqaZblTL+PcAAAAgQZ6GRRUsJ/8AxF7NWEy5JCKPBIAHN5o0a4RBAdQujmkAAAANAZ6l\n",
       "dEI/ACOr9/ChgQAAAA0BnqdqQj8AI7yf8KGAAAAAPkGarEmoQWyZTAhn//6eEAAVBePwAD5JxsOB\n",
       "GgWHUABGKcKuVEsUYZiUWoItkOKh+uZcNrq4eA9g6szzn0lgAAAAH0GeykUVLCf/AMRezVhMuSQi\n",
       "jwSABzeaNGuEQQGgl4EAAAARAZ7pdEI/ACOr+belrBfAwIAAAAAQAZ7rakI/ACO8oa2T5uDrYAAA\n",
       "ADpBmvBJqEFsmUwIZ//+nhAAFPxegAomKDciBUg2sNfwQGpeMdWDropMfJlVtXnWfRk2wbMcQ6bX\n",
       "unARAAAAIEGfDkUVLCf/AMRezVhMuSQijwSABzeaNGuEQQGjWvYJAAAAEAGfLXRCPwAjq/ms0k2d\n",
       "1sEAAAAPAZ8vakI/ACO8oa2T/1JAAAAAF0GbNEmoQWyZTAhn//6eEAAKgADqNEYEAAAAH0GfUkUV\n",
       "LCf/AMRezVhMuSQijwSABzeaNGuEQQIHA48AAAAPAZ9xdEI/ACOr+azSjnTAAAAADwGfc2pCPwAj\n",
       "vKGtk/9SQAAAACNBm3hJqEFsmUwIZ//+nhAACoBYHIVSOKtwADXj25/7OgIwQQAAACFBn5ZFFSwn\n",
       "/wDEXs1YTLkkIo8EgAc3mjRrhEECB6WAmYAAAAAPAZ+1dEI/ACOr+azSjnTBAAAAEQGft2pCPwAj\n",
       "vKGtlA3a4FtBAAAAW0GbvEmoQWyZTAhn//6eEAAU+iO9AQQFOLaivH4bUEoxbQouO8P60jHoaBcH\n",
       "TtrceOHtjphje8pzxOcJe4lel18W/nucQeHzysWAu8LZ+w70YkxTjpBWeOhHpyYAAAAqQZ/aRRUs\n",
       "J/8AxF7NWEy5JCKPBIAHN5o0a4RBAkAF5WzLZMjZeSqlaRmxAAAAEQGf+XRCPwAjq/ms0qONUvqA\n",
       "AAAAGAGf+2pCPwAjvKGtlDkQMJn2oOdxnnBbwQAAAEBBm+BJqEFsmUwIZ//+nhAAFP6k7ADYIzGL\n",
       "XzZ5wp9jBYyUHEsOPgC+R5xGCL1QX9/wgXuTmEeKlccfEwDlvC5BAAAAIUGeHkUVLCf/AMRezVhM\n",
       "uSQijwSABzeaNGuEQQIHKb9YwAAAABEBnj10Qj8AI6v5rNJNoV6aaQAAABABnj9qQj8AI7yhrZPm\n",
       "4OthAAAAOUGaJEmoQWyZTAhn//6eEAAVAG8MgBpnH56BQJaQHdGzV8z2tdI38Fv7hYi7+CzpHua1\n",
       "ZMqviWmpGAAAACFBnkJFFSwn/wDEXs1YTLkkIo8EgAc3mjRrhEECBylgmYEAAAAQAZ5hdEI/ACOr\n",
       "+azSTZ3WwAAAABMBnmNqQj8AI7yhrZPm19oO/2+BAAAARkGaaEmoQWyZTAhn//6eEAAU+6hkAT9N\n",
       "QtKjctOPYiXCKVawCt9MOdR3ET9S6KiWiuAeTJnnkI5uSFOiNvaIf94pR//XuzEAAAAhQZ6GRRUs\n",
       "J/8AxF7NWEy5JCKPBIAHN5o0a4RBAgcpk2VBAAAAEgGepXRCPwAjq/ms0k2EF33aCwAAABIBnqdq\n",
       "Qj8AI7yhrZPm4rC0QsAAAAA+QZqsSahBbJlMCGf//p4QABUAYi1wC765/v292BKcMkKf8hBtTNm0\n",
       "urV+7/RGFv5muJhyUm94OO/+N0Pk4BAAAAAlQZ7KRRUsJ/8AxF7NWEy5JCKPBIAHN5o0a4RBGHlg\n",
       "TkDj2rb64QAAABEBnul0Qj8AI6v5rNJNoV6aaQAAABEBnutqQj8AI7yhrZQCtrhbQAAAADVBmvBJ\n",
       "qEFsmUwIZ//+nhAAFPtcQEcbO3qHowBPhOzKbTkrBqvFFOmPjAubX4fotBNNGYXA9wAAAChBnw5F\n",
       "FSwn/wDEXs1YTLkkIo8EgAc3mjRrhEEYeWZhnGI3PwIgyLrdAAAAEQGfLXRCPwAjq/ms0k2hXppp\n",
       "AAAAEgGfL2pCPwAjvKGtk+bisLRCwAAAAD9BmzRJqEFsmUwIZ//+nhAAFPtcQEoLJAKP8AGaIT81\n",
       "cNetV6eqDvBF/jpCTlFazNQP3v1hItWQoM5yKNbaAUAAAAAuQZ9SRRUsJ/8AxF7NWEy5JCKPBIAH\n",
       "N5o0a4RkYEFz/bZ4BeGiycF059GW8FJP4QAAABEBn3F0Qj8AI6v5rNJNoV6aaQAAABMBn3NqQj8A\n",
       "I7yhrZPm4rCy8KzAAAAAW0GbdkmoQWyZTBRMM//+nhAHWF32ADcBAXtOX1NOF5mJDpEuiTRmz9pQ\n",
       "YdYxDoyRe0HX/l+DhNleiEDRtctn7PnOOTh2L9Lj++d74+ub+2zXptH89QbdXbS2g5MAAAAhAZ+V\n",
       "akI/AOhuFGDkeb6eR8wANTamdR7cV8Lg9BlHQp7AAAAAk0GbmknhClJlMCGf/p4QABUAWsYPgQSC\n",
       "RXIlgATqvsqZmcux9pZfAOB7npLtS5AP1+GG09E35QZKfn5Jhl6/xGEoqhCMhDzy2bl01wPGVO5o\n",
       "acyOAFjiEjIo8vHPz/q39gwBVEdqA/YIoA9E3ihypwd9m6tgnzKQG0gXea42dyaSE7txgCgs8/Pd\n",
       "Vab/wWL2wui1cQAAADtBn7hFNEwn/wDETxcP6HxEJcjFHABliaNGuB4pL+xkAA087OaMMOiNt93n\n",
       "dwki2WokZ2SUI08Wxc8XQQAAABIBn9d0Qj8AI6v40+i7Jh+oMUAAAAAiAZ/ZakI/ACO8oNRVYdbV\n",
       "wuANmrLnp1+zhyL1y9EKC//mDwAAAF5Bm95JqEFomUwIZ//+nhAAFPstetR0gKCeO4cZIjfBtI38\n",
       "wa9rUqyVkB18k8632f+QUpaAaxNltqBSLEudyTKCF/Y/cHG8kGpXwClofXM1LpytWHlCbvtqs/H3\n",
       "2eOeAAAANEGf/EURLCf/AMRezVhMuSQijwSABzeaNGuCN3isygvJSSxmIWc2H+JqghQBN3O/g/SF\n",
       "UkEAAAAUAZ4bdEI/ACOr+M5/5com1TyOqMEAAAAYAZ4dakI/ACO8objkRay+g15PRvBapAMWAAAA\n",
       "QUGaAUmoQWyZTAhn//6eEAAVHmXOuy+rQZ5h/mNgAhG/egTFMHfRbr1KFFzE/RounUHY5bA+AvFF\n",
       "3goxctQPESvFAAAALUGeP0UVLCf/AMRezVhMuSQijwSABzeaNGuCNclE2D5cEhCDIsOORLlxS75V\n",
       "HwAAACQBnkBqQj8AI7yhuKikDZEYQBEOB6tZk/ZwvL0YMMGE1/PLoIAAAABIQZpFSahBbJlMCF//\n",
       "/oywABSa10MwBsqQL0nDG7mXR3ddHsIm8UcqmKPAXvY0fpKI1z+yKMePAYdjTgywGqrbs594PfRI\n",
       "fix9AAAAP0GeY0UVLCf/AMRezVhMuSQijwSABzeaNGuEQNQ5QAIh1gUTTEcz0WikJj/BlO10a+Yq\n",
       "qzv7XYoSIqWlyAIDwAAAABkBnoJ0Qj8AI6v403Aznx4C6zdZDurlcIjXAAAAGgGehGpCPwAjvKDT\n",
       "7Ahe7uoVcJhq7WxkgBVRAAAAPUGah0moQWyZTBRML//+jLAAFKVZUeAip+KOLWO2nmeuWTS9lE93\n",
       "STieJjLq90FtOab8ZIRF7RqEziqIF2kAAAAtAZ6makI/AOhuFGDkeb6eR8wANTamdR7cjYJa/wHq\n",
       "hmmHuyLSdxevmVyuxaCBAAAAMkGaqEnhClJlMCF//oywABSuL//wf9rdT+IOpiloRSroPtNtD2RF\n",
       "MgZIcW/CFrzEpbaUAAAAQ0GayUnhDomUwIX//oywABSkaBtgGq+i/gYcBH0dKpDg0/8LXOwd2HIW\n",
       "1PES9lX2vGyYJIc3EyC8dmGy6fM53u93AoAAAAA5QZrqSeEPJlMCGf/+nhAAFHW/sjgExi2bgYNJ\n",
       "a8JLUZJ+wXJNrqVXyygcTxI4fohHXsZWee/OFi1RAAAANUGbDknhDyZTAhf//oywABSauGOP4cGX\n",
       "cATRfjQifeT9Kyg12sy06ORruvkQ5NIWixV9JgAoAAAAL0GfLEURPCf/AMRD1qSmtg7zQ6OGgAkA\n",
       "VSrRS86WMG8Ys1rqSetBZQbFpqcVb3xMAAAAGwGfS3RCPwAGmrfqLLmm+kP2eSb5tKgrcq8QQQAA\n",
       "ABgBn01qQj8ABpgqAyo0xhO/3ph8b2Zzm90AAAA1QZtPSahBaJlMCF///oywAtv1OFxpnevLQyzt\n",
       "6K53ADNnX2mezL5Ec4WTF4h7pjX2PO8uUzEAAAAsQZtwSeEKUmUwIZ/+nhAAFGtcQFDq8UBTm/N1\n",
       "Jmgv2E6e85HWoJNpzvNFWMAAAABBQZuUSeEOiZTAhn/+nhAAFGsG/QADh4SkiLVqBdj6exx1dym3\n",
       "BKu5OShO1c6CoEW46pRhodLK/WnyorQ/1+/hkyAAAAAzQZ+yRRE8J/8AumxRD+h8RCXIxRxk3UzV\n",
       "XhJzfx6R/aF45Ku8/6f0mRj91sAXkMG8ifbnAAAAGAGf0XRCPwAGmV72jLyDE9TJHgGkBtKvHAAA\n",
       "ABoBn9NqQj8ABpUUC7RwrMugDeCUBPIg8hgiWwAAAPtBm9hJqEFomUwIZ//+nhAAFHBVsidp+5Zc\n",
       "mdADSv/ufgbGQsOYuFOInxZokKROFzvSDp6kTsq2DlEEXLNHtl4+Zgjz0BlElyjLMwzeozEsaDE7\n",
       "ssj2aNBP4CGwypgEETtwt0ZSBVyvf8YKCAM8ppDy7Y71z+1L9juvm+Uw1ovhrl7PfYh2drnN8t/a\n",
       "bYFE2X7Harj577lekKwmANnShhee2O2/IIVM4evTZ2xNmpA0FeTQt6EE0E7bqPkTxC+/4trDIkqK\n",
       "X0MKDdVPEpZ97V/neVTLe31AB/q1dOLc7UzJLWBo6irwwHZsYlJBPW32x+evXFNbXRwIAZ+MwQAA\n",
       "ADpBn/ZFESwn/wC6eaTITLkkIo8EhdnUzUO02USbgjZpS3vFpSFDMjupDVH1twlpY9O+/UnwDA+5\n",
       "QNB+AAAAFgGeFXRCPwADTDdQ1y1u56XwJO7xbcEAAAAhAZ4XakI/AAaaNSDEmDEIa1cF4ZZedCZM\n",
       "fFiJ5yTwaueBAAAAZEGaGUmoQWyZTAhn//6eEAKv6eqr0D7fxR5QBfwhfpMxZ8p6Ak+QKvdV9gGs\n",
       "8zd9hrQW3WbRU6XVOWKK1FcXnrDdb4Vu6B2Qjx7utUQzBnq9ty6NB4/tg6IcZ+60oVsqRork47AA\n",
       "AAB2QZo9SeEKUmUwIZ/+nhAAFGrGa45GiwC0NpgeMkyAzCt/V8TNy4rl3cVRzaSs67yL9g4tuLto\n",
       "ah2hZ/x2GSvIom7PAoN+uj3HHSk3XSH+mEmg23sEW9YxMHXf95uHNeJOWKnClCjLdv/p9Sccx/gz\n",
       "u58ahlBfUwAAADZBnltFNEwn/wCxbFEP6HxEJcjFm1KahSGOcyiw9Wy10tT334TwfUnji3ydU+gq\n",
       "adV4vpiVP3AAAAAbAZ56dEI/AAaamo19gyj+7I2KVc2ZqDj8j0GhAAAAHgGefGpCPwAGmCmR2Fc4\n",
       "rAikCUmx6vqHz+QvlvoSwQAAAGRBmmFJqEFomUwIZ//+nhAAFI5lzrIhDFXqOlAi6ACO7AcPIqYV\n",
       "cBWR52c00FzFviJsItIQc5frPOfa5HuFLnPn7gOhixwPNBFKy5Q9OYgxRBeBmHczPAdIcNW5N4aB\n",
       "BwMe8u+AAAAAOUGen0URLCf/ALF5pMhMuSQijwmQKahV3w9HUeYfjDnXlNE/VEcpg68vL3wEAdMK\n",
       "5L7OiKj4yZndwAAAACIBnr50Qj8ABpeelugQMRXzWs8gncN9RObFHXKtU5U/tNQFAAAAIQGeoGpC\n",
       "PwAGc1s4pdnm3+wm6ke7cAWKuVOc0oRokaetgAAAAHJBmqVJqEFsmUwIZ//+nhAAE+2DqfRAp0Cw\n",
       "c7zv6zgBJ56Mlp7d5ZzYdvuPSxRgIOIzWugEqUKYRMWIm/qyy//3tpIqMvyM4tYvhQkeRCRnacZn\n",
       "VRwMFlE/A2BePzEHxil9rzvM2pMEkpZk+bo6kTrfKBsAAABEQZ7DRRUsJ/8AsXmkyEy5JCKPCZAp\n",
       "p8zp2NwcNs8bgBHPpxgouP8n+chP3cgooHGvEdXAczxmy145Q/jRtwGiICCfC8AAAAAcAZ7idEI/\n",
       "AAZymrl69gvB3nn1r8q31mW4u33J6wAAACQBnuRqQj8ABnAp/uIrduYWMzzM5j2c8fRNdLni4XNr\n",
       "EPwh2IEAAABkQZrpSahBbJlMCGf//p4QABPlHla3DACgF1gNvDQcb1gUILVuziliEnKw451RLnZ+\n",
       "41XSraHrCyOSsZKmsPkAPYP4KUWg+24g5RxhxlN7pwMnwnO7f+RBmYqjDQWrH9nQEk65owAAADxB\n",
       "nwdFFSwn/wCxeaTITLkkIo8JkCmn0G0MgfqZe4Vm979nDU8TlawNq5viQjDChoH9H30BNRXgcCl1\n",
       "7YEAAAAgAZ8mdEI/AAZvlzLdHMlhPBxIhN+MUWvgsW3pO9oU334AAAAkAZ8oakI/AAZyPFqoxxGY\n",
       "ycHzoSkAEOG43S439rI/RVSvuqSQAAAAQ0GbK0moQWyZTBRMM//+nhAAE9sGWuVeda6VgASgCuyq\n",
       "5XnfMKifpLi2V07XdP/4v6oA7BzXcyrPtZ1ApMF6RzTjTkMAAAAnAZ9KakI/ANNu2eDkeb6eSAj9\n",
       "Syza/j0cw6qd3z6YV1wno91JU64pAAAAU0GbT0nhClJlMCGf/p4QABPa2EapwlvjtFWmskG4zRjf\n",
       "lz2kVkIqVEBmrvnXbqmDlPogABvzAnhY1u3wHmrMZUOwSDFC+gPGjEv3qkQXIl9RbuMgAAAANkGf\n",
       "bUU0TCf/ALFsUQ/ofEQlyMWbUpp+CBN2G9oW8leC/rqHt+MNRLL07HxP3Cg7zxYUbmj2tQAAAB8B\n",
       "n4x0Qj8ABnKauXqDraSmzPULDwl+ylnM8Uy6b9iFAAAAIAGfjmpCPwAGcBr+/PdvZI9s0q3WwiP5\n",
       "2GaTlP+LNRoDAAAAaUGbk0moQWiZTAhn//6eEAAT/mXPb1eCDACNtn1cvUUzEDoYPgN43w886S0N\n",
       "+7Jxq45ppDu5K32JJZTkcIhLgLn5EA7OhTIg322GSdcSmF5LmdU4wJxZqn5BIh1qadthy1cLtPu+\n",
       "WlgkNwAAADlBn7FFESwn/wCxeaTITLkkIo8JkCmn3rOjFdOKb7cERDCcULs7s7GIYna5x/ltVaCr\n",
       "OTAoVRaIiqkAAAAlAZ/QdEI/AAZyjC4p1407pqsFOuJNFAKn3nyLwFno4NJQ1O0VhQAAACEBn9Jq\n",
       "Qj8ABkgP0K4YnJaVlKLajpvN3bqox0+/Muv2kyAAAACeQZvXSahBbJlMCGf//p4QABNZEtl0lQAE\n",
       "V5BxVzu+L8oGU0doe/cVLelRmxlpdd8zi+TdBjS4i4vomwFZp48KYt/k3aL21AzfsmBTLcQ7Bruo\n",
       "EKwIfsl2oyYpMjNJXfomk4JYGaZWd0tpxXYDRfQ8kdIo+EL2q2hKrtz1IevJs+4qX6luAn+o045V\n",
       "qIz9lr7MAvAfv/D0uHeB6iSqvPIAAABBQZ/1RRUsJ/8AsXmkyEy5JCKPCZApp1BwjMrr1FNUQ5Td\n",
       "UZKDiR/cJb9XHW/hk6ivVw+XQ2dN1iBp9nIcnpAfjKEAAAAnAZ4UdEI/AAZF4WefaXG+04dpMb35\n",
       "JPcp09v5oiMkLYcoJGYKWl/TAAAAQwGeFmpCPwAGSA/Ewdu7eIASUMxu8wiC+M0yj/Axfu+UzOCl\n",
       "citOpVyk3tfxfBnnm8hTJyIFeRe3JfvFDOIuZpi2psEAAABxQZoaSahBbJlMCGf//p4QABNQ9V5I\n",
       "BvaB2g4AWThw8KfgiyMkd54l5kdc3QTMoGdvh7FWp9DGlBWnxmW1/bN7WNuj/ksLKggkkoKWls2G\n",
       "PU1SbaY2x2DothALTdlrBPxQNJxo5mxyGCUfworPAlQhA5kAAAA2QZ44RRUsJ/8AsXmkyEy5JCKP\n",
       "CZApp1/RLKN8DF9XwxUZvvooK9cvojSeSjpiPEYCAo7ke6WgAAAAGgGeWWpCPwAGSA/GCEhf8Dv0\n",
       "uOtjMNFFkQqRAAAAu0GaXkmoQWyZTAhn//6eEAATbpvOtBe5ACe+c2qXLcK4GGZ5zvOwsC3iphGG\n",
       "AI9v9OuOah+78sV4cQdcHKjEAcDTjM5Od0lS/CeqIVXIP36BV/WYlUV7E7pBpmjteoPQy+BCroHl\n",
       "wiLc1DsW+ht7IShGFCrJjOsMqKqFi2JYykI1TkAEK3caYxDDH+jGEUOK51z3NW8XlopK6yZoft/N\n",
       "vcqx787hy4CRmiqMLN57oXnjnQ+Je4jhYXJ5oMAAAAA5QZ58RRUsJ/8AsXmkyEy5JCKPCZApp0LE\n",
       "mH6mh4M6wCWhSCfni1oPPDnkmeWADb40dU4ZaMgcxtP5AAAAKAGem3RCPwAGSVkM3GtgLS0bh4MX\n",
       "9egMbcAc9WG8pQ1p5bQnTlBMDuEAAAAtAZ6dakI/AAZHWvEhtm3lYgl0qgABKXL1YKc7ebRykRgC\n",
       "TswkY1ZjOtUZnOTrAAAApkGagkmoQWyZTAhf//6MsAATBCmMbxKfXMLuoAmjkALZMDZFEl9whSIN\n",
       "IIOicIVjUzBNL0/6vyozqE6gXczS6ESdyInOHtWGp6coemwqrLxGvIBXmK4bZ0eEMvVui9LRBVt5\n",
       "uKMVJ+YuyOMuEckJzcHSNSs30HNXmsuKb5tFtH+Kf3eRyxH60H37s6tT5igQoJZbp3K3NpCRFQFu\n",
       "eZrzYW0lT4eatUAAAABRQZ6gRRUsJ/8AsXmkyEy5JCKPCZApptByTA8kSzY9D3WMyJ0MMnoHi3dk\n",
       "Ol1iVf3FlL6KubBgBVsjC33kHsh0eWoZydOYBwS6KuAkUQE7lTPFAAAAJAGe33RCPwAGIVkPV8rU\n",
       "pJpIT9Jx4vBBQ1cgVztIW+N3Erma2AAAACgBnsFqQj8ABiIwtZWuht6qtKrMmW4pU4do5kQYoh9m\n",
       "PM09tfs/XveBAAAAV0Gaw0moQWyZTAhn//6eEAKL6eqr0Dn6IoAmn0XM0Pav/LKPAAXSnhN5pztP\n",
       "Cx/+MJf/i9Md4Q2gXHyNpXc57PCYWovl6u2gDZdcoTYV+eUg5ZIQLsQJdwAAAHdBmuZJ4QpSZTAh\n",
       "n/6eEAAS0PVeN/ufgCiAG6uKAE3zpegyxmNWAzb+Sd2HpLWFPvbRyYhDIPTWyh0jS/tpeIMJlRru\n",
       "six3t1vwoOyrZ0Wn1eD9zSkij9nuLJIt0Wp2xbBEhdC0R8XD6JAfePOWsvXs0uv9AkCFwQAAADxB\n",
       "nwRFNEwn/wCobFEP6HxEJctHEt5f81IlK26ko07vs3/r9RYPvdKEbzoOc60ijbfzZrDLKrJ0h+mz\n",
       "yYEAAAAnAZ8lakI/AAYgD8WX3EEqDjFHJCgs7bSUsRWsDczVUKVFCwVnxBjxAAAAo0GbKkmoQWiZ\n",
       "TAhn//6eEAAS7ptrgokNcZPtGQGAB0g/MGVqpLL8/rmc1/4xEMYKlx2En4UA2wiVEn/dsPZfRzvB\n",
       "6MJtRMnCXTuCmkyyMsUxSEDSCxwLfQO2qtmLVZ/zIG9+89qXf/h3bXSS2afbX6154+VcUf5r5XmQ\n",
       "rvWuszufkaJNU+V9PvbjAA6dxCDJGDy1bU2nsTtyDi4btMPqgwEu7CEAAABGQZ9IRREsJ/8AqHmk\n",
       "yEy5JCKi+Jb03dye7RBKy80W/P5xkdzNJKmDDOWRGGlGphpHskwA/CmNXHwGJAtFSRngcYWTX0d3\n",
       "WAAAACkBn2d0Qj8ABiJ8MCpczAz5U5XP6Cjz6FxXuKz12+1CurDj7/mJyNA5cAAAACkBn2lqQj8A\n",
       "Bg/cY7idN3QXJVTS7uEVTjbTa6yxK/8qTItzI6Xq520nRwAAAI1Bm21JqEFsmUwIZ//+nhAAElkF\n",
       "49zzy+edK7VMs10WAAdAhFvSArT8vAz9vztzluPOQtBiU0Hpul9k5gHqQsV+9bsqCLfl8AERdV82\n",
       "YG6B+YjHb703a0ezm+ROOOVb5ZBd+ghCScqRO1494hiZ9k5FM3sL6J77j7jTOtV4Kp8sRL5b0qd0\n",
       "4+o9TJ4LSeAAAAA6QZ+LRRUsJ/8AqHmkyEy5JCKi+Jbr3rclweevtZit4qHfGU0vxm4UsPaIfcgb\n",
       "4TmemylLdkxfC8MmkAAAADABn6xqQj8ABftcYFltfoEwVxVjYrrzpCdSaX5ZMgpNi35+QfLbXhEC\n",
       "w1uNG+wTknEAAACDQZuxSahBbJlMCGf//p4QABJRAGb03WAKFmAMalXsujZRXcXZOiRGrE8Y2zM4\n",
       "jdh7usmgqcF3V7MBSEC4d5Jhvjmi95GdyskaZXb74LH+z+cext+TFbm3Zwwwu1CL6RAaMaVEP+hH\n",
       "FDtGPqXRkU8RA10tWkB//f5T3YCnf+RWMVGP5+EAAABIQZ/PRRUsJ/8AqHmkyEy5JCKi+Jbp8Oh8\n",
       "CNlT6pVjsa9Hx03mhozL9TEp8QdYfF/S+vp27/3oLUQCpcIYgLkMnVJApyduKypBAAAAJwGf7nRC\n",
       "PwAF+nxBBJVuWLkynzFXJLo9LOZ0UhHjeNbTASqBxuf0PgAAACkBn/BqQj8ABfoqLUeZL8vWMm8C\n",
       "ZolmNPcxO8Dg6IKCdGZG93c9eHPEvAAAASRBm/VJqEFsmUwIZ//+nhAAEm6b0DJi8Pa+u4oXDCRG\n",
       "AELwWGQZlKbQrqUJRqPvl9zk/Kf9eYA8Q6sb1CUAXz0vxXdeULPHoFzzvKXiT5ks8AAMLI6+kkAp\n",
       "QEnqqqXuqgEguzhccsiACrEsr1tkrB9vypAxx5Og2KjHYn4AIC1weytzbLWLdXvi4iis3xFXW588\n",
       "xZnQzWRAKyPkL8ZcdUETNm2MG0D9778g2P7+6mOk1Ndzcj9ZfFfagj7UkSq33diBc+zzYKfwNXG2\n",
       "SV+NK+cpnCVjhrTZoKhrOClOGRD2odiQG/ZWP72HWMOL8Xfs2toOKDA+ekOP6tws9g+VpQEZx3xh\n",
       "7ZVoCMwCixdsmjsd6B9oeNivYY8G9WHasGargKulc2sNAAAAWkGeE0UVLCf/AKh5pMhMuSQioviW\n",
       "6wk4jC8N2k21K4hNNArs/c4Bit7/bngATd1e+RsUckQkJhhoupfT0zWXERKX7zYozapoaz5oKKXu\n",
       "Lzy90kWUKjCf1I7UtAAAACkBnjJ0Qj8ABfeIA++GiupGW0QTHLdfFO0/GmnfKzWQKJm4cQ9PrL+w\n",
       "2AAAADIBnjRqQj8ABdLX4znL7QXKTf3Gq/3a7WPoUrpU7uTqj+VhPXyQtV2fo7FV+OZIdbiH/QAA\n",
       "ANJBmjlJqEFsmUwIZ//+nhAAEdEFRo1dXRLDLq76ADaTA80EUrQ5U6UQ8P2L3v/f7JsveIZnm+Fz\n",
       "UbD5zA/a3zjPu7P25BpkdQ9ZzSHbHSoGJ68xHJ+0ySmYK7KvdZd4mg/N6uzvyze3KpdIrIIEF/cX\n",
       "G8zc5rnpzrh5PEtAz/CZiUiclVd8DmD/yFcB/52VDeXv7ux6qaA+U2sM6XRSaX7HvopIn2O9WtMs\n",
       "6QJ5sWPgaOrjVqyUWy7TvZRMQInnGmNszr/t8qpXSOq77o6jitlhs+gAAABSQZ5XRRUsJ/8AqHmk\n",
       "yEy5JCKi+Jbf6UEDwBT6IjuGBDGCHCGvg9BLKwTiDGdfCEsREjqDW1lcZEyxFIG15kxcV47BVAnO\n",
       "uW/AxRGzDzB3zQuxoQAAAC8BnnZ0Qj8ABdA2x0em+Dndw1/a0hvrUlLqRoHVCuASqjj8jWbBfbT/\n",
       "70gaSPJLlwAAADUBnnhqQj8ABdLX4Ll750NIVCP1Wgqkmoekxt6+HFFSMcjBX+DE4Stjm2jsc11Z\n",
       "48UNeo1HrwAAANdBmn1JqEFsmUwIX//+jLAAEh6bz6OFQAj3qSZltbIUW82qiVZueA2EKFxtNsM9\n",
       "OsCft1WDc1uyeDWd5YxDBlQKlwbS+0WVa63moXOj/49h2B3kBBYNEeLFpRH2wqYZt+gHjB9h3Qkb\n",
       "04YaeyS4qF+gf9lQkjdwHL91FAsh8XkadckH0IP061FgCYfR/WSABVmJdq06OmRYGjNNB81CAKws\n",
       "AeS2diQuO/59LCno+c8ZngzrkKNjcuvVIwOmXUqk7Lyav6V7b1HSr77yZQEQDDtpVLc8i/el3QAA\n",
       "AFxBnptFFSwn/wCoeaTITLkkIqL4luIT6Dy6V8roPcj/S707KRo1ea29ZAmSRF6z11IOVwFOzHhG\n",
       "JD51Wznm9eYIvYr5urxXkjLNSPtiL+obxBCeOHUMeTvT3kCPQAAAADQBnrp0Qj8ABdAysB4ZHS3I\n",
       "SxfN85m2cv/x9dueUVMG9MboPKjDL/KhwZFmfzgtG8rUjO1BAAAAMwGevGpCPwAFrtfjOU3/eoDt\n",
       "HriJaOTQOiWDVbpw/f9pRhOC3LOXk7mvz6UMMezQY+wwWQAAAHNBmr9JqEFsmUwUTC///oywABGA\n",
       "UcalSIfh0ADrSs81FeV0hwm0CpRjM0UJIL3jG1nYXo8bnWEW7RYDwdm4kfpqZ2cXMumvqlx3/yx7\n",
       "/FZf1G+JYmyX7e5pJ4rtf0tHHVBd626Ls2otKKQWfHWLXZTeOEcgAAAAPQGe3mpCPwDJbtng5Hm+\n",
       "nsREb/KXsAQI9gOQWSnBrxfG5sC/HbzyRN/WrAEMvqIVu4ZJE/jilwfE07GKa7gAAABvQZrBSeEK\n",
       "UmUwUsL//oywABGAVfG4UwpAkk0N/78id8lW5VSABeuNh4YyTK2GAeyJ/Z9zL6+i/+G8SKWPUn35\n",
       "9JAsQVF95N2d2iSX0nDcyOwQ/5PV4zMxeqe2dYr3sZCiuB94xJdDvkXt7QcN4gOxAAAAQAGe4GpC\n",
       "PwDJU9YmFFeJYJuaI4ATBVYrmZDsUyNo0xqdxoE+OrnMQSJFQ+6CkXq9aE6K9tV2f5bjhdW32pbP\n",
       "A5YAAAB1QZrjSeEOiZTBRML//oywABGem89zOuWAA4VEhdSFwBRigfD7G9LYc1X0zdZgzBtvIvCw\n",
       "LIHfRlswqY68dPyyuPY6PGU49vLCov8AFPyi30cvYED13UId1tnQmjfSeM/gfF0deARQr+XN90FU\n",
       "cplYvTRokturAAAAPAGfAmpCPwDJU9YmFFeJYJuaI3/U4ARo/ZgRWMNz5N1SDAje+WpaB06B7wGm\n",
       "saBe7zq1e9M45vdndiPWQAAAAHRBmwVJ4Q8mUwU8K//+OEAAQTzcMjqoMozzYWXDd0US34wmLwAm\n",
       "raVCqB8imMF7aJcFyOHsx6MSiiXn29AJH/EBT4HfA0Vbw8YhLtE32RihLdvrdhEV6zG5CqIeWmB/\n",
       "6lXggMyqo9HlcKWwGqDtXUcWo/7MNQAAADwBnyRqQj8AyVPWJhRXiWCbmiN9UwWwBvSQH6icMzOR\n",
       "g+15Qa5RuGBIEwB4j8wd97VpJy03XP8fyhRI8UEAAAAuQZsoSeEPJlMCEf/94QAA/vo4G7tA3fpu\n",
       "Dls6PN8E/HwezxQkWSx/0ac0Kr8k4QAAAEtBn0ZFETwn/wCoYv5COONeFJ5HM7SiHKY9Xc6HGFUS\n",
       "AEGhp3brxRPXkJRskNx9xbORis0gHC1HuWIzmOlG9tYxANlyGBpJJftma8EAAAAvAZ9nakI/AAWK\n",
       "xh185EPlg/SAF5ToQgJxkY6kYZgbcZNAFz+5KI3YUGrm71JD65QAAAxabW9vdgAAAGxtdmhkAAAA\n",
       "AAAAAAAAAAAAAAAD6AAAE6EAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAA\n",
       "AAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAAC4R0cmFrAAAAXHRraGQA\n",
       "AAADAAAAAAAAAAAAAAABAAAAAAAAE6EAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAB\n",
       "AAAAAAAAAAAAAAAAAABAAAAAAbAAAAEgAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAABOhAAAC\n",
       "AAABAAAAAAr8bWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAAoAAAAyQBVxAAAAAAALWhkbHIAAAAA\n",
       "AAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAAKp21pbmYAAAAUdm1oZAAAAAEAAAAA\n",
       "AAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAACmdzdGJsAAAAs3N0c2QA\n",
       "AAAAAAAAAQAAAKNhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAbABIABIAAAASAAAAAAAAAAB\n",
       "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAAMWF2Y0MBZAAV/+EAGGdkABWs\n",
       "2UGwloQAAAMABAAAAwFAPFi2WAEABmjr48siwAAAABx1dWlka2hA8l8kT8W6OaUbzwMj8wAAAAAA\n",
       "AAAYc3R0cwAAAAAAAAABAAAAyQAAAQAAAAAUc3RzcwAAAAAAAAABAAAAAQAABhhjdHRzAAAAAAAA\n",
       "AMEAAAABAAACAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAAC\n",
       "AAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUA\n",
       "AAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAA\n",
       "AAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAA\n",
       "AAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAA\n",
       "AQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAAB\n",
       "AAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEA\n",
       "AAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAA\n",
       "AQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAA\n",
       "AAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIA\n",
       "AAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAA\n",
       "AAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAA\n",
       "AAEAAAMAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAA\n",
       "AQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABAAAAAACAAABAAAAAAEAAAUAAAAAAQAAAgAAAAAB\n",
       "AAAAAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAMAAAIAAAAAAQAABQAAAAABAAACAAAAAAEA\n",
       "AAAAAAAAAQAAAQAAAAACAAACAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAA\n",
       "BQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAACAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAA\n",
       "AAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIA\n",
       "AAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAAAwAA\n",
       "AAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAA\n",
       "AAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAA\n",
       "AgAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAAB\n",
       "AAAAAAAAAAEAAAEAAAAAAQAAAgAAAAABAAAEAAAAAAIAAAEAAAAAAQAABQAAAAABAAACAAAAAAEA\n",
       "AAAAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAA\n",
       "AQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAA\n",
       "AAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAADAAAAAAEAAAEA\n",
       "AAAAAQAAAwAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAADAAAAAAEAAAEAAAAAAQAABAAA\n",
       "AAACAAABAAAAABxzdHNjAAAAAAAAAAEAAAABAAAAyQAAAAEAAAM4c3RzegAAAAAAAAAAAAAAyQAA\n",
       "DgwAAACHAAAANAAAAB0AAAAXAAAALwAAACUAAAAXAAAAEQAAADQAAAAgAAAAEQAAABEAAAA1AAAA\n",
       "IAAAABEAAAARAAAAOgAAACIAAAARAAAAEQAAAEYAAAAiAAAAEQAAABMAAAA8AAAAIwAAABEAAAAT\n",
       "AAABFwAAADIAAAAfAAAAFQAAAD0AAAAjAAAAFgAAABEAAAAwAAAAJAAAABEAAAARAAAAQgAAACMA\n",
       "AAAVAAAAFAAAAD4AAAAkAAAAFAAAABMAAAAbAAAAIwAAABMAAAATAAAAJwAAACUAAAATAAAAFQAA\n",
       "AF8AAAAuAAAAFQAAABwAAABEAAAAJQAAABUAAAAUAAAAPQAAACUAAAAUAAAAFwAAAEoAAAAlAAAA\n",
       "FgAAABYAAABCAAAAKQAAABUAAAAVAAAAOQAAACwAAAAVAAAAFgAAAEMAAAAyAAAAFQAAABcAAABf\n",
       "AAAAJQAAAJcAAAA/AAAAFgAAACYAAABiAAAAOAAAABgAAAAcAAAARQAAADEAAAAoAAAATAAAAEMA\n",
       "AAAdAAAAHgAAAEEAAAAxAAAANgAAAEcAAAA9AAAAOQAAADMAAAAfAAAAHAAAADkAAAAwAAAARQAA\n",
       "ADcAAAAcAAAAHgAAAP8AAAA+AAAAGgAAACUAAABoAAAAegAAADoAAAAfAAAAIgAAAGgAAAA9AAAA\n",
       "JgAAACUAAAB2AAAASAAAACAAAAAoAAAAaAAAAEAAAAAkAAAAKAAAAEcAAAArAAAAVwAAADoAAAAj\n",
       "AAAAJAAAAG0AAAA9AAAAKQAAACUAAACiAAAARQAAACsAAABHAAAAdQAAADoAAAAeAAAAvwAAAD0A\n",
       "AAAsAAAAMQAAAKoAAABVAAAAKAAAACwAAABbAAAAewAAAEAAAAArAAAApwAAAEoAAAAtAAAALQAA\n",
       "AJEAAAA+AAAANAAAAIcAAABMAAAAKwAAAC0AAAEoAAAAXgAAAC0AAAA2AAAA1gAAAFYAAAAzAAAA\n",
       "OQAAANsAAABgAAAAOAAAADcAAAB3AAAAQQAAAHMAAABEAAAAeQAAAEAAAAB4AAAAQAAAADIAAABP\n",
       "AAAAMwAAABRzdGNvAAAAAAAAAAEAAAAsAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAA\n",
       "AAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1\n",
       "Ny44My4xMDA=\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.4127338508478733"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this block is for testing performance\n",
    "dq_stepper.show_performance(1.0, x0 = [0.0, 0], show_episode=True)\n",
    "# dq_stepper.save(\"dqs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05798403128825269"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
