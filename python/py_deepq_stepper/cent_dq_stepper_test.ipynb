{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80462914 0.        ] 2.95\n",
      "[-0.02736886  0.        ] 0.69\n",
      "[-2.48091146  0.        ] 107.67\n"
     ]
    }
   ],
   "source": [
    "# This code tests the trained 3d dq stepper\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#for live plotting\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%run cent_env.ipynb #imports LIPM Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lipm - 8 layers , 512 each\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, inp_size, out_size):\n",
    "        \n",
    "        super(NN, self).__init__()\n",
    "        self.l1 = nn.Linear(inp_size, 512)\n",
    "        self.l2 = nn.Linear(512, 512)\n",
    "        self.l3 = nn.Linear(512, 512)\n",
    "        self.l4 = nn.Linear(512, 512)\n",
    "        self.l5 = nn.Linear(512, 512)\n",
    "        self.l6 = nn.Linear(512, 512)\n",
    "        self.l7 = nn.Linear(512, 512)\n",
    "        self.l8 = nn.Linear(512, 512)\n",
    "        self.l9 = nn.Linear(512, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = F.relu(self.l5(x))\n",
    "        x = F.relu(self.l6(x))\n",
    "        x = F.relu(self.l7(x))\n",
    "        x = F.relu(self.l8(x))\n",
    "        x = self.l9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQStepper:\n",
    "    def __init__(self, env, lr = 1e-4, gamma = 0.9, use_tarnet = False, trained_model = None):\n",
    "        '''\n",
    "        This is a 3d dq stepper.\n",
    "        State = [x-ux, y-uy, z-uz, xd, yd, n, action_x, action_y, action_z]\n",
    "        '''\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.dq_stepper = NN(11, 1).to(self.device) #state+ action -> q_value\n",
    "        if trained_model:\n",
    "            self.dq_stepper.load_state_dict(torch.load(trained_model))\n",
    "            self.dq_stepper.eval()\n",
    "        self.optimizer = torch.optim.SGD(self.dq_stepper.parameters(), lr)\n",
    "        self.use_tarnet = use_tarnet\n",
    "        if self.use_tarnet:\n",
    "            self.dq_tar_stepper = NN(11, 1).to(self.device)\n",
    "            self.dq_tar_stepper.load_state_dict(self.dq_stepper.state_dict())\n",
    "            self.dq_tar_stepper.eval()\n",
    "        self.gamma = gamma #discount factor\n",
    "        self.no_actions = env.no_actions\n",
    "        \n",
    "        # This is the template of x_in that goes into the dq stepper\n",
    "        self.max_step_height = 0.00\n",
    "        self.max_no = 5 #number of actions with non zero step in z\n",
    "        self.x_in = np.zeros((self.no_actions[0]*self.no_actions[1], 11))\n",
    "        self.x_in[:,8] = np.tile(np.arange(self.no_actions[0]), self.no_actions[1])\n",
    "        self.x_in[:,9] = np.repeat(np.arange(self.no_actions[1]), self.no_actions[0])\n",
    "        \n",
    "    def predict_action_value(self, x):\n",
    "        # this function predicts the q_value for different actions and returns action and min q value\n",
    "        self.x_in[:,[0, 1, 2, 3, 4, 5, 6, 7]] = x\n",
    "        for e in np.random.randint(0, len(self.x_in), self.max_no):\n",
    "            self.x_in[e, 10] = 2*self.max_step_height*(np.random.rand() - 0.5)\n",
    "        torch_x_in = torch.FloatTensor(self.x_in, device = self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.dq_stepper(torch_x_in).detach().numpy()\n",
    "            action_index = np.argmin(q_values)\n",
    "            action_x = int(action_index%self.no_actions[0])\n",
    "            action_y = int(action_index//self.no_actions[0])\n",
    "            action_z = self.x_in[action_index,10]\n",
    "        return [action_x, action_y, action_z], q_values[action_index]\n",
    "    \n",
    "    def tar_predict_action_value(self, x):\n",
    "        # this function uses tar net to predict \n",
    "        # the q_value for different actions and returns action and min q value\n",
    "        self.x_in[:,[0, 1, 2, 3, 4, 5, 6, 7]] = x\n",
    "        for e in np.random.randint(0, len(self.x_in), self.max_no):\n",
    "            self.x_in[e, 10] = 2*self.max_step_height*(np.random.rand() - 0.5)\n",
    "        torch_x_in = torch.FloatTensor(self.x_in, device = self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.dq_tar_stepper(torch_x_in).detach().numpy()\n",
    "            action_index = np.argmin(q_values)\n",
    "            action_x = int(action_index%self.no_actions[0])\n",
    "            action_y = int(action_index//self.no_actions[0])\n",
    "            action_z = self.x_in[action_index,10]\n",
    "        return [action_x, action_y, action_z], q_values[action_index]\n",
    "    \n",
    "    def predict_eps_greedy(self, x, eps = 0.1):\n",
    "        # This function returns prediction based on epsillon greedy algorithm\n",
    "        if np.random.random() > eps:\n",
    "            return self.predict_action_value(x)[0]\n",
    "        else:\n",
    "            action_x = np.random.randint(self.no_actions[0])\n",
    "            action_y = np.random.randint(self.no_actions[1])\n",
    "            action_z = 2*self.max_step_height*(np.random.rand() - 0.5)\n",
    "            \n",
    "        return [action_x, action_y, action_z]\n",
    "        \n",
    "    def optimize(self, mini_batch, tau = 0.001):\n",
    "        # This function performs one step of back propogation for the given mini_batch data\n",
    "        x_in = torch.FloatTensor(mini_batch[:,0:11].copy(), device = self.device)\n",
    "        y_train = torch.FloatTensor(mini_batch[:,11].copy(), device = self.device)\n",
    "        for i in range(len(mini_batch)):\n",
    "            if not np.isnan(mini_batch[i,12:]).all():\n",
    "                if not self.use_tarnet:\n",
    "                    y_train[i] += self.gamma * self.predict_action_value(mini_batch[i,12:])[1]\n",
    "                else:\n",
    "                    y_train[i] += self.gamma * self.tar_predict_action_value(mini_batch[i,12:])[1]\n",
    "\n",
    "        y_train = y_train.unsqueeze(1).detach() #ensures that gradients are not computed on this\n",
    "        x_train = self.dq_stepper(x_in)\n",
    "\n",
    "        loss = F.mse_loss(x_train, y_train)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.use_tarnet:\n",
    "            for tar_param, param in zip(self.dq_tar_stepper.parameters(), self.dq_stepper.parameters()):\n",
    "                tar_param.data.copy_(tar_param.data * (1.0 - tau) + param.data * tau)\n",
    "                \n",
    "        return loss\n",
    "    \n",
    "    def live_plot(self, history, e, figsize=(15,25), window = 500, title='history'):\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(3, 1, figsize=figsize)\n",
    "        ax[0].plot(history['epi_cost'], label='epi_cost', color = 'orange')\n",
    "        ax[0].grid(True)\n",
    "        ax[0].legend() # the plot evolves to the right\n",
    "        if e > window:\n",
    "            ax[1].plot(np.arange(e-window+1, e), history['epi_cost'][e-window:], label='epi_cost zoom')\n",
    "            ax[1].grid(True)\n",
    "            ax[1].legend() # the plot evolves to the right\n",
    "        ax[2].plot(history['loss'], label='loss', color = 'black')\n",
    "        ax[2].grid(True)\n",
    "        ax[2].legend() # the plot evolves to the right\n",
    "        ax[2].set_ylim(0, 60)\n",
    "        plt.xlabel('episode')\n",
    "        plt.show();\n",
    "        \n",
    "    def predict_q(self, x, terrain):\n",
    "        #for debugging\n",
    "        # this function predicts the q_value for different actions and returns action and min q value\n",
    "        self.x_in[:,[0, 1, 2, 3, 4, 5, 6, 7]] = x\n",
    "        self.x_in[:,10] = terrain\n",
    "        torch_x_in = torch.FloatTensor(self.x_in, device = self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.dq_stepper(torch_x_in).detach().numpy()\n",
    "            action_index = np.argmin(q_values)\n",
    "            action_x = int(action_index%self.no_actions[0])\n",
    "            action_y = int(action_index//self.no_actions[0])\n",
    "            action_z = self.x_in[action_index,10]\n",
    "            \n",
    "        return q_values, [action_x, action_y, action_z] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.35 -0.28 -0.21 -0.14 -0.07  0.    0.07  0.14  0.21  0.28  0.35]\n"
     ]
    }
   ],
   "source": [
    "ht = 0.28\n",
    "step_time = 0.1\n",
    "air_time = 0.1\n",
    "env = CentEnv(ht, 0.0, 0.35, [1.5, 3.0, 0.5], [11,1])\n",
    "print(env.action_space_x)\n",
    "dqs = DQStepper(env, lr=1e-4, gamma=0.98, use_tarnet= True, trained_model='../../models/dqs_1_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "[5, 0, 0.0] 1.8 False [0. 0.]\n",
      "showing episode...\n"
     ]
    }
   ],
   "source": [
    "no_steps = 20 ## number of steps simulated per episode (pendulum steps)\n",
    "\n",
    "v_des = [0.6, 0]\n",
    "# v_init = [1.0*(np.random.rand() - 0.5), 0.0*(np.random.rand() - 0.5)]\n",
    "v_init = [.0, 0.0]\n",
    "print(v_init)\n",
    "state = env.reset_env([0.0, -0.0, ht, v_init[0], v_init[1], 0.0], v_des, no_steps*(2*step_time + air_time))\n",
    "epi_cost = 0\n",
    "for n in range(no_steps):\n",
    "    terrain = 0.00\n",
    "    action = dqs.predict_q(state, terrain)[1]\n",
    "    next_state, cost, done = env.step_env(action, step_time, air_time)\n",
    "    print(action, cost, done, state[3:5])\n",
    "    epi_cost += cost\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(epi_cost)\n",
    "        break\n",
    "print(\"showing episode...\")\n",
    "# env.show_episode(5)\n",
    "# env.show_episode_side(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f58e5298208>]"
      ]
     },
     "execution_count": 1078,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPfElEQVR4nO3cf6zddX3H8edr7ej8kVF+VMSWrt1oZorb1JygRrcQRWidWrLxR5mJXcbSfyTzx5athGQo+odsTpwRXRpwdsQIjum80zhWi2bJosgtGqVgbQW1rUWqRRwzEzvf++N8ux0v99p7e057evp5PpKT+/18vp9zzvuTz+W8+v18zyVVhSSpXb8w7gIkSeNlEEhS4wwCSWqcQSBJjTMIJKlxi8ddwPE499xza9WqVeMuQ5Imys6dO79XVctm9k9kEKxatYrp6elxlyFJEyXJt2brd2tIkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkho3kiBIsi7J7iR7k2yZ5fySJHd05+9JsmrG+ZVJnkjyZ6OoR5I0f0MHQZJFwM3AemAtcFWStTOGXQ08VlUXAjcBN844/27g08PWIklauFFcEVwM7K2qh6rqSeB2YMOMMRuAbd3xncArkgQgyRXAw8CuEdQiSVqgUQTBcmDfQHt/1zfrmKo6AjwOnJPkmcBfAG871psk2ZxkOsn0oUOHRlC2JAnGf7P4rcBNVfXEsQZW1daq6lVVb9myZSe+MklqxOIRvMYB4IKB9oqub7Yx+5MsBs4Evg+8CLgyyV8BS4GfJvnvqnrfCOqSJM3DKILgXmBNktX0P/A3An8wY8wUsAn4PHAlcHdVFfDbRwckeSvwhCEgSSfX0EFQVUeSXAPcBSwCPlhVu5LcAExX1RRwK3Bbkr3AYfphIUk6BaT/D/PJ0uv1anp6etxlSNJESbKzqnoz+8d9s1iSNGYGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS40YSBEnWJdmdZG+SLbOcX5Lkju78PUlWdf2vTLIzyVe7ny8fRT2SpPkbOgiSLAJuBtYDa4GrkqydMexq4LGquhC4Cbix6/8e8Jqq+g1gE3DbsPVIkhZmFFcEFwN7q+qhqnoSuB3YMGPMBmBbd3wn8IokqaovVdV3uv5dwNOSLBlBTZKkeRpFECwH9g2093d9s46pqiPA48A5M8b8PnBfVf14BDVJkuZp8bgLAEhyEf3tost+zpjNwGaAlStXnqTKJOn0N4orggPABQPtFV3frGOSLAbOBL7ftVcAHwdeX1XfmOtNqmprVfWqqrds2bIRlC1JgtEEwb3AmiSrk5wBbASmZoyZon8zGOBK4O6qqiRLgU8BW6rqP0ZQiyRpgYYOgm7P/xrgLuBB4KNVtSvJDUle2w27FTgnyV7gLcDRr5heA1wI/GWSL3ePZw1bkyRp/lJV465hwXq9Xk1PT4+7DEmaKEl2VlVvZr9/WSxJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuNGEgRJ1iXZnWRvki2znF+S5I7u/D1JVg2cu7br353k8lHUI0mav6GDIMki4GZgPbAWuCrJ2hnDrgYeq6oLgZuAG7vnrgU2AhcB64D3d68nSTpJFo/gNS4G9lbVQwBJbgc2AA8MjNkAvLU7vhN4X5J0/bdX1Y+Bh5Ps7V7v8yOo6yne9i+7eOA7PzwRLy1JJ9za5/wy17/mopG/7ii2hpYD+wba+7u+WcdU1RHgceCceT4XgCSbk0wnmT506NAIypYkwWiuCE6KqtoKbAXo9Xp1PK9xIpJUkibdKK4IDgAXDLRXdH2zjkmyGDgT+P48nytJOoFGEQT3AmuSrE5yBv2bv1MzxkwBm7rjK4G7q6q6/o3dt4pWA2uAL46gJknSPA29NVRVR5JcA9wFLAI+WFW7ktwATFfVFHArcFt3M/gw/bCgG/dR+jeWjwBvqKr/GbYmSdL8pf8P88nS6/Vqenp63GVI0kRJsrOqejP7/ctiSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1LihgiDJ2Um2J9nT/TxrjnGbujF7kmzq+p6e5FNJvpZkV5J3DlOLJOn4DHtFsAXYUVVrgB1d+2ckORu4HngRcDFw/UBgvKuqngu8AHhpkvVD1iNJWqBhg2ADsK073gZcMcuYy4HtVXW4qh4DtgPrqupHVfVZgKp6ErgPWDFkPZKkBRo2CM6rqoPd8SPAebOMWQ7sG2jv7/r+T5KlwGvoX1VIkk6ixccakOQzwLNnOXXdYKOqKkkttIAki4GPAO+tqod+zrjNwGaAlStXLvRtJElzOGYQVNWlc51L8t0k51fVwSTnA4/OMuwAcMlAewXwuYH2VmBPVb3nGHVs7cbS6/UWHDiSpNkNuzU0BWzqjjcBn5hlzF3AZUnO6m4SX9b1keQdwJnAm4asQ5J0nIYNgncCr0yyB7i0a5Okl+QWgKo6DLwduLd73FBVh5OsoL+9tBa4L8mXk/zxkPVIkhYoVZO3y9Lr9Wp6enrcZUjSREmys6p6M/v9y2JJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkho3VBAkOTvJ9iR7up9nzTFuUzdmT5JNs5yfSnL/MLVIko7PsFcEW4AdVbUG2NG1f0aSs4HrgRcBFwPXDwZGkt8DnhiyDknScRo2CDYA27rjbcAVs4y5HNheVYer6jFgO7AOIMkzgbcA7xiyDknScRo2CM6rqoPd8SPAebOMWQ7sG2jv7/oA3g78DfCjY71Rks1JppNMHzp0aIiSJUmDFh9rQJLPAM+e5dR1g42qqiQ13zdO8nzg16rqzUlWHWt8VW0FtgL0er15v48k6ec7ZhBU1aVznUvy3STnV9XBJOcDj84y7ABwyUB7BfA54CVAL8k3uzqeleRzVXUJkqSTZtitoSng6LeANgGfmGXMXcBlSc7qbhJfBtxVVR+oqudU1SrgZcDXDQFJOvmGDYJ3Aq9Msge4tGuTpJfkFoCqOkz/XsC93eOGrk+SdApI1eRtt/d6vZqenh53GZI0UZLsrKrezH7/sliSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4VNW4a1iwJIeAbx3n088FvjfCcsbpdJnL6TIPcC6nqtNlLsPO41eqatnMzokMgmEkma6q3rjrGIXTZS6nyzzAuZyqTpe5nKh5uDUkSY0zCCSpcS0GwdZxFzBCp8tcTpd5gHM5VZ0uczkh82juHoEk6We1eEUgSRpgEEhS45oJgiTrkuxOsjfJlnHXsxBJLkjy2SQPJNmV5I1d/9lJtifZ0/08a9y1zleSRUm+lOSTXXt1knu69bkjyRnjrnE+kixNcmeSryV5MMlLJnFdkry5+926P8lHkvzSpKxJkg8meTTJ/QN9s65B+t7bzekrSV44vsqfao65/HX3+/WVJB9PsnTg3LXdXHYnufx437eJIEiyCLgZWA+sBa5Ksna8VS3IEeBPq2ot8GLgDV39W4AdVbUG2NG1J8UbgQcH2jcCN1XVhcBjwNVjqWrh/hb416p6LvBb9Oc0UeuSZDnwJ0Cvqp4HLAI2Mjlr8iFg3Yy+udZgPbCme2wGPnCSapyvD/HUuWwHnldVvwl8HbgWoPsM2Ahc1D3n/d1n3YI1EQTAxcDeqnqoqp4Ebgc2jLmmeauqg1V1X3f8n/Q/bJbTn8O2btg24IrxVLgwSVYAvwvc0rUDvBy4sxsyEXNJcibwO8CtAFX1ZFX9gMlcl8XA05IsBp4OHGRC1qSq/h04PKN7rjXYAPxD9X0BWJrk/JNT6bHNNpeq+reqOtI1vwCs6I43ALdX1Y+r6mFgL/3PugVrJQiWA/sG2vu7vomTZBXwAuAe4LyqOtidegQ4b0xlLdR7gD8Hftq1zwF+MPDLPinrsxo4BPx9t811S5JnMGHrUlUHgHcB36YfAI8DO5nMNTlqrjWY9M+CPwI+3R2PbC6tBMFpIckzgX8C3lRVPxw8V/3vAZ/y3wVO8mrg0araOe5aRmAx8ELgA1X1AuC/mLENNAnr0u2fb6AfbM8BnsFTtycm1iSswXwkuY7+NvGHR/3arQTBAeCCgfaKrm9iJPlF+iHw4ar6WNf93aOXtd3PR8dV3wK8FHhtkm/S36J7Of199qXdtgRMzvrsB/ZX1T1d+076wTBp63Ip8HBVHaqqnwAfo79Ok7gmR821BhP5WZDkD4FXA6+r///jr5HNpZUguBdY030L4gz6N1imxlzTvHV76LcCD1bVuwdOTQGbuuNNwCdOdm0LVVXXVtWKqlpFfx3urqrXAZ8FruyGTcpcHgH2Jfn1rusVwANM3rp8G3hxkqd3v2tH5zFxazJgrjWYAl7ffXvoxcDjA1tIp6Qk6+hvpb62qn40cGoK2JhkSZLV9G+Af/G43qSqmngAr6J/x/0bwHXjrmeBtb+M/qXtV4Avd49X0d9b3wHsAT4DnD3uWhc4r0uAT3bHv9r9Eu8F/hFYMu765jmH5wPT3dr8M3DWJK4L8Dbga8D9wG3AkklZE+Aj9O9t/IT+VdrVc60BEPrfIPwG8FX635Qa+xyOMZe99O8FHP1v/+8Gxl/XzWU3sP5439f/xYQkNa6VrSFJ0hwMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4/wVuo8ayVu96BgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(env.sim_data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.04 0.09 0.13 0.18 0.24 0.34 0.46 0.63]\n"
     ]
    }
   ],
   "source": [
    "env_main = CentEnv(ht, 0.13, 0.5, [0.5, 3.0, 1.5], [11,9])\n",
    "\n",
    "b = 0.13\n",
    "step_time = 0.1\n",
    "air_time = 0.1\n",
    "actions = [11, 9]\n",
    "max_step_length = [0.5, 0.5]\n",
    "\n",
    "env_x = CentEnv(ht, 0., max_step_length[0], [0.5, 3.0, 1.5], [actions[0],1])\n",
    "dqs_x = DQStepper(env_x, lr=1e-4, gamma=0.98, use_tarnet= True, trained_model='../../models/dqs_1_str')\n",
    "env_y = CentEnv(ht, b, max_step_length[1], [0.5, 3.0, 1.5], [1,actions[1]])\n",
    "dqs_y = DQStepper(env_y, lr=1e-4, gamma=0.98, use_tarnet= True, trained_model='../../models/dqs_1_side')\n",
    "print(env_y.action_space_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03, -0.24]\n",
      "[5 0 0] 1.52 False [ 0.     0.015  0.3    0.03  -0.24 ]\n",
      "[6 6 0] 1.23 False [ 0.01134101 -0.05351529  0.29996423  0.07081046 -0.36553874]\n",
      "[3 5 0] 2.03 False [-0.0596296   0.11635803  0.29992299 -0.11250006 -0.12332326]\n",
      "[5 0 0] 1.35 False [ 0.03474802 -0.0241571   0.29992418 -0.22383742  0.28541542]\n",
      "[5 5 0] 1.47 False [ 0.00465037  0.0462644   0.29992415 -0.04258433  0.33709572]\n",
      "[5 0 0] 2.16 False [-0.00419303 -0.03013691  0.29992415 -0.03561362  0.39244928]\n",
      "[3 6 0] 3.42 False [-0.02444157  0.07159043  0.29992415 -0.14321277  0.50672774]\n",
      "[9 2 0] 107.34 True [ 0.11417733 -0.01589947  0.29992415  0.19721592  0.70998148]\n",
      "120.52\n",
      "showing episode...\n"
     ]
    }
   ],
   "source": [
    "no_steps = 10 ## number of steps simulated per episode (pendulum steps)\n",
    "\n",
    "v_des = [0., 0]\n",
    "v_init = [3.0*(np.random.rand() - 0.5), 1.4*(np.random.rand() - 0.5)]\n",
    "v_init = [0.03, -0.24]\n",
    "print(v_init)\n",
    "state = env_main.reset_env([0.0, -0.05, ht, v_init[0], v_init[1], 0.0], v_des, no_steps*(2*step_time + air_time))\n",
    "epi_cost = 0\n",
    "for n in range(no_steps):\n",
    "    terrain = 0.00\n",
    "    # for x axis\n",
    "    state_x = state.copy()\n",
    "    state_x[1] = 0.0\n",
    "    state_x[4] = 0.0\n",
    "    state_x[7] = 0.0\n",
    "    action_x = dqs_x.predict_q(state_x, terrain)[1] \n",
    "    # for y axis\n",
    "    state_y = state.copy()\n",
    "    state_y[0] = 0.0\n",
    "    state_y[3] = 0.0\n",
    "    state_y[6] = 0.0\n",
    "    action_y = dqs_y.predict_q(state_y, terrain)[1] \n",
    "    action = np.array([int(action_x[0]), int(action_y[1]), 0])\n",
    "\n",
    "    next_state, cost, done = env_main.step_env(action, step_time, air_time)\n",
    "    print(action, cost, done, state[0:5])\n",
    "    epi_cost += cost\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(epi_cost)\n",
    "        break\n",
    "print(\"showing episode...\")\n",
    "# env_main.show_episode(5)\n",
    "# env_main.show_episode_side(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
