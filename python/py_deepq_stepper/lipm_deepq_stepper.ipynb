{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This file contains a prototype idea of trying to learn a value function that represents the viable region\n",
    "## for a LIP model. \n",
    "## Author : Avadesh Meduri\n",
    "## Date : 20/02/2020\n",
    "\n",
    "import numpy as np\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import pickle as p\n",
    "import os\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIPM Environment\n",
    "\n",
    "class LipmEnv:\n",
    "    def __init__(self, h):\n",
    "        self.omega = np.sqrt(9.81/h)\n",
    "        self.max_leg_length = 0.6\n",
    "        self.dt = 0.001\n",
    "        self.h = h\n",
    "        self.A = np.matrix([[1, self.dt], [(self.omega**2)*self.dt, 1]])\n",
    "        self.B = np.matrix([0, -(self.omega**2)*self.dt])\n",
    "        self.t = 0\n",
    "                                 \n",
    "    def integrate_lip_dynamics(self, x_t, u_t):\n",
    "        ## integrates dynamics for one step\n",
    "        assert np.shape(x_t) == (2,)\n",
    "        x_t_1 = np.matmul(self.A, np.transpose(x_t)) + np.matmul(self.B.transpose(), [u_t])\n",
    "        return x_t_1\n",
    "\n",
    "    def reset_env(self, x0, u0, epi_time):\n",
    "        ## initialises environment\n",
    "        self.t = 0\n",
    "        self.sim_data = np.zeros((4, int(epi_time/self.dt)+1))\n",
    "        self.sim_data[:,0][0:2] = x0\n",
    "        self.sim_data[:,0][2] = u0\n",
    "        self.sim_data[:,0][3] = self.h\n",
    "    def step_env(self):\n",
    "        ## integrates the simulation one step\n",
    "        self.sim_data[:,self.t + 1][0:2] = self.integrate_lip_dynamics(self.sim_data[:,self.t][0:2],\\\n",
    "                                                   self.sim_data[:,self.t][2])\n",
    "        self.sim_data[:,self.t + 1][2] = self.sim_data[:,self.t][2]\n",
    "        self.sim_data[:,self.t + 1][3] = self.sim_data[:,self.t][3] \n",
    "        self.t += 1\n",
    "    \n",
    "    def set_action(self, u):\n",
    "        self.sim_data[:,self.t][2] = u\n",
    "        \n",
    "    def return_sample_data(self):\n",
    "        return self.sim_data[:,0:self.t]\n",
    "           \n",
    "    def show_episode(self, freq, i_no):\n",
    "        ## Input:\n",
    "            ## Freq : frame rate (if freq = 5 one in 5 is shown)\n",
    "            ## i_no : iteration number \n",
    "        sim_data = self.sim_data[:,::freq]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes(xlim=(-5, 5), ylim=(0, sim_data[:,0][3] + 0.2))\n",
    "        text_str = \"iter - \" + str(i_no)\n",
    "        line, = ax.plot([], [], lw=3)\n",
    "        def init():\n",
    "            line.set_data([], [])\n",
    "            return line,\n",
    "        def animate(i):\n",
    "            x = sim_data[:,i][0]\n",
    "            y = sim_data[:,i][3]\n",
    "            u = sim_data[:,i][2]\n",
    "            line.set_data([u,x], [0,y])\n",
    "            return line,\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(0.05, 0.95, text_str, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "        \n",
    "        anim = FuncAnimation(fig, animate, init_func=init,\n",
    "                                       frames=np.shape(sim_data)[1], interval=25, blit=True)\n",
    "\n",
    "        plt.close(fig)\n",
    "        plt.close(anim._fig)\n",
    "        IPython.display.display_html(IPython.core.display.HTML(anim.to_html5_video()))\n",
    "\n",
    "    def compute_reward(self, step_time):\n",
    "        ## Computes the reward after step\n",
    "        r = 0\n",
    "        step_data = self.sim_data[:,int(self.t - step_time*1000):int(self.t)].copy()\n",
    "        step_data[0] = np.subtract(step_data[0], step_data[2])\n",
    "        min_dist = step_data[0].argmin()\n",
    "        if abs(step_data[0][min_dist]) < 0.02: ## min distance between COM and COP\n",
    "            r += 10\n",
    "        if abs(step_data[1][min_dist]) < 0.02: ## Min velocity when min dist is achieved\n",
    "            r += 10\n",
    "        \n",
    "#         if self.sim_data[:,int(self.t - step_time*1000)-1][2] != self.sim_data[:,int(self.t - step_time*1000)][2]:\n",
    "#             r -= 1 ## penalises if step is taken\n",
    "#         else:\n",
    "#             r += 10 ## rewards if no step is taken\n",
    "            \n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This block samples and store data using epsillon greedy algorithm\n",
    "\n",
    "def sample_data(no_episodes, epi_t, h, action_set, value_function, show_episode = False):\n",
    "    # this function samples data\n",
    "    env = LipmEnv(h)\n",
    "    sample_data = []\n",
    "    for e in range(no_episodes):\n",
    "        print(\"running iter number - \" + str(e))\n",
    "        x = [0.0, 4*np.random.random() - 2]\n",
    "        u0 = action_set[np.random.randint(9)]\n",
    "        #espillon greedy\n",
    "        if np.random.random() > 0.2:\n",
    "            x_in = np.tile([u0 - x[0], x[1], 0],(len(action_set),1)) \n",
    "            x_in[:,2] = action_set\n",
    "            a = np.argmax(value_function(torch.tensor((x_in), dtype=torch.float)).cpu().detach().numpy())\n",
    "        else:\n",
    "            a = np.random.randint(9)\n",
    "        step_time = 0.15\n",
    "        env.reset_env(x, u0, epi_t)\n",
    "\n",
    "        ## sars_t = s_t, a_t, r_t, s_t+1\n",
    "        sars_t = np.zeros(6)\n",
    "        sars_t[0] = u0 - x[0]\n",
    "        sars_t[1] = x[1]\n",
    "        sars_t[2] = action_set[a]\n",
    "        for t in range(0, int(epi_t*1000) - 1):\n",
    "            if t % int(step_time * 1000) == 0 and t > 0:\n",
    "                sars_t[3] = env.compute_reward(step_time)\n",
    "                env.set_action(env.sim_data[:,env.t][0] + action_set[a]) ## setting action\n",
    "                sars_t[4] = env.sim_data[:,env.t][2] - env.sim_data[:,env.t][0]\n",
    "                sars_t[5] = env.sim_data[:,env.t][1]\n",
    "                sample_data.append(sars_t)\n",
    "                \n",
    "                sars_t = np.zeros(6)\n",
    "                sars_t[0] = env.sim_data[:,env.t][2] - env.sim_data[:,env.t][0]\n",
    "                sars_t[1] = env.sim_data[:,env.t][1]\n",
    "                # epsillon greedy\n",
    "                if np.random.random() > 0.2:\n",
    "                    x_in = [sars_t[0], sars_t[1], 0].copy()\n",
    "                    x_in = np.tile(x_in,((len(action_set),1)))\n",
    "                    x_in[:,2] = action_set\n",
    "                    a = np.argmax(value_function(torch.tensor((x_in), dtype=torch.float)).cpu().detach().numpy())\n",
    "                else:\n",
    "                    a = np.random.randint(9)\n",
    "    \n",
    "                sars_t[2] = action_set[a]\n",
    "        \n",
    "            env.step_env()\n",
    "    \n",
    "        if show_episode: \n",
    "            env.show_episode(5, e)\n",
    "            \n",
    "    ## shifting reward up since the reward\n",
    "    sample_data = np.asarray(sample_data)\n",
    "    sample_data[:,3] = np.roll(sample_data[:,3],-1)\n",
    "    sample_data = sample_data[0:-2]\n",
    "    return np.asarray(sample_data)\n",
    "\n",
    "def store_data(data_array, file_name, dir):\n",
    "    batch_no = str(len(os.listdir(dir)))\n",
    "    f = open(dir + file_name + \"_\" + batch_no + \".pkl\", 'wb')\n",
    "    print(\"dumping data ...\")\n",
    "    p.dump(data_array, f, -1)  \n",
    "    f.close()    \n",
    "    print(\"finished dumping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this block is for the Q function\n",
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, outputs):\n",
    "        super(ANN, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, 128)\n",
    "        self.l2 = nn.Linear(128, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.action_value = nn.Linear(128, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l1 = F.relu(self.l1(x))\n",
    "        l2 = F.relu(self.l2(l1))\n",
    "        l3 = F.relu(self.l3(l2))\n",
    "        return self.action_value(l3)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iter number - 0\n"
     ]
    }
   ],
   "source": [
    "## This block shows how data sampling is done\n",
    "device = torch.device(\"cpu\")\n",
    "dq_sampler = ANN(3, 1).to(device) \n",
    "## input to the ANN is u - x (u is cop, x is com location), xd, a_set(possible set of actions)\n",
    "action_set = np.linspace(-0.2, 0.2, 9)\n",
    "sample = sample_data(1, 1.5, 0.2, action_set, dq_sampler, False)\n",
    "# sample[0:20]\n",
    "## The simulation below shows stepping sequences using an epsilon greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iter number - 0\n",
      "running iter number - 1\n",
      "running iter number - 2\n",
      "running iter number - 3\n",
      "running iter number - 4\n",
      "running iter number - 5\n",
      "running iter number - 6\n",
      "running iter number - 7\n",
      "running iter number - 8\n",
      "running iter number - 9\n",
      "Epoch 1/50\n",
      "88/88 [==============================] - 0s 430us/step - loss: 40.6655\n",
      "Epoch 2/50\n",
      "88/88 [==============================] - 0s 77us/step - loss: 6.5521\n",
      "Epoch 3/50\n",
      "88/88 [==============================] - 0s 121us/step - loss: 7.0706\n",
      "Epoch 4/50\n",
      "88/88 [==============================] - 0s 100us/step - loss: 7.1477\n",
      "Epoch 5/50\n",
      "88/88 [==============================] - 0s 122us/step - loss: 6.9291\n",
      "Epoch 6/50\n",
      "88/88 [==============================] - 0s 129us/step - loss: 7.8549\n",
      "Epoch 7/50\n",
      "88/88 [==============================] - 0s 168us/step - loss: 6.1531\n",
      "Epoch 8/50\n",
      "88/88 [==============================] - 0s 131us/step - loss: 5.8045\n",
      "Epoch 9/50\n",
      "88/88 [==============================] - 0s 117us/step - loss: 5.4444\n",
      "Epoch 10/50\n",
      "88/88 [==============================] - 0s 127us/step - loss: 5.2855\n",
      "Epoch 11/50\n",
      "88/88 [==============================] - 0s 120us/step - loss: 5.3737\n",
      "Epoch 12/50\n",
      "88/88 [==============================] - 0s 121us/step - loss: 5.2672\n",
      "Epoch 13/50\n",
      "88/88 [==============================] - 0s 129us/step - loss: 5.3710\n",
      "Epoch 14/50\n",
      "88/88 [==============================] - 0s 111us/step - loss: 5.2170\n",
      "Epoch 15/50\n",
      "88/88 [==============================] - 0s 107us/step - loss: 5.3205\n",
      "Epoch 16/50\n",
      "88/88 [==============================] - 0s 123us/step - loss: 5.3341\n",
      "Epoch 17/50\n",
      "88/88 [==============================] - 0s 131us/step - loss: 5.2973\n",
      "Epoch 18/50\n",
      "88/88 [==============================] - 0s 143us/step - loss: 5.2597\n",
      "Epoch 19/50\n",
      "88/88 [==============================] - 0s 136us/step - loss: 5.2985\n",
      "Epoch 20/50\n",
      "88/88 [==============================] - 0s 145us/step - loss: 5.2269\n",
      "Epoch 21/50\n",
      "88/88 [==============================] - 0s 130us/step - loss: 5.2229\n",
      "Epoch 22/50\n",
      "88/88 [==============================] - 0s 146us/step - loss: 5.2509\n",
      "Epoch 23/50\n",
      "88/88 [==============================] - 0s 115us/step - loss: 5.2122\n",
      "Epoch 24/50\n",
      "88/88 [==============================] - 0s 126us/step - loss: 5.2153\n",
      "Epoch 25/50\n",
      "88/88 [==============================] - 0s 119us/step - loss: 5.2639\n",
      "Epoch 26/50\n",
      "88/88 [==============================] - 0s 131us/step - loss: 5.2630\n",
      "Epoch 27/50\n",
      "88/88 [==============================] - 0s 124us/step - loss: 5.2063\n",
      "Epoch 28/50\n",
      "88/88 [==============================] - 0s 127us/step - loss: 5.2390\n",
      "Epoch 29/50\n",
      "88/88 [==============================] - 0s 119us/step - loss: 5.2671\n",
      "Epoch 30/50\n",
      "88/88 [==============================] - 0s 140us/step - loss: 5.2241\n",
      "Epoch 31/50\n",
      "88/88 [==============================] - 0s 128us/step - loss: 5.2079\n",
      "Epoch 32/50\n",
      "88/88 [==============================] - 0s 130us/step - loss: 5.3147\n",
      "Epoch 33/50\n",
      "88/88 [==============================] - 0s 120us/step - loss: 5.2464\n",
      "Epoch 34/50\n",
      "88/88 [==============================] - 0s 100us/step - loss: 5.2114\n",
      "Epoch 35/50\n",
      "88/88 [==============================] - 0s 111us/step - loss: 5.2568\n",
      "Epoch 36/50\n",
      "88/88 [==============================] - 0s 133us/step - loss: 5.3098\n",
      "Epoch 37/50\n",
      "88/88 [==============================] - 0s 125us/step - loss: 5.2368\n",
      "Epoch 38/50\n",
      "88/88 [==============================] - 0s 149us/step - loss: 5.1814\n",
      "Epoch 39/50\n",
      "88/88 [==============================] - 0s 144us/step - loss: 5.2260\n",
      "Epoch 40/50\n",
      "88/88 [==============================] - 0s 136us/step - loss: 5.2561\n",
      "Epoch 41/50\n",
      "88/88 [==============================] - 0s 130us/step - loss: 5.2429\n",
      "Epoch 42/50\n",
      "88/88 [==============================] - 0s 134us/step - loss: 5.2304\n",
      "Epoch 43/50\n",
      "88/88 [==============================] - 0s 129us/step - loss: 5.2110\n",
      "Epoch 44/50\n",
      "88/88 [==============================] - 0s 127us/step - loss: 5.2243\n",
      "Epoch 45/50\n",
      "88/88 [==============================] - 0s 139us/step - loss: 5.2547\n",
      "Epoch 46/50\n",
      "88/88 [==============================] - 0s 141us/step - loss: 5.2277\n",
      "Epoch 47/50\n",
      "88/88 [==============================] - 0s 125us/step - loss: 5.2169\n",
      "Epoch 48/50\n",
      "88/88 [==============================] - 0s 128us/step - loss: 5.2754\n",
      "Epoch 49/50\n",
      "88/88 [==============================] - 0s 99us/step - loss: 5.2116\n",
      "Epoch 50/50\n",
      "88/88 [==============================] - 0s 78us/step - loss: 5.2201\n"
     ]
    }
   ],
   "source": [
    "########### To be removed ###############################################################\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "sample = sample_data(10, 1.5, 0.2, action_set, dq_sampler, False)\n",
    "x_train = sample[:,0:3]\n",
    "y_train = sample[:,3]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=3, activation = 'relu'))\n",
    "model.add(Dense(128, activation= 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.8)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "metrics=['loss']\n",
    "history = model.fit(x_train, y_train, epochs = 50, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ameduri/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([88])) that is different to the input size (torch.Size([88, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7470, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9019, grad_fn=<MseLossBackward>)\n",
      "tensor(7.8112, grad_fn=<MseLossBackward>)\n",
      "tensor(10.1871, grad_fn=<MseLossBackward>)\n",
      "tensor(5.8703, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7834, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5984, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7729, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5357, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6065, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5505, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5110, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5339, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4768, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4830, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4752, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4467, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4503, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4374, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4239, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4240, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4147, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4068, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4053, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3996, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3945, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3925, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3888, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3854, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3836, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3811, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3787, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3774, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3757, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3740, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3729, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3718, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3706, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3698, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3690, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3682, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3675, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3669, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3664, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3659, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3655, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3651, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3647, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3644, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3641, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3638, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3636, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3634, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3632, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3630, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3629, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3627, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3626, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3625, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3623, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3622, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3621, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3620, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3619, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3619, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3618, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3617, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3616, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3616, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3615, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3614, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3614, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3613, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3613, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3612, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3612, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3611, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3611, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3610, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3610, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3609, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3609, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3609, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3608, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3608, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3607, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3607, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3607, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3606, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3606, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3606, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3605, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3605, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3605, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3604, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3604, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3604, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3603, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3603, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3603, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3602, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3602, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3602, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3601, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3601, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3601, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3600, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3600, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3599, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3599, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3599, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3599, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3598, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3598, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3598, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3598, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3598, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3598, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3597, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3597, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3597, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3597, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3597, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3597, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3597, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3596, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3596, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3596, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3596, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3596, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3596, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3596, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3596, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3594, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3593, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3592, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3591, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3591, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3591, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3591, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "######################### To be removed #################################################\n",
    "\n",
    "ann = ANN(3, 1).to(device)\n",
    "optimizer = optim.SGD(ann.parameters(), lr= 0.005, momentum=0.8)\n",
    "x_train_tc = torch.from_numpy(x_train).float()\n",
    "y_train_tc = torch.from_numpy(y_train).float()\n",
    "\n",
    "for i in range(200):\n",
    "    prediction = ann(x_train_tc)\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.mse_loss(prediction, y_train_tc)\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block contains the deep Q stepper\n",
    "class DeepQStepper:\n",
    "    \n",
    "    def __init__(self, no_inputs, no_outputs, action_set, h, step_time):\n",
    "        '''\n",
    "        Input:\n",
    "            no_inputs : size of the feature vector into the ANN\n",
    "            no_outputs: Size of the output array from the ANN\n",
    "            action_set: The list of all possible actions\n",
    "            h : height of lipm above ground\n",
    "            step_time: the step time\n",
    "        '''\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        ## input to the ANN is u - x (u is cop, x is com location), xd, a_set(possible set of actions)\n",
    "        self.dq_stepper = ANN(no_inputs, no_outputs).to(self.device) \n",
    "        ## creating a target network to stabilize training\n",
    "        self.dq_stepper_tar = copy.deepcopy(self.dq_stepper)\n",
    "        self.action_set = action_set\n",
    "        self.h = h\n",
    "        self.step_time = step_time\n",
    "        self.dq_stepper_optimizer = optim.SGD(self.dq_stepper.parameters(), lr = 1e-3, momentum=0.8)\n",
    "        \n",
    "    def sample_data(self, no_episodes, epi_t, h, action_set, q_function, show_episode = False):\n",
    "        '''\n",
    "        This method samples data and returns the data in the SARS form [state, action, reward, state_t+1].\n",
    "        Input:\n",
    "            no_episodes : number of episodes of sample data\n",
    "            epi_t : duration of each episode\n",
    "            h : height of LIPM from the ground\n",
    "            action_set : the array of possible actions\n",
    "            q_function : the ANN that predicts value function given current state, action. Q(s,a)\n",
    "            show_episode : shows a simulation of the episode.\n",
    "        '''\n",
    "        # this function samples data\n",
    "        env = LipmEnv(h)\n",
    "        sample_data = []\n",
    "        for e in range(no_episodes):\n",
    "#             print(\"running iter number - \" + str(e))\n",
    "            x = [0.0, 4*np.random.random() - 2]\n",
    "            u0 = action_set[np.random.randint(9)]\n",
    "            env.reset_env(x, u0, epi_t)\n",
    "            #espillon greedy\n",
    "            x[0] = u0 - x[0]\n",
    "            if np.random.random() > 0.2:\n",
    "                a = self.compute_max_Q(x, action_set, q_function)[1]\n",
    "            else:\n",
    "                a = np.random.randint(9)\n",
    "\n",
    "            ## sars_t = s_t, a_t, r_t, s_t+1\n",
    "            sars_t = np.zeros(6)\n",
    "            sars_t[0] = u0 - x[0]\n",
    "            sars_t[1] = x[1]\n",
    "            sars_t[2] = action_set[a]\n",
    "            for t in range(0, int(epi_t*1000) - 1):\n",
    "                if t % int(self.step_time * 1000) == 0 and t > 0:\n",
    "                    sars_t[3] = env.compute_reward(self.step_time)\n",
    "                    env.set_action(env.sim_data[:,env.t][0] + action_set[a]) ## setting action\n",
    "                    sars_t[4] = env.sim_data[:,env.t][2] - env.sim_data[:,env.t][0]\n",
    "                    sars_t[5] = env.sim_data[:,env.t][1]\n",
    "                    sample_data.append(sars_t)\n",
    "\n",
    "                    sars_t = np.zeros(6)\n",
    "                    sars_t[0] = env.sim_data[:,env.t][2] - env.sim_data[:,env.t][0]\n",
    "                    sars_t[1] = env.sim_data[:,env.t][1]\n",
    "                    # epsillon greedy\n",
    "                    if np.random.random() > 0.2:\n",
    "                        a = self.compute_max_Q(sars_t[0:2], action_set, q_function)[1]\n",
    "                    else:\n",
    "                        a = np.random.randint(9)\n",
    "\n",
    "                    sars_t[2] = action_set[a]\n",
    "\n",
    "                env.step_env()\n",
    "\n",
    "            if show_episode: \n",
    "                env.show_episode(5, e)\n",
    "        \n",
    "        ## shifting reward up since the reward is obtained in the next step\n",
    "        ## TODO; Check is this matters\n",
    "        sample_data = np.asarray(sample_data)\n",
    "        sample_data[:,3] = np.roll(sample_data[:,3],-1)\n",
    "        sample_data = sample_data[0:-2]\n",
    "                        \n",
    "        return np.asarray(sample_data)\n",
    "        \n",
    "    def compute_max_Q(self, x, action_set, q_function):\n",
    "        '''\n",
    "        This function returns the max Q value for the given state for the set of possible actions.\n",
    "        It also returns the action that has maximum value\n",
    "        Input:\n",
    "            x : state to be evaluated\n",
    "            action_set : the array of possible actions\n",
    "            q_function: model to compute Q value\n",
    "        '''\n",
    "        x_in = [x[0], x[1], 0]\n",
    "        x_in = np.tile(x_in, ((len(action_set),1)))\n",
    "        x_in[:,2] = action_set\n",
    "        state_values = q_function(torch.FloatTensor(x_in)).cpu().detach().numpy()\n",
    "        a_opt = np.argmax(state_values)\n",
    "        q_opt = np.max(state_values)\n",
    "\n",
    "        return q_opt, a_opt\n",
    "    \n",
    "    def sample_mini_batch(self, buffer, batch_size):\n",
    "        '''\n",
    "        This function generates a mini batch by randomly selecting elements from the buffer. \n",
    "        It tries to keep atleast 10 elements that are from the latest sample to ensure fast convergence\n",
    "        Input:\n",
    "            buffer: buffer array\n",
    "            batch_size : size of the batch\n",
    "        '''\n",
    "        mini_batch = np.zeros((batch_size, np.shape(buffer)[1]))\n",
    "        for i in range(batch_size):\n",
    "            if i < batch_size - 10:\n",
    "                mini_batch[i] = buffer[np.random.randint(0, int(0.7*len(buffer)))]\n",
    "            else:\n",
    "                mini_batch[i] = buffer[np.random.randint(int(0.7*len(buffer)), int(len(buffer))-1)]\n",
    "                \n",
    "        return mini_batch\n",
    "        \n",
    "    def optimize(self, no_iter, no_episodes, epi_t, batch_size, gamma, tau):\n",
    "        '''\n",
    "        This method trains the deepQstepper model.\n",
    "        Input:\n",
    "            no_episodes: number of episodes worth of data to be stored in buffer\n",
    "            no_iter: number of iteration of training\n",
    "            epi_t: duration of one episode in seconds\n",
    "            batch_size: size of the batch\n",
    "            gamma : discount factor\n",
    "            tau: the rate at which the target q function is updated\n",
    "        '''\n",
    "        for i in range(no_iter):\n",
    "            if i == 0:\n",
    "                buffer = self.sample_data(no_episodes, epi_t, self.h, self.action_set, self.dq_stepper)\n",
    "            else:\n",
    "                new_sample = self.sample_data(1 , epi_t, self.h, self.action_set, self.dq_stepper)\n",
    "                buffer = np.concatenate((buffer, new_sample), axis = 0)\n",
    "                buffer = buffer[len(new_sample):]\n",
    "\n",
    "            mini_batch = self.sample_mini_batch(buffer, batch_size)\n",
    "            X_train = torch.from_numpy(mini_batch[:,0:3]).float()\n",
    "            Y_train = mini_batch[:,3]\n",
    "\n",
    "            for j in range(len(mini_batch)):\n",
    "                Y_train[j] += gamma*self.compute_max_Q(mini_batch[j][4:6], self.action_set, self.dq_stepper_tar)[0]\n",
    "            \n",
    "            Y_train = torch.from_numpy(np.reshape(Y_train,(len(Y_train), 1))).float()\n",
    "            \n",
    "            prediction = self.dq_stepper(X_train)\n",
    "            self.dq_stepper_optimizer.zero_grad()\n",
    "            loss = F.mse_loss(prediction, Y_train)\n",
    "            loss.backward()\n",
    "            self.dq_stepper_optimizer.step()\n",
    "            \n",
    "            for param, target_param in zip(self.dq_stepper.parameters(), self.dq_stepper_tar.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \n",
    "            \n",
    "            ## evaluating the performance of the deepq_stepper at the current iteration\n",
    "            reward = self.show_performance(epi_t)\n",
    "            \n",
    "#             print(\"running iteration number: \" + str(i), \"loss:\" + str(loss))\n",
    "            print(\"running iteration number: \" + str(i), \"reward:\" + str(reward))\n",
    "\n",
    "        return self.dq_stepper\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        '''\n",
    "        This saves the dq_stepper\n",
    "        '''\n",
    "        torch.save(self.dq_stepper.state_dict(), \"../../models/\" + file_name)\n",
    "    \n",
    "    def show_performance(self, epi_t, x0 = None, show_episode = False):\n",
    "        '''\n",
    "        This function returs the reward obtained from the episode using the stepper policy\n",
    "        It also returns a simulation of stepping using the policy\n",
    "        epi_t : episode time\n",
    "        x0 : initial state (position, velocity)\n",
    "        '''\n",
    "        env = LipmEnv(self.h)\n",
    "        if x0:\n",
    "            x = x0\n",
    "        else:\n",
    "            x = [0.0, 4*np.random.random() - 2]\n",
    "        u0 = action_set[np.random.randint(9)]\n",
    "        env.reset_env(x, u0, epi_t)\n",
    "        x[0] = u0 - x[0]\n",
    "        a = self.compute_max_Q(x, self.action_set, self.dq_stepper)[1]\n",
    "        episode_reward = 0\n",
    "        for t in range(0, int(epi_t*1000) - 1):\n",
    "            if t % int(self.step_time * 1000) == 0 and t > 0:\n",
    "                episode_reward += env.compute_reward(self.step_time) # compute reward\n",
    "                env.set_action(env.sim_data[:,env.t][0] + action_set[a]) ## setting action\n",
    "                x = env.sim_data[:,env.t][0:3].copy()\n",
    "                x[0] = x[2] - x[0]\n",
    "                x = x[0:2]\n",
    "                a = self.compute_max_Q(x, self.action_set, self.dq_stepper)[1]\n",
    "            env.step_env()\n",
    "        if show_episode:\n",
    "            env.show_episode(5, 0)        \n",
    "        return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration number: 0 reward:0\n",
      "running iteration number: 1 reward:10\n",
      "running iteration number: 2 reward:0\n",
      "running iteration number: 3 reward:0\n",
      "running iteration number: 4 reward:0\n",
      "running iteration number: 5 reward:0\n",
      "running iteration number: 6 reward:30\n",
      "running iteration number: 7 reward:0\n",
      "running iteration number: 8 reward:0\n",
      "running iteration number: 9 reward:10\n",
      "running iteration number: 10 reward:0\n",
      "running iteration number: 11 reward:0\n",
      "running iteration number: 12 reward:0\n",
      "running iteration number: 13 reward:0\n",
      "running iteration number: 14 reward:10\n",
      "running iteration number: 15 reward:0\n",
      "running iteration number: 16 reward:0\n",
      "running iteration number: 17 reward:30\n",
      "running iteration number: 18 reward:30\n",
      "running iteration number: 19 reward:0\n",
      "running iteration number: 20 reward:20\n",
      "running iteration number: 21 reward:0\n",
      "running iteration number: 22 reward:10\n",
      "running iteration number: 23 reward:0\n",
      "running iteration number: 24 reward:0\n",
      "running iteration number: 25 reward:0\n",
      "running iteration number: 26 reward:0\n",
      "running iteration number: 27 reward:0\n",
      "running iteration number: 28 reward:10\n",
      "running iteration number: 29 reward:0\n",
      "running iteration number: 30 reward:0\n",
      "running iteration number: 31 reward:0\n",
      "running iteration number: 32 reward:0\n",
      "running iteration number: 33 reward:0\n",
      "running iteration number: 34 reward:0\n",
      "running iteration number: 35 reward:10\n",
      "running iteration number: 36 reward:0\n",
      "running iteration number: 37 reward:0\n",
      "running iteration number: 38 reward:10\n",
      "running iteration number: 39 reward:0\n",
      "running iteration number: 40 reward:0\n",
      "running iteration number: 41 reward:0\n",
      "running iteration number: 42 reward:0\n",
      "running iteration number: 43 reward:0\n",
      "running iteration number: 44 reward:0\n",
      "running iteration number: 45 reward:0\n",
      "running iteration number: 46 reward:0\n",
      "running iteration number: 47 reward:0\n",
      "running iteration number: 48 reward:10\n",
      "running iteration number: 49 reward:0\n",
      "running iteration number: 50 reward:0\n",
      "running iteration number: 51 reward:0\n",
      "running iteration number: 52 reward:0\n",
      "running iteration number: 53 reward:0\n",
      "running iteration number: 54 reward:0\n",
      "running iteration number: 55 reward:10\n",
      "running iteration number: 56 reward:0\n",
      "running iteration number: 57 reward:0\n",
      "running iteration number: 58 reward:10\n",
      "running iteration number: 59 reward:0\n",
      "running iteration number: 60 reward:0\n",
      "running iteration number: 61 reward:10\n",
      "running iteration number: 62 reward:0\n",
      "running iteration number: 63 reward:10\n",
      "running iteration number: 64 reward:10\n",
      "running iteration number: 65 reward:0\n",
      "running iteration number: 66 reward:20\n",
      "running iteration number: 67 reward:0\n",
      "running iteration number: 68 reward:0\n",
      "running iteration number: 69 reward:0\n",
      "running iteration number: 70 reward:10\n",
      "running iteration number: 71 reward:0\n",
      "running iteration number: 72 reward:0\n",
      "running iteration number: 73 reward:0\n",
      "running iteration number: 74 reward:0\n",
      "running iteration number: 75 reward:10\n",
      "running iteration number: 76 reward:0\n",
      "running iteration number: 77 reward:10\n",
      "running iteration number: 78 reward:20\n",
      "running iteration number: 79 reward:10\n",
      "running iteration number: 80 reward:0\n",
      "running iteration number: 81 reward:0\n",
      "running iteration number: 82 reward:0\n",
      "running iteration number: 83 reward:20\n",
      "running iteration number: 84 reward:10\n",
      "running iteration number: 85 reward:10\n",
      "running iteration number: 86 reward:0\n",
      "running iteration number: 87 reward:10\n",
      "running iteration number: 88 reward:20\n",
      "running iteration number: 89 reward:0\n",
      "running iteration number: 90 reward:0\n",
      "running iteration number: 91 reward:20\n",
      "running iteration number: 92 reward:0\n",
      "running iteration number: 93 reward:0\n",
      "running iteration number: 94 reward:0\n",
      "running iteration number: 95 reward:0\n",
      "running iteration number: 96 reward:10\n",
      "running iteration number: 97 reward:0\n",
      "running iteration number: 98 reward:0\n",
      "running iteration number: 99 reward:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (l1): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (l2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (action_value): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this block is for training the dq_stepper\n",
    "action_set = np.linspace(-0.15, 0.15, 9)\n",
    "dq_stepper = DeepQStepper(3, 1, action_set, 0.2, 0.15)\n",
    "dq_stepper.optimize(100, 2, 1.5, 32, 0.8, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"432\" height=\"288\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAtQm1kYXQAAAKuBgX//6rcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTUyIHIyODU0IGU5YTU5MDMgLSBILjI2NC9NUEVHLTQg\n",
       "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE3IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
       "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
       "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
       "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
       "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9OSBsb29r\n",
       "YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n",
       "ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n",
       "bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n",
       "aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n",
       "aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n",
       "cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAuIZYiE\n",
       "ADP//vbsvgU1/Z/QlxEsxdpKcD4qpICAdzTAAAADAAB5FfwTZP1ApEOAKLGJP0SpUh2XxRDdCJTc\n",
       "ZzZ4v4cZYQuR+HDQoeqd8MgYlM90eLyIShBlFOfsfeC3y6ZMB6mIjbVgfgAAAwCHrYt4WrCE2FYv\n",
       "soOKL18FFh6KEO0+xpWmSScXj1JLKqArswGLk2qWuFWZ9VDPZBMhWASqGjdGSGAKi7OT8kWUDOCa\n",
       "JMn6zKbjXSR06XShN15KxK0szcoGeRpx+vJZZfOx/943m+4cMLBNZZyPR1p/0c8DzanAmAh4Ye5L\n",
       "aixpPshaN0s28EtyC2y8zN5ycW6mGS4ZGMZlAkWZ0zhZ4kA/pKqRb8z/I325jP/ZTDplYHAjvM8P\n",
       "JPmYXM0VDSMw8/8wbtsnsKYIOnmzqfnVUcXKZH41BbJZSx1pK2FU5dh+8cI9QmPl40p2a4HKcuXe\n",
       "q+LEF/Ty5Sxt454akmZcTeAQ6RzQ1nyRRFNMAAkbmYnrCqjbGdyd5mbe387JseLElE5do6VwSkR+\n",
       "cm6WTEdTAX9jVfXct1ZWquJ0aN5k0GHL+5rY3MuPx+QYq9GcIyx6ZOGBwOeZ8ioUxOaDEQV7xzTC\n",
       "1emR3f7z25TIIxEc+dkFXHwoNLiKANiEW8FCbV8m7XsGDiY2RT8FswAAc35OodQ9kM7gtf6DKaba\n",
       "x1Tw2gm8J2C9zHFPQHHM793Mal0X0QXCOODYJP67CDfYSoRq21/s8bJB3qHXbCyU7dpw3vBwIV4f\n",
       "Pcu+tGTIU7uS0GMiDSRDmCy9R75hRbMvd0bqjqbt45DHYGTk8gxehJ9I0Nt1IwjCGTNFQUVBJI2v\n",
       "0J2F7WYYIIz3WKKfXbXZr6urM8QAIAF7K5e8thniK1li/H/NrHPPQLvSxDmKLojWHP1AztJIrfQb\n",
       "6wbl6K7Ofthl1hX4BBidAzxcTEPi3D6GXT4EaoUWa/a51dwPH4wUKuvZShw45PHtMeLrpFV4r6EL\n",
       "MJVLkyvpNDaAH+S41Kf0WQ/xQvRTL4Gl/eOXXzUX2n7tZ3NiCOxMAyYyP1XeQn8x0fXzuedOoxgu\n",
       "cj1BEduebkeehg/+Hg2k0CxJgAAAAwDgZXE1liTIBGOwY17OzLDuuULynslVuXm014AysWP1TroU\n",
       "PcX9ZD73ORV8BRlPCQucyjZ//kRXdLFWerbyujah8L4mdV+6l4JJFrkAPYD7Rd3OXmRgndNYD5lG\n",
       "MJjUx7L2o4MGIbdImAAAIU0Z2XAveiHWbKSVNuYasJwzeEXp0742X27XYRZFr/01Jb95dY4HQdyS\n",
       "BDGj2d71FeuWvuE43/7VVo4SEuPL1tmlnr2eStPN3GEcgYbHc5MZ9jOR4pQFu3aFELn0AJfRFLWH\n",
       "o9maRyKxMJpeX0OUDjViKqw79Fn+T0O3tUnnlBaO6E5sd2QSSQaCSArlSfzmwQhzQ4i5Tw/IvMbV\n",
       "N0dw8zuQZh/R4PtSDcJ5xjVhkDP2W57rgHMyk8AAzZqBL9AE1U2oucDCgFR6J2xyd+DvDlcJMs4W\n",
       "+bjjiG2Jl/iwjRegNmQAF60ifBopLg3/m/6UaIYWrmB57//DGdwRGHmof1G0YCV+vcMt5auBGuQq\n",
       "Yo0WZ7+VhcJcgyqk3ULM410SvNCUFJHy93Hu7TRjXkjrGAo2m7rEfBYH00Np2vsxJHcX8q47k80K\n",
       "V5sWWLAdpOWt9YJ/FzCjL3h8hd8L3LL1wKMq6mbworbogTsTYTMEeBhOyW3+iyY9YHH8tq7Tt90x\n",
       "fuik8qDWWFL/YZGmyqicvcHR8spErdmEaJzBwd+68kzfXdNBnlloBMpGRr0MA8WrGe8hQbHRk1/5\n",
       "n9q2yQXFd1hAFYRg3W7+0A3knktbU7HEYNyZJQAPcX4aJSILA7wJZuiflOmwPHk5zBLQZCIEzfP+\n",
       "AeAXhMx2S+HpEs3bwyPIT/I56xawxHR6WBdwhJnJ+9ntn+VSbofm10DKGU5PpdsulCD+p/FUyElM\n",
       "pl3r9Jru0WQZOPr5VyDQaBQmaR6JAkK3iQcr7VfRoROCTy3TS7D/monXXj+Xg6Da3H7BX9tH/9Nz\n",
       "dW+9wEweXi7nJRf2wkIZXHTCAzkoalW8kxj2qrNL9s2Xw5Wxj7fvtDkQBOFPXG9BuCRNbaLHEeDT\n",
       "Phh3em//jVXL+7aEn5o1TcFjmvlDuvBkcbau3uyJqRCGZDIEDFcwu1FftqwcBBhxmfyGlAVlXtCx\n",
       "Sh9B7tiSvX/+fkZxNSuZSmvVuPzeN4sFLem+ETneH5TUm4Q75DqS8shXEnOWtyDBxP5smq5jtU/f\n",
       "b8eHrwX2k2rH5aQz7CC/k2/mTKyMxWuuuhdZwwkKk+fEO14MuVNtsAVg3K3z+AGBTR3M5svXifbQ\n",
       "1v8E1YnSliG+XDPJO1raGqjG9/6mxFTQ/QgBfIbIkO9Mi4bxZIkTxdol/FiuesHdy5R/zK87ty5f\n",
       "ZEJu7vlobhtDSzvHwguD/NaDvT8s/MQwY6qiVbjjAY9dDJvT/UTXwxs2BVG4Ugif4VQFDzlAIz40\n",
       "4gTiXh+YcuYN32hLHkvFpig7htCtFk+pvddVpHHtQcRvkLCfcvtEo8HkRhG/JITUPvH1erdrtGJD\n",
       "/ACLe6zqXkfB+uxjRsfiLz9WfEJT/mvA2SIxcWLpE6v5AXT20Nxz+a9tetqOksHyPO4YpPsfN0f3\n",
       "LZiI9uDupSx+k9kCAfntmI2XtSU+w4rqFlx5/W84QTCueGGgyKHEotGz3kskvaG4Ow5Quav8UMpA\n",
       "lcT/EdWKqVtHYu3q6dz5NEQdv+Mmd2KTmqJSmTuhq76WHMOHfCinqrFHcIsYIppQehcOeZLGtVfu\n",
       "yNuJmCM1ErifRBuVRgx5gipuuFkC9Zt1NivlguYocdTakjv6Q+S9rwZY6rq47q70L6tmufAEpLz+\n",
       "OJjf/CEYVl8ajFg963wxDed6yfNNDRfmksraWT0J8WVs9Q5iX8C5rAnIB+8Zhz3R3kCRNncm4mg8\n",
       "pWk44wpJgN12MulVll3FKdKkBNoC1dcSqdpi5IEiCijpgYWOtbsZs0Izb/5onWUZc5LooofOOkJc\n",
       "M6bfzWdcjL0wtpY0e7EqR0eH8chavHyTZayLZ7j3RSICuLcjbc5mMWGynmeZhGn2s/NAlPSd6Ngt\n",
       "Ls3OjoLnoqrm0FcnA5Ko5Z2RlQ6D0TVRLRWf+THWG5a1lodTugvdo5D1KhswMZO42Lx2XGxtbg2R\n",
       "lsu6u6AvxsYnng3ezh+ZvwhvvPErIfI8vPrTlrAjRxMEwdBXrx5gjL+zJ6HvVUAjvL0lGtzBAYD4\n",
       "eBa7G6sd2x1cI99PIsp9sR74AXx+JocbgOJJQNNhHf+9N/jEeJwo1hzwdXwsIVEUmi+U0mdyIQT2\n",
       "zDf4r+HFAcOK86pjwqlWFfjWNKuNnxOyR9g09MKsFPd9RAsCG+nMM9BT49KShsYABQF0NL5ukMn9\n",
       "MAp5+XWlKkvzkaQV69dep24DpWajcV5UDYXvDAU46S70ro+ZzjWosE/3Vqy6pgC0FXrMIjhyrbSf\n",
       "5eKZNS/qJ0IUsxcBkIHSs626emBOLX3Yy0h7dx6wK4y35buDipPE/pTAH6ncgrTsnnr8/wJVzf3L\n",
       "whNn/z49+5dao1auGqHSmwxsT7PqTErNWpK+9Jyp5E1gQx7zzy/LKIcpTH4wo8VLBHqdQlGX84ta\n",
       "7iWT8NKd8qP+4Fc9OFsjM4UlJWMnAk29+yI2Q6il9JcaMyQZVtQDqdMkGubIWbT0EgY+UwV1CbFs\n",
       "SL+YiyXjoNjx4ptFDtZuIARHCtI6NkdkgCqPcpsJ1RmlEmntHdsq1I0+Unjf/Ad0GinbPMUY174v\n",
       "2N+lHDOhRucecdKbV9ydhtfw/QGEBfMMXvqtuchVT3pGDypjmjAT5oyI02KI/iPw4HkG2/CnVcPt\n",
       "fBa07tpZOGXU0HqNt5goBtyOhzkCqOdZwCpzD2RCUztNYX23ABxYaEiZAAAAf0GaJGxDP/6eEAey\n",
       "u2QAj35Ei4hU1uHa5+lKkdOz049kbBufe5vTEX6FVc/6I5lLZsjnM19ngAWlGkKs71wPxERGFGAA\n",
       "cq+PGbRMOYxwDacXM8nww50uBNnqtcTD/mMFjl331DfxntMFB/nm3OGFQJfh7YWXedryQdjPd5dw\n",
       "DTgAAAAxQZ5CeIT/Ac4JXBk5snU+6/V6VQ6AgAwJqaT1ouuvQQpYpYz59D/A+Dq47iWaGD2i5wAA\n",
       "ABkBnmF0Qj8CKr9/B0qXX8lNk3XFzfN8II2AAAAADwGeY2pCPwAAB0IfHOxdYQAAADRBmmhJqEFo\n",
       "mUwIZ//+nhAC92LrCiHGwKAWpNZ6UV11ZDDSwdDlpN2iurPNMUn7U+0bh6WpAAAAJEGehkURLCf/\n",
       "AMRezVhMuSQijwSABzeaNGuAifX6V6TjfWGvYwAAABEBnqV0Qj8AANL9ApB8/5jugQAAABEBnqdq\n",
       "Qj8AANNMev4AHSPAUAAAADxBmqxJqEFsmUwIZ//+nhAAFPoOF/CV8AESXpjfOV3dHg3foC9HDDaX\n",
       "xFUC/HvmLK0fOqoVliq2906MvLAAAAAlQZ7KRRUsJ/8AxF7NWEy5JCKPBIAHN5o0a4RA7RfysOzF\n",
       "maZ3xQAAABQBnul0Qj8AAaX54uCZYruD+jsbWAAAABQBnutqQj8AAaaY9gGT485KWY9k4AAAADZB\n",
       "mvBJqEFsmUwIZ//+nhAAFQlmD8gBqsUDyE9I+8D3+i2qMavKmpISxm9eU8mznc2Y7LiND00AAAAl\n",
       "QZ8ORRUsJ/8AxF7NWEy5JCKPBIAHN5o0a4RA7NVKeSAWZGlh8QAAABIBny10Qj8AAaX54uCUyoQB\n",
       "AYEAAAAPAZ8vakI/AAGmmRKNwEJAAAAAQ0GbM0moQWyZTAhn//6eEAAVAGXH4AUFUoYpseOb322N\n",
       "Paps7aD62la+bBna4WXnvpt+1nvK+RQW/YZBv9jpMv2Ua4AAAAAjQZ9RRRUsJ/8AxF7NWEy5JCKP\n",
       "BIAHN5o0a4RA7NSGguRXIp8AAAATAZ9yakI/AAaaZEo3DereDyx+swAAADJBm3dJqEFsmUwIX//+\n",
       "jLAAFS9r/0uibT8bQAaj6VaC8yfUNgF/FtWyfhscJa82XfCQwgAAAChBn5VFFSwn/wDEXs1YTLkk\n",
       "Io8EgAc3mjRrgid3cD1ETDmUe+73AEHBAAAAEQGftHRCPwABpfohiBo1pMIFAAAAEgGftmpCPwAB\n",
       "ppj1+Z5yL9ojYQAAAEZBm7hJqEFsmUwIX//+jLAHcDJMAFJC70TQy0tOsSpT85WyN2NoUAd2KTgd\n",
       "bP8PY9bM9Fge1MVuMto7PauNrY6LagbHJy4JAAAANEGb2UnhClJlMCGf/p4QABT8bjgDlO3jcZLV\n",
       "X+sV++HlDU439HdBsYJdw7d5cHqdXAv2ResAAAAuQZv6SeEOiZTAhn/+nhAAFPoHNYsCgF6DGdwt\n",
       "kPEVcryF366+fzxvaBolg/zd2QAAAPJBmh5J4Q8mUwIZ//6eEAAU/w70P1AI1X5B2c1zDJQ2Dmkd\n",
       "oQdWUuLFy+vKtlimV1Zxp+Wh7YAXHPvxFv14K069pGAp2e9zIG/3RS+akmq9XB4fYSWyxVqboVsm\n",
       "G2dYeK9WjOsYzyRKUxrQM8I/SEbom28Ulr1tf+2pSC4U9wMrI13Y6Qe77Ft6NtuN/QRyb9EYnJSH\n",
       "wPE2QQ61JAx/lkaDuvyPn1AxSqpNbOHZwoIxXUmoGCN7TvV1IDMdcWQOVyQNTKMkrG/OP6yxoEEA\n",
       "N0cHFsBcggxUabUQTwIxBccR/wUkSplk15qlWg/cOLc5VaTpkAAAADtBnjxFETwn/wC6Yv5COONe\n",
       "FJ1YoM6sKZhWpSWkN4AVVNaK4pBwIDwh5wwfajMDNp3w032uMzq6J1qgKQAAABMBnlt0Qj8AA0w2\n",
       "nkOsza+48qBBAAAAHQGeXWpCPwADYWJqbhB1L0tQVbqHgGP/uSX1TbPbAAAAVkGaQkmoQWiZTAhn\n",
       "//6eEAAU+2eZswehAW/NILn+UG4TpW5k+xj037sJjfH/S7gb3xeACL95050Lu3gFuoRkFQ9cgxv8\n",
       "zZWQJ3V91MpcQ0JlTtzA7ZHAAAAAK0GeYEURLCf/ALp5pMhMuSQijwSF2dTMLbfPBXB+BxKpdZTb\n",
       "iQPPg5YHUuEAAAAUAZ6fdEI/AAGw4DbPOaWyUcn9X0wAAAALAZ6BakI/AAADAl8AAAA4QZqGSahB\n",
       "bJlMCGf//p4QABUeZc/4vVKgEcXsg0QuyJEmC2TK+kg4vwb4AOSeICLbxUD6lYw/9YAAAAAgQZ6k\n",
       "RRUsJ/8AunmkyEy5JCKPBIXZ1M1VYB+xDBD+3F0AAAAQAZ7DdEI/AAbBNmFuRqMWzQAAABABnsVq\n",
       "Qj8AAaZkM80Y3sUxAAAANkGaykmoQWyZTAhn//6eEAAUbw/akgDUl8xG26TXRL6kQViJxjpBsHfK\n",
       "tFYCTA6J26f5wPfxQQAAACVBnuhFFSwn/wC6eaTITLkkIo8EhdnUzCKN4WEEvHIRjqBMgCggAAAA\n",
       "DQGfB3RCPwAAGv+gxIAAAAANAZ8JakI/AAAbCZBiwQAAADtBmw5JqEFsmUwIZ//+nhAAFG8P2BwB\n",
       "sEYIDzIiAp035KifromseGV7+LLrbjQkuSBYpc1ZXndW9fmQwwAAACFBnyxFFSwn/wC6eaTITLkk\n",
       "Io8EhdnUy5Ee2aWWBstRFeAAAAAOAZ9LdEI/AAAa/4O6/ikAAAAOAZ9NakI/AAA2Ex6/h2EAAAAs\n",
       "QZtSSahBbJlMCGf//p4QABR1vSeBkhj8uR8oz2kpRF4MNfov2szT3ajwyw0AAAAkQZ9wRRUsJ/8A\n",
       "unmkyEy5JCKPBIXZ1M1FG4GxnleBa+sxOLKAAAAAEQGfj3RCPwADTcHyTEBmHHo4AAAACwGfkWpC\n",
       "PwAAAwJfAAAARkGblkmoQWyZTAhn//6eEAAUcGJkCwEIepgRkuJ35n4096VJgE0HcSIaWdHGzIUk\n",
       "hWPjIMpnwiw232KJ/oNg8CCE6aiz9IYAAAArQZ+0RRUsJ/8AunmkyEy5JCKPBIXZ1M1a4lzkXzhv\n",
       "nkW2EWBelSvfG6JiwAAAAA8Bn9N0Qj8ABr/oCgIInIEAAAARAZ/VakI/AAbCY9fmfwwV9mAAAABU\n",
       "QZvaSahBbJlMCGf//p4QABR5VHxKAhQZeaKTf7z5SNPCa9tDUR1uoCG3bCg+zD0mGgR6olwnLA4Y\n",
       "YOwarx+OK5FiAew9WOeCRdraFM39wzls7xiVAAAAJEGf+EUVLCf/ALp5pMhMuSQijwSF2dTNU9wD\n",
       "wr7laVJfhH+DgQAAABUBnhd0Qj8ABr/ni4JsteNwNvbNirgAAAASAZ4ZakI/AAbCY9fmecZFt0fR\n",
       "AAAAbUGaHkmoQWyZTAhn//6eEAAUcGItcBDZJPeNSErIAVY63RE5FMYGCMZmKy0DdhtpIpNkCevr\n",
       "58V0Z/2S1OLNyQP08GVoKkIna1GwYaxmfjwdIBOhO6fg8Hd+uNCjYz8dl/2NGayuLSUsqGgQmRMA\n",
       "AAAwQZ48RRUsJ/8AunmkyEy5JCKPBIXZ1M1T3ygzi179AB9t7vaPoAxhFVzD5cuzcOVBAAAAIQGe\n",
       "W3RCPwAGv+evY+d1TmVGekgfJjzXf1wLC2WvEsZCGQAAABABnl1qQj8ABsJfrSU4IPHAAAAAKUGa\n",
       "QkmoQWyZTAhn//6eEAAUe8MXXuImWhMfCUhFH++mGdY59sq4t7KwAAAAHkGeYEUVLCf/ALp5pMhM\n",
       "uSQijwSF2dTNU9R26mLefQAAAA0Bnp90Qj8ABr/oAryAAAAADwGegWpCPwAGwmQHRuBKQQAAADVB\n",
       "moZJqEFsmUwIZ//+nhAAFH3iyLcBKiQp6vKiuEzz8E99/QzCpT3b2OZZEyT+8ey71K0xaQAAACNB\n",
       "nqRFFSwn/wC6eaTITLkkIo8EhdnUzVPbtVmG9ay4SmqOaQAAAA0BnsN0Qj8ABr/oAryBAAAADwGe\n",
       "xWpCPwAGwmSSLlWFgQAAACZBmslJqEFsmUwIZ//+nhAAFGoOEc3OaQ0SxNIfJ33cILqRIltsKQAA\n",
       "AB1BnudFFSwn/wC6eaTITLkkIo8EhdnUzVPTPrqjjgAAAA0BnwhqQj8ABsJj/mrAAAAAJkGbDUmo\n",
       "QWyZTAhn//6eEAAUa6Lr+QNqb3uPrlN7eM2lzWRKKLXhAAAAH0GfK0UVLCf/ALp5pMhMuSQijwSF\n",
       "2dTNU9u1W38AdMAAAAANAZ9KdEI/AAa/6AK8gAAAABEBn0xqQj8ABsJkSEAlrQpltwAAAEBBm1FJ\n",
       "qEFsmUwIZ//+nhAAFH2FNMhgDblaERNshpAOf9mhtuhyBtoo63sAOvC9fO4obXo4dhd/T2zMBES5\n",
       "fKZFAAAAHUGfb0UVLCf/ALp5pMhMuSQijwSF2dTNU9bjs8KLAAAAEgGfjnRCPwAGv+kCQPODCGZQ\n",
       "gAAAAA8Bn5BqQj8ABsJkki5VhYAAAAA+QZuVSahBbJlMCGf//p4QABRrq5QA250gnuI7/eyhyB8n\n",
       "Hl9jwRhSc8y2ysodIhb0nWHWkfVMQx4k6FcLMrEAAAAgQZ+zRRUsJ/8AunmkyEy5JCKPBIXZ1M1T\n",
       "23BhktmVOz4AAAATAZ/SdEI/AAa/6EIQNFu3I+31kgAAABABn9RqQj8ABsJkjm3zcIyhAAAAKEGb\n",
       "2UmoQWyZTAhn//6eEAAUcGWIEPcYAK2VrUSqYVA268y7fmFJli4AAAAkQZ/3RRUsJ/8AunmkyEy5\n",
       "JCKPBIXZ1M1T24JGNOYpqFHX1m9BAAAAEAGeFnRCPwAGv+j70mbPGUEAAAAPAZ4YakI/AAbCZEg3\n",
       "ABJAAAAAaUGaHUmoQWyZTAhn//6eEAAUa6zjAFBm3y8suxcG6yFTZA558ugW+Vev9+pnrYNJluc9\n",
       "mzu/+wEF2fGFf/d20ajwMiwjWUePLeylSATexoHxrV9WYCOJBeX2md4zy4OSpsR0SH+c3URYaQAA\n",
       "AClBnjtFFSwn/wC6eaTITLkkIo8EhdnUzVPb1yeL2K2Od49onIu0FbmLOgAAABoBnlp0Qj8ABr/o\n",
       "gRfjJD0Lhb93ACdFxWTUhwAAAA0BnlxqQj8ABsJj/mrBAAAANUGaQUmoQWyZTAhn//6eEAAUbxou\n",
       "cgDoWUlHziKhrRWly/XneGYheOuPOh1wt7HvIMIEyUteAAAAHUGef0UVLCf/ALp5pMhMuSQijwSF\n",
       "2dTNU9R1wHdAAAAADQGennRCPwAGv+gCvIEAAAANAZ6AakI/AAbCY/5qwAAAADpBmoVJqEFsmUwI\n",
       "Z//+nhAAFHUgSjqwC0dgyvALkJITzreiuINsw3ELJOU1I+l7nmXNn2dkmBrbj4kxAAAAG0Geo0UV\n",
       "LCf/ALp5pMhMuSQijwSF2dTNU9IHHAAAAA8BnsJ0Qj8ABr/oQhBAZIEAAAANAZ7EakI/AAbCY/5q\n",
       "wQAAADpBmslJqEFsmUwIZ//+nhAAFHYRuUBBxX5GFQI0TZA4Nkiao6xJ1Rb8v5W29BPFlIc9xWpW\n",
       "fNAGQ/4ZAAAAIkGe50UVLCf/ALp5pMhMuSQijwSF2dTNVAZg+o6442/Rt/UAAAANAZ8GdEI/AAa/\n",
       "6AK8gAAAABMBnwhqQj8ABsJkRjb5tfQ80y1AAAAAPkGbDUmoQWyZTAhn//6eEAAUe8zPk4voqQAQ\n",
       "Ik2wX4DXiv9/H0Uy/DYppIqpI+vX2pNmPYVpKauqjnPGKtZlAAAAIUGfK0UVLCf/ALp5pMhMuSQi\n",
       "jwSF2dTNVAcIwrtVLiRKwAAAABIBn0p0Qj8ABr/ofukzYQBgVdsAAAAUAZ9MakI/AAbCZI5t82vo\n",
       "eaNV6YEAAABXQZtRSahBbJlMCGf//p4QABRqJo4BpqIc6rlyp04QVxB9H1+wn2VUNBG84HKMINfa\n",
       "LB0PuULpT8jLvogpl1keBvrPXjmhryMcdxPUO91P4dOrJD4tb8e9AAAAIEGfb0UVLCf/ALp5pMhM\n",
       "uSQijwSF2dTNVAcI40SG9SJxAAAAFQGfjnRCPwAGv+j70mbCAMC62GNNsAAAABMBn5BqQj8ABsJk\n",
       "kc98QlG3jHgsAAAAQUGblUmoQWyZTAhn//6eEAAUdnCg4BqUeSZMbo+pMJtmY6T9p3W1RrkW7A4S\n",
       "IDniZ3L+qlY49SGwdJDT75vxoEQ1AAAAHkGfs0UVLCf/ALp5pMhMuSQijwSF2dTNVAcIwreLuAAA\n",
       "ABIBn9J0Qj8ABr/pAwqsEEhlasQAAAAQAZ/UakI/AAbCZI5t83CMoQAAALFBm9lJqEFsmUwIZ//+\n",
       "nhAAFHBl1NgFlAQx35QtB10bgUXuL6bn0BSbd+oUDnNtENDgFUFO+RNI9yVB82n14+dcfi1Xr+ob\n",
       "6MFyL1cKigJzjEr82fnagMC4wzoL/t9v0OF+PsuOZ9iQBLalxMeDUhDo0v6yIU2xQUzfAyvO5kIi\n",
       "bhxJG1PmJFR1K3Dace/5ZMU8wmd6+wzJdtdex5xIlBZxxvROkyO5akxf/PAMPBaLk5AAAAAsQZ/3\n",
       "RRUsJ/8AunmkyEy5JCKPBIXZ1M1UBwlqsDE1RXpRRxGjkvy0HmxTxAcAAAAQAZ4WdEI/AAa/6PvS\n",
       "Zs8ZQQAAABsBnhhqQj8ABsJkkkwVSKqKwZhQmV6MijjitsQAAAAzQZodSahBbJlMCGf//p4QABRq\n",
       "IqACkn/u1YWJQ1fogWUbdEwFzgERGUOMVvKIvBMiTQl5AAAAIEGeO0UVLCf/ALp5pMhMuSQijwSF\n",
       "2dTNVAcHt7dLr8c8AAAAEQGeWnRCPwAGv+hAdKMG1gyBAAAAEQGeXGpCPwAGwmiFwmCXAMWBAAAA\n",
       "P0GaQUmoQWyZTAhn//6eEAAUhWgIAAPlY//WEBo6akQmbT2pMte8x97u17wX42BqQ5saI0v00Gb+\n",
       "yqxhAwkmgAAAAB9Bnn9FFSwn/wC6eaTITLkkIo8EhdnUzVQHCIofcFmaAAAAEAGennRCPwAGv+9h\n",
       "bqY1ktUAAAAPAZ6AakI/AAbCaIXB5LuAAAAAO0GahUmoQWyZTAhn//6eEAAUaiaOA+si2aMRno0d\n",
       "WrTKer1ZNghinpxIjoFsAMJn2MWDtMBYV6hckD6hAAAAHUGeo0UVLCf/ALp5pMhMuSQijwSF2dTN\n",
       "VAcIhgUEAAAADwGewnRCPwAGv+9hbPdvIwAAAA0BnsRqQj8ABsJj/mrBAAAAS0GayUmoQWyZTAhn\n",
       "//6eEAAUaiKgAo3ZpyWjdWBQOUoftyt6WoESrTLxspl5xS9yMkmVrPW1ozWsFawQ0kMyqQAU4nOS\n",
       "MesFkvWC4QAAAB9BnudFFSwn/wC6eaTITLkkIo8EhdnUzVQHCIYbcWjBAAAADQGfBnRCPwAGv+gC\n",
       "vIAAAAANAZ8IakI/AAbCY/5qwAAAACxBmw1JqEFsmUwIZ//+nhAACjcDw2F9n83eRyTg8tvryANR\n",
       "iQsR51mGpItn1QAAAB1BnytFFSwn/wC6eaTITLkkIo8EhdnUzVQHCIYFBAAAAA0Bn0p0Qj8ABr/o\n",
       "AryAAAAADQGfTGpCPwAGwmP+asEAAAAlQZtRSahBbJlMCGf//p4QABRqDhMojKVkyDmeUL7JUqKf\n",
       "NpZG+QAAAB1Bn29FFSwn/wC6eaTITLkkIo8EhdnUzVQHCIYFBQAAAA0Bn450Qj8ABr/oAryAAAAA\n",
       "DQGfkGpCPwAGwmP+asAAAAAeQZuVSahBbJlMCGf//p4QAAo2/gyF9HveFGXQXdqhAAAAH0Gfs0UV\n",
       "LCf/ALp5pMhMuSQijwSF2dTNVAcIhjWia0AAAAANAZ/SdEI/AAa/6AK8gAAAAA0Bn9RqQj8ABsJj\n",
       "/mrBAAAAnEGb2UmoQWyZTAhn//6eEAAKQlafAVF8+pMEGqAL9DbvjId346YHjlHISnUI0OeFKsHI\n",
       "eH3M0rE5yBhkF1f06XSvAhjlK/SWCiwnWNCvBMZtSNRWXnio1F7yIDXdQXpAhb7jND2rWrgQRHvF\n",
       "UjXhEfxtpTvx7DQEO+olQRaJ8u89f9Rfa3qt1AB55UFjEa8dVZNCZM+Byu0f/IzhOQAAACRBn/dF\n",
       "FSwn/wC6eaTITLkkIo8EhdnUzVQHCW8ax54c810g2k8AAAAXAZ4WdEI/AAa/6QI+hkNdHgQR/aES\n",
       "jEEAAAAPAZ4YakI/AAbCZCIbgDTAAAAAMEGaHUmoQWyZTAhn//6eEAAUbGL0BCZytPliGI5E14nQ\n",
       "tvGZk6SlqYu74wHscRgBgQAAAB9BnjtFFSwn/wC6eaTITLkkIo8EhdnUzVQHCIercHHAAAAADwGe\n",
       "WnRCPwAGv+hAdKI3QQAAAA8BnlxqQj8ABsJkIhuANMEAAAA7QZpBSahBbJlMCGf//p4QABRwH/yA\n",
       "aSIXFUKppeU+DgnrkeUjKZ23Jkl/w8kP0e9hXYHbkRv+e4VefFgAAAAfQZ5/RRUsJ/8AunmkyEy5\n",
       "JCKPBIXZ1M1UBwiHq3BxwAAAAA8Bnp50Qj8ABr/oQHSiN0EAAAAPAZ6AakI/AAbCZCIbgDTAAAAA\n",
       "OkGahUmoQWyZTAhn//6eEAAUaVQfq1ReqkABUwataSUZNw3I43xfhT1t84kVjPSl2kCToStFcivo\n",
       "vP0AAAAfQZ6jRRUsJ/8AunmkyEy5JCKPBIXZ1M1UGUjL8asdMAAAAA8BnsJ0Qj8ABr/oQHSiN0EA\n",
       "AAARAZ7EakI/AAbCZI5uG9XAYsEAAABFQZrJSahBbJlMCGf//p4QABR14pQBQH7k5GZsE+egDKwA\n",
       "+cOFZMl/3GrKpAlz9MFGYdwH3Nv7zdk0UEBpAnn/PX7b9quxAAAAIEGe50UVLCf/ALp5pMhMuSQi\n",
       "jwSF2dTNVAcIh6tyVKjJAAAAEQGfBnRCPwAGv+j70q+6pSqAAAAAEgGfCGpCPwAGwmSObhvVwfvE\n",
       "twAAAElBmw1JqEFsmUwIZ//+nhAAFG8h3wA2EQxb6w5xDyxyChcokv7FcQZvTm5ZbZhS5dQNE4CA\n",
       "mLkGHP+kgXcuncL2jLdlTLAp0uc3AAAAIEGfK0UVLCf/ALp5pMhMuSQijwSF2dTNVAcIh6tyUg3Y\n",
       "AAAAEgGfSnRCPwAGv+j70q+6qR7BCgAAABIBn0xqQj8ABsJkjm4b1cH7xLcAAABGQZtRSahBbJlM\n",
       "CGf//p4QABRwY4RAE+FUMw6inAkghVNQSw5V7J3s4oHhTKqG+gPLCU7dUItfycSFTcP4z4yBz38C\n",
       "kuGIgQAAAClBn29FFSwn/wC6eaTITLkkIo8EhdnUzVQHCXDcv7gmE35ilhU+MfpNVQAAABIBn450\n",
       "Qj8ABr/o+9KvuqkewQoAAAASAZ+QakI/AAbCZCIbfNz/2mXAAAAAekGblUmoQWyZTAhn//6eEAAU\n",
       "eKbEzgGo6tEmmnNIAUHQNqGErV0j311jWI01x+/LSVWn0NYmUIz5a/LmaVcwtpDBLeCB2DxdOVym\n",
       "d3cM+gIcNb+NC/Vw7k/+0eIcVBeFKuvQLjYYMu42BpbsLXJMGRgdRpQPSoPFpMbRAAAALkGfs0UV\n",
       "LCf/ALp5pMhMuSQijwSF2dTNVAcJeSplf/4kjBSsme3Cz30GCHcoujAAAAARAZ/SdEI/AAa/6EB0\n",
       "mbV+TfcAAAAcAZ/UakI/AAbCZEglGMJ+6GC5JHzQYfLTv11mOQAAADRBm9lJqEFsmUwIZ//+nhAA\n",
       "FHZZPgAM8LZAhTNP8hogzSrYFBGEO/aOGZju3TXKYIt/AmQYAAAAK0Gf90UVLCf/ALp5pMhMuSQi\n",
       "jwSF2dTNVAcJcN1plHp5Qda6nyVQJ38KI7EAAAASAZ4WdEI/AAa/6EB0mbCEhu4XAAAAEgGeGGpC\n",
       "PwAGwmQQDb5u8tplwAAAADhBmhxJqEFsmUwIZ//+nhAAFHBeq6gD42t8WSHThGbFnUMt2H5Xg5Vi\n",
       "7wiMdJvzuMoI9a47h6i1oQAAACJBnjpFFSwn/wC6eaTITLkkIo8EhdnUzVQHB1wbD7vA62TXAAAA\n",
       "FQGeW2pCPwAGwmRIJRjZkz+hlc7wgQAAADhBmkBJqEFsmUwIZ//+nhAAFGs29cqjNYcHgBVe7LZ/\n",
       "eMRGOdTQkGMSsMBF7KcMr3Md8FIDaUzz2wAAADFBnn5FFSwn/wC6eaTITLkkIo8EhdnUzVQWp8Iy\n",
       "dWY3IABaigpcwYzPLotY6iZy0jvoAAAAEQGenXRCPwAGv+ghOlNbqzSAAAAAFAGen2pCPwAGwmSS\n",
       "SorlU0LIHuCBAAAALkGahEmoQWyZTAhf//6MsAAUoLKUCHdXP8F3jtPkfIz/R0WiXbVi4GuTip+8\n",
       "uKYAAAAzQZ6iRRUsJ/8AunmkyEy5JCKPBIXZ1M1UE/8Mpua3ACNwgTVF6C+LP1KTF8fEgXTjjLph\n",
       "AAAAFwGewXRCPwAGv+kCZKiBlQlzTtRFEaAoAAAAFgGew2pCPwAGwmSSSqiOMil0sb08cWEAAAAj\n",
       "QZrISahBbJlMCEf//eEAATb3Vg54ATqQNbPcRxEaKTmd+O0AAAA+QZ7mRRUsJ/8AunmkyEy5JCKP\n",
       "BIXZ1M1UG+wa9w0B14iPgA24K4rTu1srnP/Y4yYS7rrtdFxCK76cSxZF00EAAAAXAZ8FdEI/AAa/\n",
       "6QJkxlGGe7I2dcMefYEAAAAhAZ8HakI/AAbCXRVrfC3MpKgVn1JYrMSpl8if/8V88sNgAAAMcm1v\n",
       "b3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAABOhAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAA\n",
       "AAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAuc\n",
       "dHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAABOhAAAAAAAAAAAAAAAAAAAAAAABAAAA\n",
       "AAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAGwAAABIAAAAAAAJGVkdHMAAAAcZWxzdAAA\n",
       "AAAAAAABAAAToQAAAgAAAQAAAAALFG1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAKAAAAMkAVcQA\n",
       "AAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAACr9taW5mAAAA\n",
       "FHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAp/\n",
       "c3RibAAAALNzdHNkAAAAAAAAAAEAAACjYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAGwASAA\n",
       "SAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADFhdmND\n",
       "AWQAFf/hABhnZAAVrNlBsJaEAAADAAQAAAMBQDxYtlgBAAZo6+PLIsAAAAAcdXVpZGtoQPJfJE/F\n",
       "ujmlG88DI/MAAAAAAAAAGHN0dHMAAAAAAAAAAQAAAMkAAAEAAAAAFHN0c3MAAAAAAAAAAQAAAAEA\n",
       "AAYwY3R0cwAAAAAAAADEAAAAAQAAAgAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAA\n",
       "AAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAA\n",
       "AQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAAB\n",
       "AAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAMAAAIAAAAAAQAABQAAAAABAAACAAAAAAEA\n",
       "AAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAA\n",
       "AgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAF\n",
       "AAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEA\n",
       "AAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAA\n",
       "AAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAA\n",
       "AAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAA\n",
       "AgAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAAB\n",
       "AAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEA\n",
       "AAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAA\n",
       "BQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAAB\n",
       "AAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAA\n",
       "AAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAA\n",
       "AAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAA\n",
       "AAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAA\n",
       "AQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAAB\n",
       "AAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEA\n",
       "AAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAA\n",
       "AgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAF\n",
       "AAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEA\n",
       "AAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAA\n",
       "AAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAA\n",
       "AAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAA\n",
       "AgAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAAB\n",
       "AAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAAcc3RzYwAAAAAA\n",
       "AAABAAAAAQAAAMkAAAABAAADOHN0c3oAAAAAAAAAAAAAAMkAAA4+AAAAgwAAADUAAAAdAAAAEwAA\n",
       "ADgAAAAoAAAAFQAAABUAAABAAAAAKQAAABgAAAAYAAAAOgAAACkAAAAWAAAAEwAAAEcAAAAnAAAA\n",
       "FwAAADYAAAAsAAAAFQAAABYAAABKAAAAOAAAADIAAAD2AAAAPwAAABcAAAAhAAAAWgAAAC8AAAAY\n",
       "AAAADwAAADwAAAAkAAAAFAAAABQAAAA6AAAAKQAAABEAAAARAAAAPwAAACUAAAASAAAAEgAAADAA\n",
       "AAAoAAAAFQAAAA8AAABKAAAALwAAABMAAAAVAAAAWAAAACgAAAAZAAAAFgAAAHEAAAA0AAAAJQAA\n",
       "ABQAAAAtAAAAIgAAABEAAAATAAAAOQAAACcAAAARAAAAEwAAACoAAAAhAAAAEQAAACoAAAAjAAAA\n",
       "EQAAABUAAABEAAAAIQAAABYAAAATAAAAQgAAACQAAAAXAAAAFAAAACwAAAAoAAAAFAAAABMAAABt\n",
       "AAAALQAAAB4AAAARAAAAOQAAACEAAAARAAAAEQAAAD4AAAAfAAAAEwAAABEAAAA+AAAAJgAAABEA\n",
       "AAAXAAAAQgAAACUAAAAWAAAAGAAAAFsAAAAkAAAAGQAAABcAAABFAAAAIgAAABYAAAAUAAAAtQAA\n",
       "ADAAAAAUAAAAHwAAADcAAAAkAAAAFQAAABUAAABDAAAAIwAAABQAAAATAAAAPwAAACEAAAATAAAA\n",
       "EQAAAE8AAAAjAAAAEQAAABEAAAAwAAAAIQAAABEAAAARAAAAKQAAACEAAAARAAAAEQAAACIAAAAj\n",
       "AAAAEQAAABEAAACgAAAAKAAAABsAAAATAAAANAAAACMAAAATAAAAEwAAAD8AAAAjAAAAEwAAABMA\n",
       "AAA+AAAAIwAAABMAAAAVAAAASQAAACQAAAAVAAAAFgAAAE0AAAAkAAAAFgAAABYAAABKAAAALQAA\n",
       "ABYAAAAWAAAAfgAAADIAAAAVAAAAIAAAADgAAAAvAAAAFgAAABYAAAA8AAAAJgAAABkAAAA8AAAA\n",
       "NQAAABUAAAAYAAAAMgAAADcAAAAbAAAAGgAAACcAAABCAAAAGwAAACUAAAAUc3RjbwAAAAAAAAAB\n",
       "AAAALAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAA\n",
       "AAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTcuODMuMTAw\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this block is for testing performance\n",
    "dq_stepper.show_performance(1.0, x0 = [0.0, 0], show_episode=True)\n",
    "# dq_stepper.save(\"dqs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05798403128825269"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
